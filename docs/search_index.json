[["index.html", "Análises Ecológicas no R Capa", " Análises Ecológicas no R 2021-10-26 Capa "],["prefácio.html", "Prefácio", " Prefácio "],["cap1.html", "Capítulo 1 Pré-requisitos 1.1 Introdução 1.2 Instalação do R 1.3 Instalação do RStudio 1.4 Versão do R 1.5 Pacotes 1.6 Dados", " Capítulo 1 Pré-requisitos 1.1 Introdução O objetivo desta seção é informar como fazer a instalação dos Programas R e RStudio, além de descrever os pacotes e dados necessários para reproduzir os exemplos do livro. 1.2 Instalação do R Para começarmos a trabalhar com o R é necessário baixá-lo na página do R Project. Então, acesse esse site e em seguida, clique no link download R. Figura 1.1: Página do R project indicando o link para download do programa Esse link o levará à página do CRAN Mirrors (Comprehensive R Archive Network). Escolha a página espelho do Brasil mais próxima de você para baixar o programa. Figura 1.2: Página do R project mostrando os espelhos distribuídos em diferentes países Escolha agora o sistema operacional do seu computador (passos adicionais existem para diferentes distribuições Linux). Figura 1.3: Página do R project indiciando os sistemas operacionais disponíveis. Selecionamos a opção do Windows. Agora clique em base para finalmente chegar à página de download com a versão mais recente do R. Figura 1.4: Página do R project indicando os passos para instalação do programa. 1.3 Instalação do RStudio O RStudio possui algumas características que o tornam popular: várias janelas de visualização, marcação e preenchimento automático do script, integração com controle de versão, dentre outras funcionalidades. Para fazer o download do RStudio, acessamos o site, e clique em download. Figura 1.5: Página inicial do R Studio indicando o local de download. Escolhemos a versão gratuita. Figura 1.6: Página do R Studio para download do programa. Escolhemos o instalador com base em nosso sistema operacional. Figura 1.7: Página do R Studio para instalação do programa. 1.4 Versão do R Todas os comandos, pacotes e análises disponibilizados no livro foram realizos no Programa R versão 4.1.1 (10-08-2021). 1.5 Pacotes Descrevemos no Capítulo 4 o que são e como instalar os pacotes para realizar as análises estatísticas no R.  Importante: Criamos o pacote ecodados que contem todas as informações e dados utilizados neste livro. Assim, recomendamos que você instale e carregue este pacote no início de cada capítulo, para ter acesso aos dados necessários para executar as funções no R. Abaixo, listamos todos os pacotes que foram utilizados em alguma das análises descritas no livro. Você pode instalar os pacotes agora ou esperar para instalá-los quando ler o @ref[cap4] e entender o que são as funções install.packages(), library() e install_github(). install.packages(c(&quot;ade4&quot;, &quot;adespatial&quot;, &quot;ape&quot;, &quot;bbmle&quot;, &quot;betapart&quot;, &quot;BiodiversityR&quot;, &quot;car&quot;, &quot;cati&quot;, &quot;datasauRus&quot;, &quot;devtools&quot;, &quot;DHARMa&quot;, &quot;dplyr&quot;, &quot;emmeans&quot;, &quot;factoextra&quot;, &quot;FactoMineR&quot;, &quot;fasterize&quot;, &quot;FD&quot;, &quot;forcats&quot;, &quot;geobr&quot;, &quot;GGally&quot;, &quot;ggExtra&quot;, &quot;ggforce&quot;, &quot;ggord&quot;, &quot;ggplot2&quot;, &quot;ggpubr&quot;, &quot;ggrepel&quot;, &quot;ggspatial&quot;, &quot;glmmTMB&quot;, &quot;grid&quot;, &quot;gridExtra&quot;, &quot;here&quot;, &quot;hillR&quot;, &quot;iNEXT&quot;, &quot;janitor&quot;, &quot;kableExtra&quot;, &quot;knitr&quot;, &quot;labdsv&quot;, &quot;lattice&quot;, &quot;leaflet&quot;, &quot;lmtest&quot;, &quot;lsmeans&quot;, &quot;lubridate&quot;, &quot;mapview&quot;, &quot;MASS&quot;, &quot;MuMIn&quot;, &quot;nlme&quot;, &quot;ordinal&quot;, &quot;palmerpenguins&quot;, &quot;performance&quot;, &quot;pez&quot;, &quot;phyloregion&quot;, &quot;phytools&quot;, &quot;picante&quot;, &quot;piecewiseSEM&quot;, &quot;purrr&quot;, &quot;pvclust&quot;, &quot;raster&quot;, &quot;readr&quot;, &quot;reshape2&quot;, &quot;rgdal&quot; , &quot;rnaturalearth&quot;, &quot;RVAideMemoire&quot;, &quot;sciplot&quot;, &quot;sf&quot;, &quot;sidrar&quot;, &quot;sjPlot&quot;, &quot;spData&quot;, &quot;spdep&quot;, &quot;stringr&quot;, &quot;SYNCSA&quot;, &quot;tibble&quot;, &quot;tidyr&quot;, &quot;tidyverse&quot;, &quot;tmap&quot;, &quot;tmaptools&quot;, &quot;TPD&quot;, &quot;vegan&quot;, &quot;viridis&quot;, &quot;visdat&quot;), dependencies = TRUE) Diferente dos pacotes anteriores que são baixados do CRAN, alguns pacotes são baixados do github dos pesquisadores responsáveis pelos pacotes. Nestes casos, precisamos carregar o pacote devtools para acessar a função install_github. Durante as instalações deste pacotes, o R irá pedir para você digitar um número indicando os pacotes que você deseja fazer update. Neste caso, digite 1 que irá indicar para ao programa que ele deve instalar todos os pacotes atualizados. library(devtools) install_github(&quot;paternogbc/ecodados&quot;) install_github(&quot;mwpennell/geiger-v2&quot;) install_github(&quot;fawda123/ggord&quot;) install_github(&quot;jinyizju/V.PhyloMaker&quot;) 1.6 Dados A maioria dos exemplos utilizados são baseados em dados reais que já foram publicados em artigos científicos ou são dados coletados por um dos autores deste livro. Em alguns casos, os dados foram simulados para facilitar a interpretação dos resultados de algumas análises estatísticas. Todos os dados, publicados ou simulados, estão disponíveis no pacote ecodados. Além disso, em cada capítulo fazemos uma breve descrição dos dados para facilitar a compreensão sobre o que é variável resposta ou preditora, como essas variáveis estão relacionadas com as perguntas e predições do exemplo. "],["cap2.html", "Capítulo 2 Introdução 2.1 Histórico deste livro 2.2 Objetivo deste livro 2.3 O que você não encontrará neste livro 2.4 Por que usar o R? 2.5 Indo além da linguagem de progração para a Ecologia 2.6 Como usar este livro", " Capítulo 2 Introdução 2.1 Histórico deste livro Este livro foi estruturado a partir da apostila elaborada pelos pesquisadores Diogo B. Provete, Fernando R. da Silva e Thiago Gonçalves-Souza para ministrar o curso Estatística aplicada à ecologia usando o R no PPG em Biologia Animal da UNESP de São José Rio Preto/SP, em abril de 2011. Os três pesquisadores eram então alunos do PPG em Biologia Animal quando elaboraram o material disponibilizado na apostila. A proposta de transformar a apostila em livro sempre foi um tópico recorrente desde 2011, e concretizado agora, 10 anos depois. Neste período, Diogo, Fernando e Thiago foram contratados pela Universidade Federal de Mato Grosso do Sul, Universidade Federal de São Carlos campus Sorocaba, e Universidade Federal Rural de Pernambuco, respectivamente. Nestes anos eles ofertaram diferentes versões do curso Estatística aplicada à ecologia usando o R para alunos de graduação e pós-graduação em diferentes instituições do Brasil. A possibilidade da oferta destes novos cursos fortaleceu a ideia de trasformar a apostila em um livro com base nas experiências dos pesquisadores em sala de aula. Considerando que novas abordagens ecológicas vêm sendo descritas e criadas a uma taxa elevada nos últimos anos, era de se esperar que as informações disponíveis na apostila estivessem defasadas após 10 anos. Por este motivo, Diogo, Fernando e Thiago convidaram outros dois pesquisadores, Gustavo B. Paterno da Georg-August-University of Göttingen e Maurício H. Vancine do PPG em Ecologia, Evolução e Biodiversidade da UNESP Câmpus de Rio Claro, que são referências no uso de estatística em ecologia usando o R. Com o time completo, passaram mais de um ano realizando reuniões, compartilhando scripts e pagando cerveja para os coautores por capítulos atrasados até chegarem neste primeira versão do livro. 2.2 Objetivo deste livro Nossa proposta com este livro é de traçar o melhor caminho (pelo menos do nosso ponto de vista) entre questões ecológicas e os métodos estatísticos mais robustos para testá-las. Guiar seus passos nesse caminho (nem sempre linear) necessita que você utilize um requisito básico: o de utilizar seu esforço para caminhar. O nosso esforço, em contrapartida, será o de indicar as melhores direções para que você adquira certa independência em análises ecológicas. Um dos nossos objetivos é mostrar que o conhecimento de teorias ecológicas e a utilização de questões apropriadas são o primeiro passo na caminhada rumo à compreensão da lógica estatística. Não deixe que a estatística se torne a pedra no seu caminho. Em nossa opinião, programas com ambiente de programação favorecem o entendimento da lógica estatística, uma vez que cada passo (lembre-se de que você está caminhado em uma estrada desconhecida e cheia de pedras) precisa ser coordenado, ou seja, as linhas de comando (detalhes abaixo) precisam ser compreendidas para que você teste suas hipóteses. A primeira parte deste livro pretende utilizar uma estratégia que facilita a escolha do teste estatístico apropriado, por meio da seleção de questões/hipóteses claras e da ligação dessas hipóteses com a teoria e o método (veja Figura 3.1 no Capítulo 3). Enfatizamos que é fundamental ter em mente aonde se quer chegar, para poder escolher o que deve ser feito. Posteriormente à escolha de suas questões, é necessário transferir o contexto ecológico para um contexto meramente estatístico (hipótese nula/alternativa). A partir da definição de uma hipótese nula, partiremos para a aplicação de cada teste estatístico (de modelos lineares generalizados à análises multivariadas) utilizando a linguagem R. Antes de detalhar cada análise estatística, apresentaremos os comandos básicos para a utilização da linguagem R e os tipos de distribuição estatística que são essenciais para a compreensão dos testes estatísticos. Para isso, organizamos um esquema que chamamos de estrutura lógica que facilita a compreensão dos passos necessários para testar suas hipóteses (veja Figura 3.2 no Capítulo 3). 2.3 O que você não encontrará neste livro Aprofundamento teórico, detalhes matemáticos, e explicação dos algoritmos são informações que infelizmente não serão abordadas neste livro. O foco aqui é a explicação de como cada teste funciona (teoria e procedimentos matemáticos básicos) e sua aplicação em testes ecológicos usando scripts na linguagem R. Para tanto, o livro de Pierre e Louis Legendre (P. Legendre and Legendre 2012b) é uma leitura que permite o aprofundamento de cada uma das análises multivariadas propostas aqui. Além disso, são de fundamental importância para o amadurecimento em análises ecológicas as seguintes leituras: Manly (1991), Pinheiro and Bates (2000b), Scheiner and Gurevitch (2001), Burnham and Anderson (2014a), Quinn and Keough (2002), Venables and Ripley (2002), Magurran and McGill (2011), N. J. Gotelli and Ellison (2013), Zar (2010), Zuur, Ieno, and Elphick (2009a), Crawley (2012) e James et al. (2013). 2.4 Por que usar o R? Os criadores do R o chamam de uma linguagem e ambiente de programação estatística e gráfica (Venables and Ripley 2002). A linguagem R também é chamada de programação orientada ao objeto (object oriented programming), o que significa que utilizar o R envolve basicamente a criação e manipulação de objetos em uma tela branca, em que o usuário tem de dizer exatamente o que deseja que o programa execute, ao invés de simplesmente clicar em botões. E vem daí uma das grandes vantagens em se usar o R: o usuário tem total controle sobre o que está acontecendo e também tem de compreender o que deseja antes de executar uma análise. Além disso, o R permite integração com outros programas escritos em C++, Python e Java, permitindo que os usuários possam aplicar novas metodologias sem ter que aprender novas linguagens. Na página pessoal do Prof. Nicolas J. Gotelli (link), existem vários conselhos para um estudante iniciante de ecologia. Dentre esses conselhos, o Prof. Gotelli menciona que o domínio de uma linguagem de programação é uma das habilidades mais importantes, porque dá liberdade ao ecólogo para executar tarefas que vão além daquelas disponíveis em pacotes estatísticos comerciais. Além disso, a maioria das novas análises propostas nos mais reconhecidos periódicos em ecologia normalmente são implementadas em linguagem R, e os autores incluem normalmente o código fonte no material suplementar dos artigos, tornando a análise acessível. A partir do momento que essas análises ficam disponíveis (seja por código fornecido pelo autor ou por implementação em pacotes pré-existentes), é mais simples entendermos a lógica de análises complexas, especialmente as multivariadas, utilizando nossos próprios dados, realizando-as passo a passo. Sem a utilização do R, normalmente temos que contatar os autores que nem sempre são tão acessíveis. Uma última vantagem é que por ser um software livre, a citação do R em artigos é permitida e até aconselhável. Para saber como citar o R, digite citation() na linha de comando. Para citar um pacote específico, digite citation() com o nome do pacote entre aspas dentro dos parênteses. Neste ponto, esperamos ter convencido você leitor, de que aprender a utilizar o R tem inúmeras vantagens. Entretanto, provavelmente vai ser difícil no começo, mas continue e perceberá que o investimento vai valer à pena no futuro. 2.5 Indo além da linguagem de progração para a Ecologia Um ponto em comum em que todos os autores deste livro concordaram em conversas durante sua estruturação, foi a dificuldade que todos tivemos quando estávamos aprendendo a linguagem: Como transcrever os objetivos (manipulação de dados, análises e gráficos) em linguagem R Como interpretar os resultados das análises estatísticas do R para os objetivos ecológicos Num primeiro momento, quando estamos aprendendo a linguagem R é muito desafiador pensar em como estruturar nossos códigos para que eles façam o que precisamos: importar dados, selecionar linhas ou colunas, qual pacote ou função usar para uma certa análise ou como fazer um gráfico que nas nossas anotações são simples, mas no código parece impossível. Bem, não há um caminho fácil nesse sentido e ele depende muito da experiência e familiaridade adquirida com o tempo de uso da linguagem, assim como outra língua qualquer, como inglês ou espanhol. Entretanto, uma dica pode ajudar: estruture seus códigos antes de partir para o R. Num papel escreva os pontos que quer que seus códigos façam, como se estivesse explicando para alguém os passos que precisa para realizar as tarefas. Depois disso, transcreva para o script (iremos explicar esse conceito no @[cap4]) esse texto. Por fim, traduza isso em linguam R. Pode parecer massante e cansativo no começo, mas isso o ajudará a ter maior domínio da linguagem, sendo que esse passo se torna desnecessário quando se adquire bastante experiência. Uma vez transposta esse barreira inicial e temos os resultados de nossas análises (valores de estatísticas, parâmetros estimados, valores de p e R², etc.), com gráficos e outras figuras que precisamos, como interpretamos à luz da teoria ecológica? Esse ponto é talvez um dos mais complicados. Com o tempo, ter um valor final de uma estatística ou gráfico à partir da linaguagem R é relativamente simples, mas o que esse valor ou gráfico significam para nossa hipótese ecológica é o ponto mais complicado. Essa dificuldade por ser por inexperiência teórica (ainda não lemos muito sobre um aspecto ecológico) ou inexperiência científica (ainda temos dificuldade para expandir nossos argumentos de forma indutiva). Destacamos esse ponto porque ele é fundamental no processo científico e talvez seja o principal aspecto que diferencia os cientistas de outros profissionais: sua capacidade de entendimento dos padrões à partir dos processos e mecanismos atrelados. Nesse ponto, quase sempre recorremos à nossos orientadores ou colegas mais experientes para nos ajudar, mas é natural e faz parte do processo de aprendizado de uso da linguagem R junto à Ecologia como ciência. Entretanto, contrapomos a importância dessa extrapolação para não nos tornarmos apenas especialistas em linguagem R sem a fundamental capacidade de entendimento do sistema ecológico que estamos estudando. 2.6 Como usar este livro Os conteúdos apresentados em cada capítulo são independentes entre si. Portanto, você pode utilizar este livro de duas formas. A primeira é seguir uma ordem sequencial (capítulos 1, 2, 3, ) que recomendamos, principalmente, para as pessoas que não possuem familiaridade com a linguagem R. A segunda forma, é selecionar o capítulo que contém a análise de seu interesse e mudar de um capítulo para outro sem seguir a sequência apresentada no livro. Com exceção dos capítulos 3, 4, 5 e 15, os outros capítulos foram elaborados seguindo a mesma estrutura contendo uma descrição da análise estatística (aspecto teóricos) e exemplos relacionados com questões ecólogicas que podem ser respondidas por esta análise. Todos os exemplos são compostos por: i) uma descrição dos dados utilizados, ii) pergunta e predição do trabalho, iii) descrição das variáveis resposta(s) e preditora(s), e iv) descrição e explicação das linhas de comando do R necessárias para realização das análises. Os exemplos utilizados são baseados em dados reais que já foram publicados em artigos científicos ou são dados coletados por um dos autores deste livro. Nós recomendamos que primeiro você utilize estes exemplos para se familiarizar com as análises e a formatação das linhas e colunas das planilhas. Em seguida, utilize seus próprios dados para realizar as análises. Esta é a melhor maneira de se familiarizar com as linhas de comando do R. Muitas das métricas ou índices apresentados neste livro não foram traduzidas para o português, porque seus acrônimos são clássicos e bem estabelecidos na literatura ecológica. Nestes casos, consideramos que a tradução poderia confundir as pessoas que estão começando a se familiarizar com a literatura específica. Realçamos que não estamos abordando todas as possibilidades disponíveis, e existem muito outros pacotes e funções no R que realizam as mesmas análises. Contudo, esperamos que o conteúdo apresentado permita que os leitores adquiram independência e segurança, para que possam caminhar sozinhos na exploração de novos pacotes e funções para responderem suas perguntas biológicas e ecológicas. Referências "],["cap3.html", "Capítulo 3 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes 3.1 Introdução 3.2 Perguntas devem preceder as análises estatísticas 3.3 Fluxograma: Conectando Variáveis para Melhorar o desenho experimental e as análises estatísticas 3.4 Questões fundamentais em etnobiologia, ecologia e conservação 3.5 Considerações Finais 3.6 Referências", " Capítulo 3 Voltando ao básico: como dominar a arte de fazer perguntas cientificamente relevantes Capítulo originalmente publicado por Gonçalves-Souza, Provete, Garey, Silva &amp; Albuquerque (2019), in Methods and Techniques in Ethnobiology and Ethnoecology (tradução autorizada por Springer). 3.1 Introdução Aquele que ama a prática sem teoria é como um marinheiro que embarca em um barco sem um leme e uma bússola e nunca sabe onde pode atracar - Leonardo da Vinci. Qual é a sua pergunta? Talvez esta seja a frase que pesquisadores mais jovens ouvem quando começam suas atividades científicas. Apesar de aparentemente simples, responder a esta pergunta se torna um dos maiores desafios da formação científica. Seja na pesquisa quantitativa ou qualitativa, todo processo de busca de conhecimento parte de uma questão/problema formulada pelo pesquisador no início desse processo. Esta questão guiará o pesquisador em todas as etapas da pesquisa. No caso específico de pesquisa quantitativa, a questão é a porta de entrada de uma das formas mais poderosas de pensar cientificamente: o método hipotético-dedutivo (MHD) definido por Karl Popper (1959). Este capítulo propõe uma maneira de pensar sobre hipóteses (geradas dentro do MHD) para melhorar o pensamento estatístico usando um fluxograma que relaciona variáveis por ligações causais. Além disso, argumentamos que você pode facilmente usar fluxogramas para (1) identificar variáveis relevantes e como elas afetam umas às outras; (2) melhorar (quando necessário) o desenho experimental/observacional; (3) facilitar a escolha de análises estatísticas; e (4) melhorar a interpretação e comunicação dos dados e análises. 3.2 Perguntas devem preceder as análises estatísticas 3.2.1 Um bestiário1 para o teste de hipóteses (Você está fazendo a pergunta certa?) A maioria dos alunos e professores de ciências biológicas possuem aversão à palavra estatística. Não surpreendentemente, enquanto a maioria das disciplinas acadêmicas que compõem o STEM (termo em inglês para aglomerar Ciência, Tecnologia, Engenharia e Matemática) têm uma sólida formação estatística durante a graduação, cursos de ciências biológicas têm um currículo fraco ao integrar o pensamento estatístico dentro de um contexto biológico (Metz 2008). Esses cursos têm sido frequentemente ministrados sem qualquer abordagem prática para integrar os alunos em uma plataforma de solução de problemas (Horgan et al. 1999). Infelizmente, a Etnobiologia, Ecologia e Conservação (daqui em diante EEC) não são exceções. Talvez mais importante, uma grande preocupação durante o treinamento estatístico de estudantes de EEC é a necessidade de trabalhar com problemas complexos e multidemensionais que exigem soluções analíticas ainda mais complicadas para um público sem experiência em estatística e matemática. Por este motivo, muitos pesquisadores consideram a estatística como a parte mais problemática de sua pesquisa científica. Argumentamos neste capítulo que a dificuldade de usar estatística em EEC está associada à ausência de uma plataforma de solução de problemas gerando hipóteses claras que são derivadas de uma teoria. No entanto, concordamos que há um grande desafio em algumas disciplinas como a Etnobiologia para integrar esta abordagem direcionada por hipóteses, uma vez que foi introduzida apenas recentemente [veja Phillips and Gentry (1993); phillips_useful_1993-1; U. P. Albuquerque and Hanazaki (2009)]. Devido à falta de uma plataforma de solução de problemas, frequentemente percebemos que alunos/pesquisadores na EEC geralmente têm dificuldades de responder perguntas básicas para uma pesquisa científica, tais como: Qual é a principal teoria ou raciocínio lógico do seu estudo? Qual é a questão principal do seu estudo? Qual é a sua hipótese? Quais são suas predições? Qual é a unidade amostral, variável independente e dependente de seu trabalho? Existe alguma covariável? Qual é o grupo controle? Como selecionar qualquer teste estatístico sem responder a essas cinco perguntas? A estrutura estatística frequentista fornece uma maneira de ir progressivamente suportando ou falseando uma hipótese (Neyman and Pearson 1933; Popper 1959). A decisão de rejeitar uma hipótese nula é feita usando um valor de probabilidade (geralmente P &lt; 0,05) calculado pela comparação de eventos observados com observações repetidas obtidas a partir de uma distribuição nula. Agora, vamos ensinar através de um exemplo e apresentar um guia para o pensamento estatístico que conecta alguns elementos essenciais para executar qualquer análise multivariada (ou univariada) Underwood (1997). Primeiro, imagine que você observou os seguintes fenômenos na natureza: (1) indivíduos de uma população tradicional selecionar algumas plantas para fins médicos e (2)manchas monodominantes da árvore Prosopis juliflora, uma espécie invasora em várias regiões. Do lado da etnobiologia, para entender como e porque o conhecimento tradicional é construído, existe uma teoria ou hipótese (por exemplo, hipótese de aparência: Gonçalves, Albuquerque, and Medeiros 2016) explicando os principais processos que ditam a seleção da planta (Fig. 3.1a).Então, você pode fazer uma ou mais perguntas relacionadas àquele fenômeno observado (Fig. 3.1b). Por exemplo, como a urbanização afeta o conhecimento das pessoas sobre o uso de plantas medicinais em diferentes biomas? Do lado ecológico/conservação, para entender por que espécies introduzidas afetam as espécies nativas locais, você precisa entender as teorias do nicho ecológico e evolutiva (MacDougall, Gilbert, and Levine 2009; Saul and Jeschke 2015). Você pode perguntar, por exemplo, como as plantas exóticas afetam a estrutura de comunidades de plantas nativas? Questões complexas ou vagas dificultam a construção do fluxograma de pesquisa (ver descrição abaixo) e a seleção de testes estatísticos. Em vez disso, uma pergunta útil deve indicar as variáveis relevantes do seu estudo, como as independentes e dependentes, covariáveis, unidade de amostral e a escala espacial de interesse (Fig. 3.1b). No exemplo etnobiológico fornecido, a urbanização e o conhecimento das pessoas são as variáveis independentes e dependentes, respectivamente. Além disso, este estudo tem uma escala ampla, pois compara biomas diferentes. A próxima etapa é construir a hipótese biológica (Fig. 3.1c), que indicará a associação entre variáveis independentes e dependentes. No exemplo etnobiológico, a hipótese é que (1) a urbanização afeta o conhecimento das pessoas sobre o uso de plantas medicinais, enquanto a hipótese ecológica é que (2) espécies exóticas afetam a estrutura de comunidades de plantas nativas. Observe que isso é muito semelhante à questão principal. Mas você pode ter múltiplas hipóteses (Platt 1964) derivado de uma teoria. Depois de selecionar a hipótese biológica (ou científica), é hora de pensar sobre a derivação lógica da hipótese, que é chamada de predição ou previsão (Fig. 3.1d). Os padrões preditos são uma etapa muito importante, pois após defini-los você pode operacionalizar suas variáveis e visualizar seus dados. Por exemplo, a variável teórica Urbanização pode ser medida como grau de urbanização ao longo das áreas urbanas, periurbanas e rurais e conhecimento das pessoas como o número e tipo de espécies de plantas úteis usadas para diferentes doenças. Assim, a predição é que o grau de urbanização diminua o número e tipo de espécies de plantas conhecidas utilizadas para fins medicinais. No exemplo ecológico, a variável espécies exóticas pode ser medida como a densidade da planta exótica Prosopis juliflora e Estrutura da comunidade como riqueza e composição de espécies nativas. Depois de operacionalizar o seu trabalho à luz do método hipotético-dedutivo (HDM), o próximo passo é pensar estatisticamente sobre a hipótese biológica formulada (ver Figura 3.1 e, f). Figura 3.1: Um guia para o pensamento estatístico combinando o método hipotético-dedutivo (a  d, i) e estatística frequentista (e  i). Veja também a Fig. 1 em Underwood 1997, Fig. 1 em Ford 2004 e Fig. 1.3 em Legendre &amp; Legendre 2012. Então, você precisa definir as hipótese estatística nula (H0) e a alternativa (H1). Duas hipóteses estatísticas diferentes podem ser derivadas de uma hipótese biológica (Fig. 3.1e). Portanto, nós usamos o termo hipótese estatística entre aspas, porque as chamadas hipóteses estatísticas são predições sensu stricto, e muitas vezes confundem jovens estudantes. A hipótese estatística nula representa uma ausência de relacão entre as variáveis independentese e dependentes. Depois de definir a hipótese estatística nula, você pode derivar uma ou várias hipóteses estatísticas alternativas, que demonstram a(s) associação(ões) esperada(s) entre suas variáveis (Fig. 3.1e). Em nosso exemplo, a hipótese nula é que o grau de urbanização não afeta o número de espécies de plantas úteis conhecidas pela população local. Por sua vez, a hipótese alternativa é que o grau de urbanização afeta o número de espécies de plantas úteis conhecidas pela população local. Depois de operacionalizar suas variáveis e definir o valor nulo e hipóteses alternativas, é hora de visualizar o resultado esperado (Fig. 3.2, Caixa 1) e escolher um método estatístico adequado. Por exemplo, se você deseja comparar a diferença na composição de plantas úteis entre áreas urbanas, periurbanas e rurais, você pode executar uma PERMANOVA (Gonçalves-Souza, Garey, et al. 2019) que usa uma estatística de teste chamada pseudo-F. Então, você deve escolher o limite de probabilidade (o valor P) do teste estatístico para decidir se a hipótese nula deve ou não deve ser rejeitada (Nicholas J. Gotelli and Ellison 2012). Se você encontrar um P &lt; 0,05, você deve rejeitar a hipótese estatística nula (urbanização não afeta o número e a composição das plantas). Por outro lado, um P &gt; 0,05 indica que você não pode rejeitar a hipótese nula estatística. Assim, a estatística do teste e o valor P representam a última parte do teste de hipótese estatística, que é a decisão e conclusões apropriadas que serão usadas para retroalimentar a teoria principal (Figura 3.1g  i). Generalizando seus resultados e falseando (ou não) suas hipóteses, o estudo busca refinar a construção conceitual da teoria, que muda constantemente (Fig. 1i, Ford 2004). No entanto, há um ponto crítico nesta última frase, porque a significância estatística não significa necessariamente relevância biológica Martínez-Abraín (2008). Nas palavras de Ford (2004): as estatísticas são usadas para iluminar o problema, e não para apoiar uma posição. Além disso, o procedimento de teste de hipótese tem alguma incerteza, que pode influenciar resultados falso-positivos (erro tipo 1) e falso-negativos (erro tipo 2) (Whitlock and Schluter 2015). Para simplificar, não discutiremos em detalhes os prós e contras da estatística frequentista, bem como métodos alternativos (por exemplo, Bayesiano e Máxima Verossimilhança), e questões filosóficas relativas ao valor P (para uma discussão sobre esses tópicos, consulte o fórum em Ellison et al. 2014). Caixa 1. Tipo de variáveis e visualização de dados Conforme descrito na Seção 3, o fluxograma é essencial para conectar variáveis relevantes para a pesquisa. Para aproveitar ao máximo esta abordagem, você pode desenhar suas próprias predições gráficas para te ajudar a pensar sobre diferentes possibilidades analíticas. Aqui, nós fornecemos uma descrição completa dos tipos de variáveis que você deve saber antes de executar qualquer análise estatística e representar seus resultados. Além disso, mostramos uma breve galeria (Fig. 3.2) com exemplos de boas práticas em visualização de dados (Fig. 3.3b, veja também figuras em Gonçalves-Souza, Garey, et al. 2019). Além de conectar diferentes variáveis no fluxograma, você deve distinguir o tipo de variável. Primeiro você deve identificar as variáveis independentes (também conhecidos como explicativas ou preditoras) e dependentes (também conhecidas como resposta). A variável independente é aquela (ou aquelas) que prevê ou afeta a variável resposta (por exemplo, a fertilidade do solo é a variável independente capaz de afetar a abundância de uma espécie de planta focal, a variável dependente). Além disso, uma covariável é uma variável contínua que pode afetar tanto a variável resposta quanto a independente (ou ambos), mas geralmente não é do interesse do pesquisador. Depois de definir as variáveis relevantes, conectando-as no fluxograma, é hora de diferenciar seu tipo: (1) quantitativa ou contínua, e (2) categórica ou qualitativa (Fig. 3.2a, Caixa 1). O tipo de variável irá definir que tipo de figura você pode selecionar. Por exemplo, se você está comparando duas variáveis contínuas ou uma variável contínua e uma binária, a melhor maneira de visualizá-los (Fig. 3.2b) é um gráfico de dispersão (Fig. 3.2c, d). A linha representa os valores preditos pelo modelo estatístico usado (por exemplo, linear, logístico). Se você está interessado em comparar a gama de diferentes atributos (ou a descrição de qualquer variável numérica) entre as variáveis categóricas (por exemplo, espécies ou populações locais), um gráfico de halteres (do inglês Dumbbell plot) é uma boa opção (Fig. 3.2e). Histogramas também podem ser usados para mostrar a distribuição de duas variáveis contínuas de dois grupos ou fatores (Fig. 3.2f). No entanto, se você quiser testar o efeito de uma variável categórica independente (como em um desenho de ANOVA) sobre uma variável dependente, boxplots (Fig. 3.2g) ou gráficos de violino podem resumir essas relações de maneira elegante. Conjuntos de dados multivariados, por sua vez, podem ser visualizados com ordenação (Fig. 3.2h) ou gráficos de agrupamento (não mostrados). Existe um site abrangente apresentando várias maneiras de visualizar dados chamado https://www.datavizproject.com/. Figura 3.2: (A) Tipos de variáveis e (B) visualização de dados para representar a relação entre variáveis independentes e dependentes ou covariáveis. 3.3 Fluxograma: Conectando Variáveis para Melhorar o desenho experimental e as análises estatísticas McIntosh e Pontius (2017) afirmaram que o pensamento estatístico (representado na Fig. 3.1 inclui quatro etapas importantes: (1) quais perguntas você investigaria (Seção 4), (2) como e onde coletar os dados (Ruxton and Colegrave 2016), (3) quais fatores devem ser considerados e como eles afetam suas variáveis de interesse (e como elas afetam umas às outras), e (4) qual análise estatística você deve usar e como interpretar e comunicar os resultados (Seção 4). No entanto, a etapa (3) deve ser feita antes de coletar os dados. Por exemplo, se você está interessado na investigação dos benefícios das matas ciliares para as espécies nativas de peixes, quais variáveis devem ser incluídas no estudo? Se você escolher rios com e sem mata ciliar como única variável preditora, seu projeto de amostragem irá omitir outras variáveis de confusão, como ordem do rio e carbono orgânico do solo a montante. Vellend (2016) nomeou este problema como o problema de três caixas (ver também Ruxton and Colegrave 2016) , que se refere à limitação em inferir que X (variável independente) causa variação em Y (variável depende) quando outras variáveis criam ou ampliam a correlação entre X e Y (ver Fig. 2 em Ruxton and Colegrave 2016). Uma ferramenta útil para compreender a relação entre todas as variáveis relevantes do seu estudo é um fluxograma. No fluxograma de pesquisa [ver também magnusson_statistics_2015] proposto aqui, variáveis dependentes (também conhecidas como resposta) e independentes (ou preditora), bem como covariáveis são representadas como caixas (com formas distintas: Fig. 3.3). Além disso, você pode usar uma seta para representar uma (possível) via causal indicando força e sinal (positivo ou negativo) da variável preditora na variável dependente (Fig. 3.3) Ao fazer isso, você pode melhorar o desenho experimental ou observacional incluindo ou controlando variáveis de confusão o que, por sua vez, pode ajudar a separar a contribuição relativa de diferentes variáveis preditoras em seu sistema. Mais importante, fazer conexões entre variáveis melhora sua capacidade de visualizar o Quadro geral de sua pesquisa, o que pode afetar seu experimento, análise estatística e revisão da literatura. Na verdade, Arlidge et al. (2017)argumentam que fluxogramas facilitam a construção de narrativas, melhorando: (1) a definição de múltiplas hipóteses, (2) coleta, interpretação e disseminação de dados e (3) a comunicação do conteúdo do estudo. Você também pode ler o livro de Magnusson et al. (2015) para entender mais como usar fluxogramas para auxiliar análises estatísticas. Além disso, Ford (2004) recomenda o uso de uma abordagem analítica para fomentar o desenvolvimento da pesquisa. Além disso, o fluxograma de pesquisa pode ser usado como uma ferramenta forte para contemplar os conselhos de Ford (2004), que foram: (1) definir a pergunta da pesquisa, (2) definir a teoria a ser usada, (3) definir a técnica de investigação (por exemplo, experimento, observação de campo), (4) definir as medições, (5) definir como fazer inferência, e (6) interpretar, generalizar,e sintetizar a partir de dados que, por sua vez, são usados para refinar a teoria e modificar (quando necessário) questões futuras (Fig. 3.1). Figura 3.3: Exemplo de como usar um fluxograma para melhorar o entendimento do sistema estudado. A pergunta teórica Qual é o impacto da invasão na comunidade nativa e nas propriedades do ecossistema? pode gerar duas predições: (1) a planta exótica Prosopis juliflora reduz a diversidade beta de comunidades de plantas nativas, e (2) Prosopis juliflora modifica a composição das comunidades de plantas e reduz o estoque de carbono e as taxas de decomposição. Após selecionar suas predições, você pode construir um fluxograma conectando as variáveis relevantes e as associações entre elas. Além disso, você pode usar as informações na Caixa 1 para identificar que tipo de variável você irá coletar e quais figuras podem ser usadas (b). 3.4 Questões fundamentais em etnobiologia, ecologia e conservação As teorias são generalizações. As teorias contêm perguntas. Para algumas teorias, as perguntas são explícitas e representam o que a teoria pretende explicar. Para outras, as questões são implícitas e se relacionam com a quantidade e tipo de generalização, dada a escolha de métodos e exemplos usados por pesquisadores na construção da teoria. As teorias mudam continuamente, à medida que exceções são encontradas às suas generalizações e como questões implícitas sobre método e opções de estudos são expostas. - E. David Ford (2004) Como argumentamos antes, uma questão relevante e testável precede as análises estatísticas. Assim, apresentamos a seguir 12 questões que podem estimular pesquisas futuras na ECC. Observe, no entanto, que não queremos dizer que eles são as únicas questões relevantes a serem testadas na EEC (ver, por exemplo, Sutherland et al. (2013) para uma avaliação completa da pesquisa de ponta em Ecologia; e Caixa 6.1 em Pickett et al. (2007)2). Especificamente, essas questões são muito amplas e podem ser desenvolvidas em perguntas, hipóteses e predições mais restritas. Depois de cada questão teórica, apresentamos um estudo que testou essas hipóteses bem como as variáveis relevantes que podem estimular estudos futuros. Como o uso da terra afeta a manutenção da biodiversidade e a distribuição de espécies em diferentes escalas espaciais? Exemplo: Vários estudos em diferentes ecossistemas e escalas investigaram como o uso da terra afeta a biodiversidade. No entanto, destacamos um estudo comparando os efeitos globais do uso da terra (por exemplo, densidade populacional humana, paisagem para usos humanos, tempo desde a conversão da floresta) em espécies terrestres (por exemplo, mudança líquida na riqueza local, dissimilaridade composicional média) (Newbold et al. 2015). Qual é o impacto da invasão biótica nas comunidades nativas e propriedades do ecossistema? Exemplo: Investigar como o estabelecimento de espécies exóticas afetam a riqueza de espécies do receptor, comunidades nativas, bem como como isso afeta a entrega do serviços ecossitêmicos. Estudos anteriores controlaram a presença de espécies invasoras ou registros históricos comparados (estudos observacionais) dessas espécies e como elas impactam a biodiversidade. Além disso, há algum esforço em compreender os preditores de invasibilidade (por exemplo, produto interno bruto de regiões, densidade populacional humana, litoral continental e ilhas) Dawson et al. (2017). Como o declínio do predador de topo afeta a entrega de serviços ecossistêmicos? Exemplo: Investigar como a remoção de grandes carnívoros afeta o fornecimento de serviços ecossistêmicos, como o sequestro de carbono, doenças e controle de danos às colheitas. Estudos anteriores investigaram esta questão controlando a presença de predadores de topo ou comparando registros históricos (estudo observacionais) de espécies e vários preditores (por exemplo, perda e fragmentação de habitat, conflito entre humanos e espécies caçadas, utilização para a medicina tradicional e superexploração de presas) (Ripple et al. 2014). Como a acidificação dos oceanos afeta a produtividade primária e teias alimentares em ecossistemas marinhos? Exemplo: Estudos recentes testaram os efeitos individuais e interativos da acidificação e do aquecimento do oceano nas interações tróficas em uma teia alimentar. A acidificação e o aquecimento foram manipulados pela mudança dos níveis de CO2 e temperatura, respectivamente. Estudos anteriores demonstraram que elevação de CO2 e temperatura aumentou a produtividade primária e afetou a força do controle de cima para baixo exercido por predadores (Goldenberg et al. 2017). Como podemos reconciliar as necessidades da sociedade por recursos naturais com conservação da Natureza? Exemplo: Existe uma literatura crescente usando abordagens de paisagem para melhorar a gestão da terra para reconciliar conservação e desenvolvimento econômico. Os estudos possuem diversos objetivos, mas em geral eles usaram o engajamento das partes interessadas, apoio institucional, estruturas eficazes de governança como variáveis preditoras e melhorias ambientais (por exemplo, conservação do solo e da água, cobertura vegetal) e socioeconômicas (renda, capital social, saúde pública, emprego) como variáveis dependentes (Reed et al. 2017). Qual é o papel das áreas protegidas (UCs) para a manutenção da biodiversidade e dos serviços ecossistêmicos? Exemplo: Houve um trabalho considerável na última década comparando a eficácia das UCs para a conservação da biodiversidade. Embora esta questão não esteja completamente separada da questão anterior, o desenho dos estudos é relativamente distinto. Em geral, os pesquisadores contrastam o número de espécies e o fornecimento de serviços ecossistêmicos (por exemplo, retenção de água e solo, sequestro de carbono) entre áreas legalmente protegidas (UCs) e não protegidas (Xu et al. 2017). Como integrar o conhecimento científico e das pessoas locais para mitigar os impactos negativos das mudanças climáticas e do uso da terra na biodiversidade? Exemplo: Eventos climáticos extremos podem ter forte impacto sobre rendimento agrícola e produção de alimentos. Autores recentes têm argumentado que esse efeito pode ser mais forte para os pequenos agricultores. Estudos futuros podem investigar como a precipitação e a temperatura afetam o rendimento agrícola e como os agricultores tradicionais ou indígenas lidam com esse impacto negativo. Sistemas de agricultura tradicional têm menor erosão do solo e emissões de N2O / CO2 do que as monoculturas e, portanto, podem ser vistos como uma atividade de mitigação viável em um mundo em constante mudança (Niggli et al. 2009; Altieri and Nicholls 2017). Como as mudanças climáticas afetam a resiliência e estratégias adaptativas em sistemas socioecológicos? Exemplo: A mudança do clima altera tanto a pesca quanto a agricultura em todo o mundo, o que por sua vez obriga os humanos a mudar suas estratégias de cultivo. Estudos recentes têm argumentado que a agricultura em alguns países enfrentará riscos com as mudanças climáticas. Esses estudos comparam diferentes sistemas de produção, de agricultura convencional a outros tipos empregados por populações locais. Por exemplo, há uma forte conexão entre (1) espécies ameaçadas e sobrepesca, (2) índice de desenvolvimento humano (IDH) e dependência média da pesca e aquicultura. Além disso, há evidências de que a biodiversidade pode amortecer os impactos das mudanças climáticas aumentando a resiliência da terra [Niggli et al. (2009); Altieri and Nicholls (2017); blanchard_linked_2017]. Uma abordagem interessante é investigar como as populações locais lidam com esses desafios em termos de percepções e comportamento. Como a invasão biológica afeta espacial e temporalmente a estrutura e funcionalidade dos sistemas sócio-ecológicos? Exemplo: Muitos estudos demonstraram que espécies invasoras têm consequências biológicas, econômicas e sociais negativas. Aqui, da mesma forma que a pergunta B, os pesquisadores controlaram a presença de espécies invasoras ou utilizaram registros históricos. No entanto, trabalhos recentes quantificam não apenas a riqueza e composição de espécies nativas, mas também atributos funcionais de animais/vegetais que afetam diretamente o fornecimento de serviços ecossistêmicos como abastecimento (comida, água), regulação (clima, controle de inundações), suporte (ciclagem de nutrientes, formação do solo) e cultural (ecoturismo, patrimônio cultural) (Chaffin et al. 2016). Mas, espécies invasoras podem provocar efeitos positivos no sistema sócio-ecológico aumentando a disponibilidade de recursos naturais, impactando como as pessoas gerenciam e usam a biodiversidade local. Qual é a relação entre as diversidades filogenética e taxonômica com a diversidade biocultural? Exemplo: Estudos recentes mostraram que existe um padrão filogenético e taxonômico nos recursos que as pessoas incorporam em seus sistemas sócio-ecológicos, especialmente em plantas medicinais. Existe uma tendência para as pessoas, em diferentes partes do mundo, para usar plantas próximas filogeneticamente para os mesmos propósitos. Aqui, os pesquisadores podem testar o quanto isso afeta a diversidade de práticas em um sistema sócio-ecológico considerando o ambiente, bem como sua estrutura e funções [C. H. Saslis-Lagoudakis et al. (2012); saslis-lagoudakis_evolution_2014]. Quais variáveis ambientais e sócio-políticas mudam a estrutura e funcionalidade dos sistemas sócio-ecológicos tropicais? Exemplo: Testar a influência das mudanças ambientais afetadas pela espécie humana (por exemplo, fogo, exploração madeireira, aquecimento) em espécies-chave e, consequentemente, como esse efeito em cascata pode afetar outras espécies e serviços ecossistêmicos (por exemplo, armazenamento de carbono, ciclo da água e dinâmica do fogo) (Lindenmayer and Sato 2018). Os atributos das espécies influenciam como as populações locais distinguem plantas ou animais úteis e não-úteis? Exemplo: Investigar se a população local possui preferência ao selecionar espécies de animais ou plantas. Você pode avaliar se grupos diferentes (por exemplo, turistas) ou populações locais (por exemplo, pescadores) selecionam espécies com base em atributos das espécies. Estudos recentes têm mostrado uma ligação potencial entre planta (por exemplo, cor, folha, floração) e pássaro (por exemplo, cor, vocalzação) e alguns serviços culturais do ecossistema, como estética, recreativa e espiritual/religiosa (Goodness et al. 2016). Como você notou, as questões eram mais teóricas e, consequentemente, você pode derivar prediões testáveis (usando variáveis) a partir delas (Figuras 1 e 3). Por exemplo, da questão Como o uso da terra afeta a manutenção da biodiversidade e distribuição de espécies em diferentes escalas? podemos derivar duas predições diferentes: (1) densidade populacional (variável operacional de uso da terra) muda a composição de espécies e reduz a riqueza de espécies na escala da paisagem (predição derivada da hipótese da homogeneização biótica: Solar et al. 2015); (2) a composição dos atributos funcionais das plantas é diferente em remanescentes florestais com diferentes matrizes (cana-de-açúcar, gado, cidade, etc.). 3.5 Considerações Finais Conte-me seus segredos E faça-me suas perguntas Oh, vamos voltar para o início Correndo em círculos, perseguindo caudas Cabeças em uma ciência à parte Ninguém disse que seria fácil () Desfazendo enígmas Questões da ciência, ciência e progresso - O Cientista, Coldplay Este é um trecho de uma música da banda britânica de rock Coldplay, do álbum de 2002 A Rush of Blood to the Head. A letra é uma comparação incrível entre a ciência e os altos e baixos de um relacionamento fadado ao fracaço. A banda traz uma mensagem surpreendentemente clara de que como cientistas, nós (deveríamos) frequentemente fazer perguntas, voltar ao início após descobrir que estávamos errados (ou não) e que corremos em círculos tentando melhorar nosso conhecimento. A banda descreveu de uma forma tão precisa o quão cíclico (mas não repetitivo) é o método científico. Como disse a canção: não é fácil, mas aprender como fazer boas perguntas é um passo essencial para a consolidação do conhecimento. Ao incluir o teste de hipótese no EEC, podemos ser mais precisos. Definitivamente, isso não significa que a ciência descritiva seja inútil. Ao contrário, o desenvolvimento da ECC e principalmente da Etnobiologia, foi construído sobre uma linha de frente descritiva, o que significa que foi valioso para a fundação da Etnobiologia como disciplina consolidada [Group (2003); stepp_advances_2005]. No entanto, estudos recentes defendem que a etnobiologia deve dialogar com disciplinas com maior respaldo teórico, como ecologia e biologia evolutiva para melhorar a pesquisa sobre biodiversidade (U. P. Albuquerque and Ferreira Júnior 2017). Por sua vez, incorporando o conhecimento local em ecologia e evolução irá certamente refinar seu próprio desenvolvimento, que em última análise beneficia a conservação biológica (C. Haris Saslis-Lagoudakis and Clarke 2013). Além disso, há uma necessidade urgente de formar jovens pesquisadores em filosofia e metodologia da ciência, bem como comunicação e produção científica (U. Albuquerque P. 2013). Como comentário final, acreditamos que a formação dos alunos em EEC precisa de uma reavaliação que necessariamente volta aos conceitos e métodos básicos. Assim, os pesquisadores podem combinar o método hipotético-dedutivo com pensamento estatístico usando um fluxograma de pesquisa para ir além da descrição básica. Termo3 Definição Pressuposto Condições necessárias para sustentar uma hipótese ou construçãoa teoria Hipótese Afirmação testável derivada ou representando vários componentes de uma teoria Mecanismo Interação direta de uma relação causal que resultaem um fenômeno Padrão Eventos repetidos, entidades recorrentes ou relações replicadasrelações observadas no tempo ou no espaço Fenômeno Um evento, entidade ou relacionamento observável Predição Uma declaração de expectativa deduzida da lógicaestrutura ou derivada da estrutura causal de um teoria Processo Um subconjunto de fenômenos em que os eventos seguem umoutro no tempo ou espaço, que pode ou não sercausalmente conectado. É causa, mecanismo ou contensão explicando um padrão 3.6 Referências Referências "],["cap4.html", "Capítulo 4 Introdução ao R Pré-requisitos do capítulo 4.1 Contextualização 4.2 R e RStudio 4.3 Funcionamento da linguagem R 4.4 Estrutura e manipulação de objetos 4.5 Exercícios 4.6 Para se aprofundar", " Capítulo 4 Introdução ao R Pré-requisitos do capítulo Pacotes e dados que serão utilizados nesse capítulo. ## Pacotes library(ecodados) library(devtools) library(knitr) 4.1 Contextualização O objetivo desta seção é apresentar os aspectos básicos da linguagem R para que qualquer pessoa possa realizar os principais passos para a análise de dados utilizando essa linguagem. Abordaremos aqui as questões básicas sobre a linguagem R, como: 1) R e RStudio, 2) funcionamento da linguagem, 3) estrutura e manipulação de objetos, 4) exercícios e 5) principais livros e material para se aprofundar nos seus estudos. Todo processo de aprendizagem torna-se mais efetivo quando a teoria é combinada com a prática, então recomendamos fortemente que você leitor(a) acompanhe os códigos e exercícios deste livro, ao mesmo tempo que os executa em seu computador e não só os leia passivamente. Além disso, se você tiver seus próprios dados é muito importante tentar executar replicar análises ou gráficos. Por motivos de espaço, não abordaremos todas as questões relacionadas ao uso da linguagem R nesta seção. Logo, aconselhamos que você consulte o material sugerido no final desta seção para se aprofundar. Este capítulo, na maioria das vezes, pode desestimular as pessoas que estão iniciando, uma vez que ele não apresenta os códigos para realizar as análises estatísticas. Contudo, ele é essencial para o entendimento e interpretação do que está sendo informado nas linhas de código, além de facilitar a manipulação dos dados antes de realizar as análises estatísticas. A leitora ou leitor vai perceber que não usará este capítulo para fazer as análises, mas voltará neste capítulo diversas vezes para relembrar qual é o código ou que significa determinada expressão ou objeto usados nos próximos capítulos. 4.2 R e RStudio Com o R, é possível manipular, analisar e visualizar dados, além de escrever desde pequenas linhas de códigos até programas inteiros. O R é a versão em código aberto de uma linguagem de programação criada por John M. Chambers (Stanford University, CA, EUA) nos anos 1980 no Bell Labs, chamada de S, que contou com três versões: Old S (1976-1987), New S (1988-1997) e S4 (1998), utilizada na IDE S-PLUS (1988-2008). Essa linguagem tornou-se bastante popular e vários produtos comerciais que a usam estão disponíveis, como o S-PLUS, SPSS, STATA e SAS. No final dos anos 1990, Robert Gentleman e Ross Ihaka (ambos da Universidade de Auckland, Nova Zelândia), iniciaram o desenvolvimento da versão free da linguagem S, com o seguinte histórico: Desenvolvimento (1997-2000), Versão 1 (2000-2004), Versão 2 (2004-2013), Versão 3 (2013-2020) e Versão 4 (2020). Para mais detalhes do histórico de desenvolvimento das linguagens S e R, consultar Wickham (2013). Atualmente a linguagem R é mantida por uma rede de colaboradores denominada R Core Team. A origem do nome R é desconhecida, mas reza a lenda que ao lançarem o nome da linguagem os autores se valeram da letra que vinha antes do S, uma vez que a linguagem R foi baseada nela e utilizaram a letra R. Outra história conta que pelo fato do nome dos dois autores iniciarem por R, batizaram a linguagem com essa letra. Um aspecto digno de nota é que a linguagem R é uma linguagem interpretada, ao contrário de outras linguagens como Fortran e C que são compiladas. Isso a faz ser mais fácil de programar, pois processa linhas de comando e as transforma em linguagem de máquina (código binário que o computador efetivamente lê), apesar desse fato diminuir a velocidade de processamento. Para começarmos a trabalhar com o R é necessário baixá-lo na página do R Project. Então, acesse esse site, e em seguida, clique no link download R, que o levará à página do CRAN Mirrors (Comprehensive R Archive Network). Os detalhes de instalação são apresentados no @ref[cap1]. Reserve algum tempo para explorar esta página do R-Project. Existem vários livros dedicados a diversos assuntos baseados no R. Além disso, estão disponíveis manuais em diversas línguas para serem baixados gratuitamente. Como o R é um software livre, não existe a possibilidade de o usuário entrar em contato com um serviço de suporte de usuários, muito comuns em softwares pagos. Ao invés disso, existem várias listas de emails que fornecem suporte à comunidade de usuários. Nós, particularmente, recomendamos o ingresso nas seguintes listas: R-help, R-sig-ecology, e R-br. Este último reúne um grupo de pessoas usuárias brasileiras do programa R. Apesar de podemos utilizar o R com o IDE (Ambiente de Desenvolvimento Integrado - Integrated Development Environment) RGui que vem com a instalação da linguagem para usuários Windows (Figura 4.1) ou no próprio terminal para usuários Linux e MacOS, existem alguns IDEs específicos para facilitar nosso uso dessa linguagem. Figura 4.1: Interface do RGui. Os números indicam: (1) R Script, (2) R Console, e (3) R Graphics. Dessa forma, nós que escrevemos utilizamos o RStudio e assumimos que você que está lendo fará o mesmo. O RStudio permite diversas personalizações, grande parte delas contidas em Tools &gt; Global options. Incentivamos as leitoras e leitores a fuçar, com certa dose de cuidado, nas opções para customização. Dentre essas mudanças, destacamos duas: Tools &gt; Global options &gt; Appearance &gt; Editor theme para escolher um tema para seu RStudio Tools &gt; Global options &gt; Code &gt; [X] Soft-wrap R source files com essa opção habilitada, quando escrevemos comentários longos ou mudamos a largura da janela que estamos trabalhando, todo o texto e o código se ajustam a janela automaticamente Um último ponto importante: para evitar possíveis erros é importante instalar primeiro o software que possui a linguagem R e depois o IDE RStudio. 4.3 Funcionamento da linguagem R Nesta seção, veremos os principais conceitos para entender como a linguagem R funciona ou como geralmente utilizamos o IDE RStudio no dia a dia, para executar nossas rotinas utilizando a linguagem R. Veremos então: 1) console, 2) script, 3) operadores, 4) objetos, 5) funções, 6) pacotes, 7) ajuda (help), 8) ambiente (environment/workspace), 9) citações e 10) principais erros. Antes de iniciarmos o uso do R pelo RStudio é fundamental entendermos alguns pontos sobre as janelas e o funcionamento delas no RStudio (Figura 4.2). Figura 4.2: Interface do RStudio. Os números indicam: (1) janela com abas de Script, R Markdown, dentre outras; (2) janela com abas de Console, Terminal e Jobs; (3) janela com abas de Environment, History, Conections e Tutorial; e (4) janela com abas de Files, Plots, Packages, Help e Viewer. Detalhando algumas dessas janelas e abas, temos: Console: painel onde os códigos são rodados e vemos as saídas Editor/Script: painel onde escrevemos nossos códigos em R, R Markdown ou outro formato Environment: painel com todos os objetos criados na sessão History: painel com o histórico dos códigos rodados Files: painel que mostra os arquivos no diretório de trabalho Plots: painel onde os gráficos são apresentados Packages: painel que lista os pacotes Help: painel onde a documentação das funções é exibida No RStudio, alguns atalhos são fundamentais para aumentar nossa produtividade: F1: abre o painel de Help quando digitado em cima de uma função Ctrl + Enter: roda a linha de código selecionada no script Ctrl + Shift + N: abre um novo script Ctrl + S: salva um script Ctrl + Z: desfaz uma operação Ctrl + Shift + Z: refaz uma operação Alt + -: insere um sinal de atribuição (&lt;-) Ctrl + Shift + M: insere um operador pipe (%&gt;%) Ctrl + Shift + C: comenta uma linha no script - insere um (#) Ctrl + Shift + R: insere uma sessão (# -) Ctrl + Shift + H: abre uma janela para selecionar o diretório de trabalho Ctrl + Shift + F10: reinicia o console Ctrl + L: limpa os códigos do console Alt + Shift + K: abre uma janela com todos os atalhos disponíveis 4.3.1 Console O console é onde a versão da linguagem R instalada é carregada para executar os códigos da linguagem R (Figura 4.2 (2)). Na janela do console aparecerá o símbolo &gt; seguida de uma barra vertical | que fica piscando, onde digitaremos ou enviaremos nossos códigos do script. Podemos fazer um pequeno exercício: vamos digitar 10 + 2, seguido da tecla Enter para que essa operação seja executada. 10 + 2 #&gt; [1] 12 O resultado retorna o valor 12, precedido de um valor entre colchetes. Esses colchetes demonstram a posição do elemento numa sequência de valores. Se fizermos essa outra operação 1:42, o R vai criar uma sequência unitária de valores de 1 a 42. A depender da largura da janela do console, vai aparecer um número diferente entre colchetes indicando sua posição na sequência: antes do 1 vai aparecer o [1], depois quando a sequência for quebrada, vai aparecer o número correspondente da posição do elemento, por exemplo, [26]. 1:42 #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #&gt; [32] 32 33 34 35 36 37 38 39 40 41 42 Podemos ver o histórico dos códigos executados no Console na aba History (Figura 4.2 (3)). 4.3.2 Scripts Scripts são rascunhos dos códigos e onde de fato os códigos são escritos e depois enviados ao console (Figura 4.2 (1)). Scripts são arquivos de texto simples, criados com a extensão (terminação) .R (ative a visualização da extensão de arquivos para ver). Para criar um script, basta ir em File &gt; New File &gt; R Script, ou clicando no ícone logo abaixo de File, ou ainda usando o atalho Ctrl + Shift + N. Também é possível usar outro editor de códigos, como o bloco de notas .txt, Sublime Text, Notepad++ e similares. Os códigos podem ser escritos nesses editores e depois salvos com a extensão .R que ao ser aberto no RStudio irão ser executados normalmente. Uma vez escrito os códigos no script podemos rodar esses códigos de duas formas: 1) todo o script de uma vez, clicando em Source ou usando o atalho Ctrl + Shift + Enter; ou 2) apenas a linha onde o cursor estiver posicionado, independente de sua posição naquela linha, clicando em Run ou usando o atalho Ctrl + Enter. Devemos sempre salvar nossos scripts, tomando por via de regra: primeiro criar o arquivo e depois ir salvando nesse mesmo arquivo a cada passo de desenvolvimento das análises (não é raro o R fechar sozinho e você perder algum tempo de trabalho). Há diversos motivos para criar um script: continuar o desenvolvimento do mesmo em outro momento ou em outro computador, preservar trabalhos passados, ou ainda compartilhar seus códigos com outra pessoa. Para criar ou salvar um script basta ir em File &gt; Save, escolher um diretório e nome para o script e salvar. Podemos ainda utilizar o atalho Ctrl + S. Em relação aos scripts, ainda há os comentários, representados pelos símbolos # (hash) ou #' (hash-linha). A diferença entre eles é que para o segundo, quando precionamos a tecla Enter o comentário #' é inserido automaticamente na linha seguinte. Linhas de códigos do script contendo comentários em seu início não são lidos pelo console do R. Se o comentário estiver no final da linha, essa linha de código ainda será lida. Os comentários são utilizados geralmente para: 1) descrever informações sobre dados ou funções e/ou 2) suprimir linhas de código. É interessante ter no início de cada script um cabeçalho identificando o objetivo ou análise, autor e data para facilitar o compartilhamento e reprodutibilidade. Os comentários podem ser inseridos ou retirados das linhas com o atalho: Ctrl + Shift + C. #&#39; --- #&#39; Título: Capítulo 04 - Introdução ao R #&#39; Autor: Maurício Vancine #&#39; Data: 20-05-2021 #&#39; --- Além disso, podemos usar comentários para adicionar informações sobre os códigos. ## Comentários # O R nao lê a linha do código depois do # (hash). 42 # Essas palavras não são executadas, apenas o 42, a resposta para questão fundamental da vida, o universo e tudo mais. #&gt; [1] 42 Por fim, outro ponto fundamental é ter boas práticas de estilo de código. Quanto mais organizado e padronizado estiver os scripts, mais fácil de entendê-los e de procurar possíveis erros. Existem dois guias de boas práticas para adequar seus scripts: Hadley Wickham e Google. Ainda temos os Code Snippets (Fragmentos de código), que são macros de texto usadas para inserir rapidamente fragmentos comuns de código. Por exemplo, o snippet fun insere uma definição de função R. Para mais detalhes, ler o artigo do RStudio: link. # fun {snippet} fun name &lt;- function(variables) { } Uma aplicação bem interessante dos Code Snippets no script é o ts. Basta digitar esse código e em seguida completar um a tecla Tab para inserir rapidamente a data e horário atuais no script em forma de comentário. # ts {snippet} # Mon Jul 26 11:25:03 2021 ------------------------------ 4.3.3 Operadores No R, temos cinco tipos de operadores: aritméticos, relacionais, lógicos, atribuição e diversos. Grande parte deles são descritos na Tabela 4.1. Tabela 4.1: Operadores no R. Operador Tipo Descrição + Aritmético Adição - Aritmético Subtração * Aritmético Multiplicação / Aritmético Divisão %% Aritmético Resto da divisão %/% Aritmético Divisão inteira ^ ou ** Aritmético Expoente &gt; Relacional Maior &lt; Relacional Menor &gt;= Relacional Maior ou igual &lt;= Relacional Menor ou igual == Relacional Igualdade != Relacional Diferença ! Lógico Lógico NÃO &amp; Lógico Lógico elementar E | Lógico Lógico elementar OU &amp;&amp; Lógico Lógico E || Lógico Lógico OU &lt;- ou = Atribuição Atribuição à esquerda &lt;&lt;- Atribuição Super atribuição à esquerda -&gt; Atribuição Atribuição à direita -&gt;&gt; Atribuição Super atribuição à direita : Diversos Sequência unitária %in% Diversos Elementos que pertencem a um vetor %*% Diversos Multiplicar matriz com sua transposta %&gt;% Diversos Pipe (pacote magrittr) |&gt; Diversos Pipe (R base nativo) %% Diversos Intervalo de datas (pacote lubridate) Como exemplo, podemos fazer operações simples usando os operadores aritméticos. ## Operações aritméticas 10 + 2 # adição #&gt; [1] 12 10 * 2 # multiplicação #&gt; [1] 20 Precisamos ficar atentos à prioridade dos operadores aritméticos: PRIORITÁRIO () &gt; ^ &gt; * ou / &gt; + ou - NÃO PRIORITÁRIO Veja no exemplo abaixo como o uso dos parênteses muda o resultado. ## Sem especificar a ordem # Segue a ordem dos operadores. 1 * 2 + 2 / 2 ^ 2 #&gt; [1] 2.5 ## Especificando a ordem # Segue a ordem dos parenteses. ((1 * 2) + (2 / 2)) ^ 2 #&gt; [1] 9 4.3.4 Objetos Objetos são palavras às quais são atribuídos dados. A atribuição possibilita a manipulação de dados ou resultados de análises. Utilizaremos os símbolos &lt; (menor), seguido de - (menos), sem espaço, dessa forma &lt;-. Também podemos utilizar o símbolo de igual (=), mas não recomendamos, por não fazer parte das boas práticas de escrita de códigos em R. Podemos inserir essa combinação de símbolos com o atalho Alt + -. Para demonstrar, vamos atribuir o valor 10 à palavra obj_10, e chamar esse objeto novamente para verificar seu conteúdo. ## Atribuição - símbolo (&lt;-) obj_10 &lt;- 10 obj_10 #&gt; [1] 10 Todos os objetos criados numa sessão do R ficam listados na aba Environment (Figura 4.2 (3)). Além disso, o RStudio possui a função autocomplete, ou seja, podemos digitar as primeiras letras de um objeto (ou função) e em seguida apertar Tab para que o RStudio liste tudo que começar com essas letras. Dois pontos importantes sobre atribuições: primeiro, o R sobrescreve os valores dos objetos com o mesmo nome, deixando o objeto com o valor da segunda atribuição. ## Sobrescreve o valor dos objetos obj &lt;- 100 obj #&gt; [1] 100 ## O objeto &#39;obj&#39; agora vale 2 obj &lt;- 2 obj #&gt; [1] 2 Segundo, o R tem limitações ao nomear objetos: nome de objetos só podem começar por letras (a-z ou A-Z) ou pontos (.) nome de objetos só podem conter letras (a-z ou A-Z), números (0-9), underscores (_) ou pontos (.) R é case-sensitive, i.e., ele reconhece letras maiúsculas como diferentes de letras minúscula. Assim, um objeto chamado resposta é diferente do objeto RESPOSTA devemos evitar acentos ou cedilha (ç) para facilitar a memorização dos objetos e também para evitar erros de encoding/codificação de caracteres nome de objetos não podem ser iguais a nomes especiais, reservados para programação (break, else, FALSE, for, function, if, Inf, NA, NaN, next, repeat, return, TRUE, while) Podemos ainda utilizar objetos para fazer operações e criar objetos. Isso pode parecer um pouco confuso para os iniciantes na linguagem, mas é fundamental aprender essa lógica para passar para os próximos passos. ## Definir dois objetos va1 &lt;- 10 va2 &lt;- 2 ## Operações com objetos e atribuicão adi &lt;- va1 + va2 adi #&gt; [1] 12 4.3.5 Funções Funções são códigos preparados para realizar uma tarefa específica de modo simples. Outra forma de entender uma função é: códigos que realizam operações em argumentos. A estrutura de uma função é muito similar à sintaxe usada em planilhas eletrônicas, sendo composta por: nome_da_função(argumento1, argumento2, ) Nome da função: remete ao que ela faz Parênteses: limitam a função Argumentos: valores, parâmetros ou expressões onde a função atuará Vírgulas: separam os argumentos Os argumentos de uma função podem ser de dois tipos: Valores ou objetos: a função alterará os valores em si ou os valores atribuídos aos objetos Parâmetros: valores fixos que informam um método ou a realização de uma operação. Informa-se o nome desse argumento, seguido de = e um número, texto ou TRUE ou FALSE Alguns exemplos de argumentos como valores ou objetos. ## Funções - argumentos como valores sum(10, 2) #&gt; [1] 12 ## Funções - argumentos como objetos sum(va1, va2) #&gt; [1] 12 Alguns exemplos de argumentos como parâmetros. Note que apesar do valor do argumento ser o mesmo (10), seu efeito no resultado muda drasticamente. Aqui também é importante destacar um ponto: 1) podemos informar os argumentos sequencialmente, sem explicitar seus nomes, ou 2) independente da ordem, mas explicitando seus nomes. Entretanto, como no exemplo abaixo, devemos informar o nome do argumento (i.e., parâmetro), para que seu efeito seja o que desejamos. ## Funções - argumentos como parâmetros ## Repetição - repete todos os elementos rep(x = 1:5, times = 10) #&gt; [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 #&gt; [48] 3 4 5 ## Repetição - repete cada um dos elementos rep(x = 1:5, each = 10) #&gt; [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 #&gt; [48] 5 5 5 Um ponto fundamental, e que deve ser entendido nesse ponto, é o fluxo de atribuições do resultado da operação de funções a novos objetos. No desenvolvimento de qualquer script na linguagem R, grande parte da estrutura do mesmo será dessa forma: atribuição de dados &gt; operações com funções &gt; atribuição dos resultados a novos objetos &gt; operações com funções desses novos objetos &gt; atribuição dos resultados a novos objetos Ao entender esse funcionamento, começamos a entender como devemos pensar na organização do nosso script para montar as análises que precisamos. ## Atribuicão dos resultados ## Repetição rep_times &lt;- rep(1:5, times = 10) rep_times #&gt; [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 #&gt; [48] 3 4 5 ## Somar e atribuir rep_times_soma &lt;- sum(rep_times) rep_times_soma #&gt; [1] 150 ## Raiz e atribuir rep_times_soma_raiz &lt;- sqrt(rep_times_soma) rep_times_soma_raiz #&gt; [1] 12.24745 Por fim, é fundamental também entender a origem das funções que usamos no R. Todas as funções são advindas de pacotes. Esses pacotes possuem duas origens. pacotes já instalados por padrão e que são carregados quando abrimos o R (R Base) pacotes que instalamos e carregamos com funções 4.3.6 Pacotes Pacotes são conjuntos extras de funções para executar tarefas específicas, além do R Base. Existe literalmente milhares de pacotes para as mais diversas tarefas: estatística, ecologia, geografia, sensoriamento remoto, econometria, ciências sociais, gráficos, machine learning, etc. Podemos verificar este vasto conjunto de pacotes pelo link que lista por nome os pacotes oficiais, ou seja, que passaram pelo crivo do CRAN. Existem ainda muito mais pacotes em desenvolvimento, geralmente disponibilizados em repositórios do GitHub ou GitLab. Primeiramente, com uma sessão do R sem carregar nenhum pacote extra, podemos verificar pacotes carregados pelo R Base utilizando a função search(). ## Verificar pacotes carregados search() Podemos ainda verificar todos pacotes instalados em seu computador com a função library(). ## Verificar pacotes instalados library() No R, quando tratamos de pacotes, devemos destacar a diferença de dois conceitos: instalar um pacote e carregar um pacote. A instalação de pacotes possui algumas características: Instala-se um pacote apenas uma vez Precisamos estar conectados à internet O nome do pacote precisa estar entre aspas na função Função (CRAN): install.packages() Vamos instalar o pacote vegan diretamente do CRAN, que possui funções para realizar uma série de análise em ecologia. Para isso, podemos ir em Tools &gt; Install Packages..., ou ir na aba Packages (Figura 4.2 (4)), procurar o pacote e simplesmente clicar em Install. Podemos ainda utilizar a função install.packages(). ## Instalar pacotes install.packages(&quot;vegan&quot;) Podemos conferir em que diretórios um pacote será instalado com a função .libPaths(). ## Diretórios de intalação dos pacotes .libPaths() #&gt; [1] &quot;C:/Users/pater/Documents/R/win-library/4.0&quot; &quot;C:/Program Files/R/R-4.0.5/library&quot; Uma vez instalado um pacote, não há necessidade de instalá-lo novamente. Entretanto, todas as vezes que iniciarmos uma sessão no R, precisamos carregar os pacotes com as funções que precisamos utilizar. O carregamento de pacotes possui algumas características: Carrega-se o pacote toda vez que se abre uma nova sessão do R Não precisamos estar conectados à internet O nome do pacote não precisa estar entre aspas na função Funções: library() ou require() Vamos carregar o pacote vegan que instalamos anteriormente. Podemos ir na aba Packages (Figura 4.2 (4)) e ticar o pacote que queremos carregar ou utilizar a função library(). ## Carregar pacotes library(vegan) Como dissemos, alguns pacotes em desenvolvimento encontram-se disponíveis em repositórios do GitHub ou GitLab. Para instalar pacotes do GitHub, por exemplo, precisamos instalar e carregar o pacote devtools. ## Instalar pacote devtools install.packages(&quot;devtools&quot;) ## Carregar pacote devtools library(devtools) Uma vez instalado e carregado esse pacote, podemos instalar o pacote do GitHub, utilizando a função devtools::install_github(). Precisamos atentar para usar essa forma nome_usuario/nome_repositorio, retirados do link do repositório de interesse. Como exemplo, podemos instalar o mesmo pacote vegan do repositório do GitHub vegandevs/vegan, e depois utilizar a função library() para carregá-lo normalmente. ## Instalar pacote do github devtools::install_github(&quot;vegandevs/vegan&quot;) ## Carregar pacote do github library(&quot;vegan&quot;) Pode ser que em algumas circunstâncias iremos precisar instalar pacotes com versões específicas para algumas análises. A forma mais simples de fazer isso é instalar um pacote a partir de um arquivo compactado .tar.gz. Para isso podemos ir à base do CRAN e realizar o download: https://cran.r-project.org/src/contrib/Archive/. Para exemplificar, vamos instalar o pacote vegan 2.4.0. ## Download do arquivo .tar.gz download.file(url = &quot;https://cran.r-project.org/src/contrib/Archive/vegan/vegan_2.4-0.tar.gz&quot;, destfile = &quot;vegan_2.4-0.tar.gz&quot;, mode = &quot;wb&quot;) ## Instalar o pacote vegan 2.4.0 install.packages(&quot;vegan_2.4-0.tar.gz&quot;, repos = NULL, type = &quot;source&quot;) A maioria dos pacotes vem com bancos de dados que podem ser acessados pela função data(). Esses bancos de dados podem ser usados para testar as funções do pacote. Se estiver com dúvida na maneira como você deve preparar a planilha para realizar uma análise específica, entre no help da função e veja os conjuntos de dados que estão no exemplo desta função. Como exemplo, vamos carregar os dados dune do pacote vegan. ## Carregar dados de um pacote library(vegan) data(dune) dune[1:6, 1:6] #&gt; Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere #&gt; 1 1 0 0 0 0 0 #&gt; 2 3 0 0 2 0 3 #&gt; 3 0 4 0 7 0 2 #&gt; 4 0 8 0 2 0 2 #&gt; 5 2 0 0 0 4 2 #&gt; 6 2 0 0 0 3 0 Se por algum motivo precisarmos desinstalar um pacote, podemos utilizar a função remove.packages(). Já para descarregar um pacote de uma sessão do R, podemos usar a função detach(). ## Descarregar um pacote detach(&quot;package:vegan&quot;, unload = TRUE) E um último ponto fundamental sobre pacotes, diz respeito à atualização dos mesmos. Os pacotes são atualizados com frequência, e infelizmente ou felizmente (pois as atualizações podem oferecer algumas quebras entre pacotes), não se atualizam sozinhos. Muitas vezes, a instalação de um pacote pode depender da versão dos pacotes dependentes, e geralmente uma janela se abre perguntando se você quer que todos os pacotes dependentes sejam atualizados. Podemos ir na aba Packages (Figura 4.2 (4)) e clicar em Update ou usar a função update.packages(checkBuilt = TRUE, ask = FALSE) para atualizá-los, entretanto, essa é uma função que costuma demorar muito para terminar de ser executada. ## Atualização dos pacotes update.packages(checkBuilt = TRUE, ask = FALSE) Para fazer a atualização dos pacotes instalados pelo GitHub, recomendamos o uso do pacote dtupdate. ## Atualização dos pacotes instalados pelo GitHub dtupdate::github_update(auto.install = TRUE, ask = FALSE) Destacamos e incentivamos ainda uma prática que achamos interessante para aumentar a reprodutibilidade de nossos códigos e scripts: a de chamar as funções de pacotes carregados dessa forma pacote::função(). Dessa forma, deixamos claro o pacote em que a função está implementada. Destacamos aqui o exemplo de como instalar pacotes do GitHub do pacote devtools. ## Pacote seguido da função implementada daquele pacote devtools::install_github() 4.3.7 Ajuda (Help) Um importante passo para melhorar a usabilidade e ter mais familiaridade com a linguagem R é aprender a usar a ajuda de cada função. Para tanto, podemos utilizar a função help() ou o operador ?, depois de ter carregado o pacote, para abrir uma nova aba (Figura 4.2 (4)) que possui diversas informações sobre a função de interesse. O arquivo de ajuda do R possui geralmente nove ou dez tópicos, que nos auxiliam muito no entendimento dos dados de entrada, argumentos e que operações estão sendo realizadas. Description: resumo da função Usage: como utilizar a função e quais os seus argumentos Arguments: detalha os argumentos e como os mesmos devem ser especificados Details: detalhes importantes para se usar a função Value: mostra como interpretar a saída (output) da função (os resultados) Note: notas gerais sobre a função Authors: autores da função References: referências bibliográficas para os métodos usados para construção da função See also: funções relacionadas Examples: exemplos do uso da função. Às vezes pode ser útil copiar esse trecho e colar no R para ver como funciona e como usar a função. Vamos realizar um exemplo, buscando o help da função aov(), que realiza uma análise de variância. ## Ajuda help(aov) ?aov Além das funções, podemos buscar detalhes de um pacote em específico, para uma página simples do help utilizando a função help() ou o operador ?. Entretanto, para uma opção que ofereça uma descrição detalhada e um índice de todas as funções do pacote, podemos utilizar a função library(), mas agora utilizando o argumento help, indicando o pacote de interesse entre aspas. ## Ajuda do pacote help(vegan) ?vegan ## Help detalhado library(help = &quot;vegan&quot;) Outra ferramenta de busca é a página rseek, na qual é possível buscar por um termo não só nos pacotes do R, mas também em listas de emails, manuais, páginas na internet e livros sobre o programa. 4.3.8 Ambiente (Environment) O ambiente Environment como vimos é onde os objetos criados são armazenados. É fundamental entender que um objeto é uma alocação de um pequeno espaço na memória RAM do seu computador, onde o R armazenará um valor ou o resultado de uma função, utilizando o nome que definimos na atribuição. Sendo assim, se fizermos uma atribuição de um objeto maior que o tamanho da memória RAM, esse objeto não será alocado, e a atribuição não funcionará. Existem opções para contornar esse tipo de limitação, mas não a abordaremos aqui. Entretanto, podemos utilizar a função object.size() para saber quanto espaço nosso objeto criado está alocando de memória RAM. ## Tamanho de um objeto object.size(adi) #&gt; 56 bytes Podemos listar todos os objetos criados com a função ls() ou objects(). ## Listar todos os objetos ls() #&gt; [1] &quot;a1&quot; &quot;abu&quot; #&gt; [3] &quot;adi&quot; &quot;agua&quot; #&gt; [5] &quot;aic_fit&quot; &quot;amostragem&quot; #&gt; [7] &quot;analise&quot; &quot;analise2&quot; #&gt; [9] &quot;ar&quot; &quot;aranha.cent&quot; #&gt; [11] &quot;aranhas&quot; &quot;assumption3&quot; #&gt; [13] &quot;autovalores&quot; &quot;autovetores&quot; #&gt; [15] &quot;betad.aves&quot; &quot;bird.rda&quot; #&gt; [17] &quot;bocaina_transf&quot; &quot;bocaina_transf2&quot; #&gt; [19] &quot;ca.env&quot; &quot;candidates&quot; #&gt; [21] &quot;chat&quot; &quot;cofresult&quot; #&gt; [23] &quot;colar&quot; &quot;composicao_acaros&quot; #&gt; [25] &quot;concatenar&quot; &quot;coord&quot; #&gt; [27] &quot;cores&quot; &quot;correlacao_arbustos&quot; #&gt; [29] &quot;CRC_LP_femea&quot; &quot;CRC_PN_macho&quot; #&gt; [31] &quot;dados&quot; &quot;dados_amostras&quot; #&gt; [33] &quot;dados_ancova&quot; &quot;dados_anova_simples&quot; #&gt; [35] &quot;dados_bloco&quot; &quot;dados_combinado&quot; #&gt; [37] &quot;dados_dois_fatores&quot; &quot;dados_dois_fatores_interacao&quot; #&gt; [39] &quot;dados_dois_fatores_interacao2&quot; &quot;dados_rarefacao&quot; #&gt; [41] &quot;dados_regressao&quot; &quot;dados_regressao_mul&quot; #&gt; [43] &quot;dados_semNA&quot; &quot;darknessmale&quot; #&gt; [45] &quot;dat&quot; &quot;dat.graf&quot; #&gt; [47] &quot;data&quot; &quot;dendro&quot; #&gt; [49] &quot;df&quot; &quot;df_sem_na&quot; #&gt; [51] &quot;dist_normal&quot; &quot;distBocaina&quot; #&gt; [53] &quot;doubs&quot; &quot;dune&quot; #&gt; [55] &quot;E&quot; &quot;ED&quot; #&gt; [57] &quot;eigen_aranhas&quot; &quot;eixos&quot; #&gt; [59] &quot;env&quot; &quot;env.contin&quot; #&gt; [61] &quot;env.dist&quot; &quot;env.mite.pco&quot; #&gt; [63] &quot;env.pad&quot; &quot;env.pad.cat&quot; #&gt; [65] &quot;env.pca&quot; &quot;env.sel&quot; #&gt; [67] &quot;env_cont&quot; &quot;env2&quot; #&gt; [69] &quot;EP&quot; &quot;EP2&quot; #&gt; [71] &quot;espher_model&quot; &quot;expon_model&quot; #&gt; [73] &quot;fator&quot; &quot;fator_nominal&quot; #&gt; [75] &quot;fator_ordinal&quot; &quot;filho_intervalo&quot; #&gt; [77] &quot;filho_nascimento&quot; &quot;fish&quot; #&gt; [79] &quot;fitofis&quot; &quot;flo&quot; #&gt; [81] &quot;g_acari_axi1&quot; &quot;g_acari_axi2&quot; #&gt; [83] &quot;g_bar_h&quot; &quot;g_bar_v&quot; #&gt; [85] &quot;g_donut&quot; &quot;g_pie&quot; #&gt; [87] &quot;g1&quot; &quot;gauss_model&quot; #&gt; [89] &quot;grp.int&quot; &quot;grp.mon&quot; #&gt; [91] &quot;grp.pla&quot; &quot;hull.data&quot; #&gt; [93] &quot;hur_NB&quot; &quot;i&quot; #&gt; [95] &quot;ind_env&quot; &quot;intror_anfibios_locais&quot; #&gt; [97] &quot;k&quot; &quot;li&quot; #&gt; [99] &quot;lista&quot; &quot;lista_nome&quot; #&gt; [101] &quot;lista_rarefacao&quot; &quot;ma&quot; #&gt; [103] &quot;ma_cbind&quot; &quot;ma_col&quot; #&gt; [105] &quot;ma_rbind&quot; &quot;ma_row&quot; #&gt; [107] &quot;mat_knn&quot; &quot;mat_listw&quot; #&gt; [109] &quot;mat_nb&quot; &quot;matriz_cov&quot; #&gt; [111] &quot;matriz_F&quot; &quot;MEM_mat&quot; #&gt; [113] &quot;MidPoint&quot; &quot;mite&quot; #&gt; [115] &quot;mite.env&quot; &quot;mite.hel&quot; #&gt; [117] &quot;mite.riqueza&quot; &quot;mite.xy&quot; #&gt; [119] &quot;mod.mite&quot; &quot;mod_log&quot; #&gt; [121] &quot;mod_nb&quot; &quot;mod_pois&quot; #&gt; [123] &quot;mod_pro&quot; &quot;mod_quasipois&quot; #&gt; [125] &quot;mod1&quot; &quot;mod2&quot; #&gt; [127] &quot;mod3&quot; &quot;model_bloco1&quot; #&gt; [129] &quot;model_bloco2&quot; &quot;modelo_ancova&quot; #&gt; [131] &quot;modelo_ancova2&quot; &quot;Modelo_anova&quot; #&gt; [133] &quot;modelo_errado&quot; &quot;Modelo_interacao1&quot; #&gt; [135] &quot;Modelo_interacao2&quot; &quot;modelo_intercepto&quot; #&gt; [137] &quot;modelo_regressao&quot; &quot;modelo_regressao_mul&quot; #&gt; [139] &quot;Modelo1&quot; &quot;Modelo2&quot; #&gt; [141] &quot;moran.comp&quot; &quot;moran.env&quot; #&gt; [143] &quot;mu&quot; &quot;multi&quot; #&gt; [145] &quot;n&quot; &quot;nmds.beta&quot; #&gt; [147] &quot;no_spat_gls&quot; &quot;nulo&quot; #&gt; [149] &quot;obj&quot; &quot;obj_10&quot; #&gt; [151] &quot;obj_caracter&quot; &quot;obj_complexo&quot; #&gt; [153] &quot;obj_logico&quot; &quot;obj_numerico_double&quot; #&gt; [155] &quot;obj_numerico_inteiro&quot; &quot;op&quot; #&gt; [157] &quot;parasitas&quot; &quot;Pareado&quot; #&gt; [159] &quot;pca.comp&quot; &quot;pca.p&quot; #&gt; [161] &quot;pcoa.dat&quot; &quot;pcoa.sps&quot; #&gt; [163] &quot;penguin_islands&quot; &quot;penguins&quot; #&gt; [165] &quot;penguins_01&quot; &quot;penguins_02&quot; #&gt; [167] &quot;penguins_arrange&quot; &quot;penguins_arrange_across&quot; #&gt; [169] &quot;penguins_arrange_desc&quot; &quot;penguins_arrange_desc_m&quot; #&gt; [171] &quot;penguins_bind_cols&quot; &quot;penguins_bind_rows&quot; #&gt; [173] &quot;penguins_count&quot; &quot;penguins_count_two&quot; #&gt; [175] &quot;penguins_distinct&quot; &quot;penguins_distinct_keep_all&quot; #&gt; [177] &quot;penguins_distinct_keep_all_across&quot; &quot;penguins_filter&quot; #&gt; [179] &quot;penguins_filter_between&quot; &quot;penguins_filter_if&quot; #&gt; [181] &quot;penguins_filter_in&quot; &quot;penguins_filter_na&quot; #&gt; [183] &quot;penguins_filter_two&quot; &quot;penguins_group_by&quot; #&gt; [185] &quot;penguins_group_by_across&quot; &quot;penguins_left_join&quot; #&gt; [187] &quot;penguins_mean&quot; &quot;penguins_mutate&quot; #&gt; [189] &quot;penguins_mutate_across&quot; &quot;penguins_pad&quot; #&gt; [191] &quot;penguins_prop&quot; &quot;penguins_raw_colunas_na&quot; #&gt; [193] &quot;penguins_raw_multi_factor&quot; &quot;penguins_raw_pivot_longer&quot; #&gt; [195] &quot;penguins_raw_pivot_wider&quot; &quot;penguins_raw_sel_col&quot; #&gt; [197] &quot;penguins_raw_separar&quot; &quot;penguins_raw_separar_linhas&quot; #&gt; [199] &quot;penguins_raw_subs_na&quot; &quot;penguins_raw_todas_na&quot; #&gt; [201] &quot;penguins_raw_unir&quot; &quot;penguins_relocate_col&quot; #&gt; [203] &quot;penguins_relocate_ncol&quot; &quot;penguins_rename&quot; #&gt; [205] &quot;penguins_rename_with&quot; &quot;penguins_select_contains&quot; #&gt; [207] &quot;penguins_select_names&quot; &quot;penguins_select_position&quot; #&gt; [209] &quot;penguins_select_pull&quot; &quot;penguins_slice&quot; #&gt; [211] &quot;penguins_slice_head&quot; &quot;penguins_slice_max&quot; #&gt; [213] &quot;penguins_slice_sample&quot; &quot;penguins_stringr_nomes&quot; #&gt; [215] &quot;penguins_stringr_valores&quot; &quot;penguins_summarise&quot; #&gt; [217] &quot;penguins_summarise_across&quot; &quot;penguins_trait&quot; #&gt; [219] &quot;penguins2&quot; &quot;perm.aves&quot; #&gt; [221] &quot;pois_plain&quot; &quot;pred.env&quot; #&gt; [223] &quot;pred.scores.mite&quot; &quot;pred.vars&quot; #&gt; [225] &quot;predito&quot; &quot;pv.birds&quot; #&gt; [227] &quot;r_hoje&quot; &quot;r_inicio&quot; #&gt; [229] &quot;r_intervalo&quot; &quot;r_quadr&quot; #&gt; [231] &quot;rarefacao_anuros&quot; &quot;rarefacao_repteis&quot; #&gt; [233] &quot;ratio_model&quot; &quot;rda.bird&quot; #&gt; [235] &quot;rda.p&quot; &quot;redmale&quot; #&gt; [237] &quot;rep_times&quot; &quot;rep_times_soma&quot; #&gt; [239] &quot;rep_times_soma_raiz&quot; &quot;repeticao&quot; #&gt; [241] &quot;res.axis&quot; &quot;res.p.axis&quot; #&gt; [243] &quot;res.p.var&quot; &quot;res.var&quot; #&gt; [245] &quot;res_indval&quot; &quot;res_rarefacao_amostras&quot; #&gt; [247] &quot;residuos&quot; &quot;residuos_LP&quot; #&gt; [249] &quot;resultados_anuros&quot; &quot;resultados_comunidades&quot; #&gt; [251] &quot;resultados_morcegos&quot; &quot;resultados_rarefacao&quot; #&gt; [253] &quot;resultados_repteis&quot; &quot;riqueza&quot; #&gt; [255] &quot;riqueza_rarefeita&quot; &quot;sel.vars&quot; #&gt; [257] &quot;sequencia&quot; &quot;sequencia_esp&quot; #&gt; [259] &quot;simulationBion&quot; &quot;simulationOutput&quot; #&gt; [261] &quot;sol1&quot; &quot;sp&quot; #&gt; [263] &quot;sp_compos&quot; &quot;spatial.pred&quot; #&gt; [265] &quot;spe&quot; &quot;spe.KM.cascade&quot; #&gt; [267] &quot;spe.kmeans&quot; &quot;spe.norm&quot; #&gt; [269] &quot;species&quot; &quot;species.hel&quot; #&gt; [271] &quot;sps.dis&quot; &quot;tab_indval&quot; #&gt; [273] &quot;tempo_estudando_r&quot; &quot;tempo_estudando_r_dur&quot; #&gt; [275] &quot;theme_book&quot; &quot;tidy_anfibios_locais&quot; #&gt; [277] &quot;va1&quot; &quot;va2&quot; #&gt; [279] &quot;var_env&quot; &quot;ve&quot; #&gt; [281] &quot;vec_1&quot; &quot;vec_2&quot; #&gt; [283] &quot;vec_ch&quot; &quot;vec_fa&quot; #&gt; [285] &quot;vec_nu&quot; &quot;W_sel_mat&quot; #&gt; [287] &quot;x&quot; &quot;xy&quot; #&gt; [289] &quot;y&quot; &quot;z&quot; #&gt; [291] &quot;ziNB_mod2&quot; &quot;ziP_mod2&quot; Podemos ainda remover objetos criados com a função rm() ou remove(). Ou ainda fazer uma função composta para remover todos os objetos do Environment. ## Remover um objeto rm(adi) ## Remover todos os objetos criados rm(list = ls()) Quando usamos a função ls() agora, nenhum objeto é listado. ## Listar todos os objetos ls() #&gt; character(0) Toda a vez que fechamos o R os objetos criados são apagados do Environment. Dessa forma, em algumas ocasiões, por exemplo, análises estatísticas que demoram um grande tempo para serem realizadas, pode ser interessante exportar alguns ou todos os objetos criados. Para salvar todos os objetos, ou seja, todo o workspace, podemos ir em Session -&gt; Save Workspace As... e escolher o nome do arquivo do workspace, por exemplo, meu_workspace.RData. Podemos ainda utilizar funções para essas tarefas. A função save.image() salva todo workspace com a extensão .RData. ## Salvar todo o workspace save.image(file = &quot;meu_workspace.RData&quot;) Depois disso, podemos fechar o RStudio tranquilamente, e quando formos trabalhar novamente, podemos carregar os objetos criados indo em Session -&gt; Load Workspace... ou utilizando a função load(). ## Carregar todo o workspace load(&quot;meu_workspace.RData&quot;) Entretanto, em algumas ocasiões, não precisamos salvar todos os objetos. Dessa forma, podemos salvar apenas alguns objetos específicos usando a função save(), também com a extensão .RData. ## Salvar apenas um objeto save(obj1, file = &quot;meu_obj.RData&quot;) ## Salvar apenas um objeto save(obj1, obj2, file = &quot;meus_objs.RData&quot;) ## Carregar os objetos load(&quot;meus_objs.RData&quot;) Ou ainda podemos salvar apenas um objeto com a extensão .rds. Para isso, usamos as funções saveRDS() e readRDS(), para exportar e importar esses dados, respectivamente. ## Salvar um objeto para um arquivo saveRDS(obj, file = &quot;meu_obj.rds&quot;) ## Carregar esse objeto readRDS(file = &quot;meu_obj.rds&quot;) 4.3.9 Citações Ao utilizar o R para realizar alguma análise em nossos estudos, é fundamental a citação do mesmo. Para saber como citar exatamente o R em artigos, existe uma função denominada citation(), que provê um formato genérico de citação e um BibTeX para arquivos LaTeX e R Markdown. ## Citação do R citation() #&gt; #&gt; To cite R in publications use: #&gt; #&gt; R Core Team (2021). R: A language and environment for statistical computing. R #&gt; Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. #&gt; #&gt; A BibTeX entry for LaTeX users is #&gt; #&gt; @Manual{, #&gt; title = {R: A Language and Environment for Statistical Computing}, #&gt; author = {{R Core Team}}, #&gt; organization = {R Foundation for Statistical Computing}, #&gt; address = {Vienna, Austria}, #&gt; year = {2021}, #&gt; url = {https://www.R-project.org/}, #&gt; } #&gt; #&gt; We have invested a lot of time and effort in creating R, please cite it when using it #&gt; for data analysis. See also &#39;citation(&quot;pkgname&quot;)&#39; for citing R packages. No resultado dessa função, há uma mensagem muito interessante: See also citation(pkgname) for citing R packages. Dessa forma, aconselhamos os usuários de R a citar também os pacotes que utilizaram em suas análises para dar os devidos créditos aos desenvolvedores das funções implementadas nos pacotes. Como exemplo, vamos ver como fica a citação do pacote vegan. ## Citação do pacote vegan citation(&quot;vegan&quot;) #&gt; #&gt; To cite package &#39;vegan&#39; in publications use: #&gt; #&gt; Jari Oksanen, F. Guillaume Blanchet, Michael Friendly, Roeland Kindt, Pierre Legendre, #&gt; Dan McGlinn, Peter R. Minchin, R. B. O&#39;Hara, Gavin L. Simpson, Peter Solymos, M. Henry #&gt; H. Stevens, Eduard Szoecs and Helene Wagner (2020). vegan: Community Ecology Package. #&gt; R package version 2.5-7. https://CRAN.R-project.org/package=vegan #&gt; #&gt; A BibTeX entry for LaTeX users is #&gt; #&gt; @Manual{, #&gt; title = {vegan: Community Ecology Package}, #&gt; author = {Jari Oksanen and F. Guillaume Blanchet and Michael Friendly and Roeland Kindt and Pierre Legendre and Dan McGlinn and Peter R. Minchin and R. B. O&#39;Hara and Gavin L. Simpson and Peter Solymos and M. Henry H. Stevens and Eduard Szoecs and Helene Wagner}, #&gt; year = {2020}, #&gt; note = {R package version 2.5-7}, #&gt; url = {https://CRAN.R-project.org/package=vegan}, #&gt; } #&gt; #&gt; ATTENTION: This citation information has been auto-generated from the package #&gt; DESCRIPTION file and may need manual editing, see &#39;help(&quot;citation&quot;)&#39;. Podemos ainda utilizar a função write_bib() do pacote knitr para exportar a citação do pacote no formato .bib. ## Exportar uma citação em formato .bib knitr::write_bib(&quot;vegan&quot;, file = &quot;vegan_ex.bib&quot;) 4.3.10 Principais erros de iniciantes Errar quando está começando a usar o R é muito comum e faz parte do aprendizado. Entretanto, os erros nunca devem ser encarados como uma forma de desestímulo para continuar tentando. Todos nós, autores desse livro, e provavelmente usuários mais ou menos experientes, já passaram por um momento em que se quer desistir de tudo. Jovem aprendiz de R, a única diferença entre você que está iniciando agora e nós que usamos há mais tempo são as horas a mais de uso (e raiva). O que temos a mais é experiência para olhar o erro, lê-lo e conseguir interpretar o que está errado e saber buscar ajuda. Dessa forma, o ponto mais importante de quem está iniciando é ter paciência, calma, bom humor, ler e entender as mensagens de erros. Listaremos aqui o que consideramos os principais erros dos iniciantes no R. 1. Esquecer de completar uma função ou bloco de códigos Esquecer de completar uma função ou bloco de códigos é algo bem comum. Geralmente esquecemos de fechar aspas \"\" ou parênteses (), mas felizmente geralmente o R nos informa isso, indicando um símbolo de +. sum(1, 2 + #&gt; Error: &lt;text&gt;:3:0: unexpected end of input #&gt; 1: sum(1, 2 #&gt; 2: + #&gt; ^ 2. Esquecer da vírgula Outro erro bastante comum é esquecer de acrescentar a vírgula , para separar argumentos dentro de uma função, principalmente se estamos compondo várias funções acopladas, i.e., uma função dentro da outra. sum(1 2) #&gt; Error: &lt;text&gt;:1:7: unexpected numeric constant #&gt; 1: sum(1 2 #&gt; ^ 3. Chamar um objeto errado Pode parecer simples, mas esse é de longe o erro mais comum que pessoas iniciantes comentem. Quando temos um longo script, é de se esperar que tenhamos atribuído diversos objetos e em algum momento atribuímos um nome do qual não lembramos. Dessa forma, quando chamamos o objeto ele não existe e devolve um erro. Entretanto, esse tipo de erro pode ser facilmente identificado, como o exemplo abaixo. obj &lt;- 10 OBJ #&gt; Error in eval(expr, envir, enclos): object &#39;OBJ&#39; not found 4. Esquecer de carregar um pacote Esse também é um erro recorrente, mesmo para usuários mais experientes. Em scripts de análises complexas, que requerem vários pacotes, geralmente esquecemos de um ou outro A melhor forma de evitar esse tipo de erro é listar os pacotes que vamos precisar usar logo no início do script. ## Carregar dados data(dune) ## Função do pacote vegan decostand(dune, &quot;hell&quot;) #&gt; Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere Bromhord Chenalbu Cirsarve #&gt; 1 0.2357023 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 2 0.2672612 0.0000000 0.0000000 0.2182179 0.0000000 0.2672612 0.3086067 0.0000000 0.0000000 #&gt; 3 0.0000000 0.3162278 0.0000000 0.4183300 0.0000000 0.2236068 0.0000000 0.0000000 0.0000000 #&gt; 4 0.0000000 0.4216370 0.0000000 0.2108185 0.0000000 0.2108185 0.2581989 0.0000000 0.2108185 #&gt; 5 0.2156655 0.0000000 0.0000000 0.0000000 0.3049971 0.2156655 0.2156655 0.0000000 0.0000000 #&gt; 6 0.2041241 0.0000000 0.0000000 0.0000000 0.2500000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 7 0.2236068 0.0000000 0.0000000 0.0000000 0.2236068 0.0000000 0.2236068 0.0000000 0.0000000 #&gt; 8 0.0000000 0.3162278 0.0000000 0.3535534 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 9 0.0000000 0.2672612 0.0000000 0.2672612 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 10 0.3049971 0.0000000 0.0000000 0.0000000 0.3049971 0.2156655 0.3049971 0.0000000 0.0000000 #&gt; 11 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 12 0.0000000 0.3380617 0.0000000 0.4780914 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 13 0.0000000 0.3892495 0.0000000 0.3892495 0.0000000 0.0000000 0.0000000 0.1740777 0.0000000 #&gt; 14 0.0000000 0.4082483 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 15 0.0000000 0.4170288 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 16 0.0000000 0.4605662 0.0000000 0.3481553 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 17 0.3651484 0.0000000 0.3651484 0.0000000 0.5163978 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 18 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.2721655 0.0000000 0.0000000 0.0000000 #&gt; 19 0.0000000 0.0000000 0.3110855 0.0000000 0.3592106 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 20 0.0000000 0.4016097 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; Comapalu Eleopalu Elymrepe Empenigr Hyporadi Juncarti Juncbufo Lolipere Planlanc #&gt; 1 0.0000000 0.0000000 0.4714045 0.0000000 0.0000000 0.0000000 0.0000000 0.6236096 0.0000000 #&gt; 2 0.0000000 0.0000000 0.3086067 0.0000000 0.0000000 0.0000000 0.0000000 0.3450328 0.0000000 #&gt; 3 0.0000000 0.0000000 0.3162278 0.0000000 0.0000000 0.0000000 0.0000000 0.3872983 0.0000000 #&gt; 4 0.0000000 0.0000000 0.2981424 0.0000000 0.0000000 0.0000000 0.0000000 0.3333333 0.0000000 #&gt; 5 0.0000000 0.0000000 0.3049971 0.0000000 0.0000000 0.0000000 0.0000000 0.2156655 0.3409972 #&gt; 6 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.3535534 0.3227486 #&gt; 7 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.2236068 0.3872983 0.3535534 #&gt; 8 0.0000000 0.3162278 0.0000000 0.0000000 0.0000000 0.3162278 0.0000000 0.3162278 0.0000000 #&gt; 9 0.0000000 0.0000000 0.3779645 0.0000000 0.0000000 0.3086067 0.3086067 0.2182179 0.0000000 #&gt; 10 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.3735437 0.2641353 #&gt; 11 0.0000000 0.0000000 0.0000000 0.0000000 0.2500000 0.0000000 0.0000000 0.4677072 0.3061862 #&gt; 12 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.3380617 0.0000000 0.0000000 #&gt; 13 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.3015113 0.0000000 0.0000000 #&gt; 14 0.2886751 0.4082483 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 15 0.2948839 0.4662524 0.0000000 0.0000000 0.0000000 0.3611576 0.0000000 0.0000000 0.0000000 #&gt; 16 0.0000000 0.4923660 0.0000000 0.0000000 0.0000000 0.3015113 0.0000000 0.0000000 0.0000000 #&gt; 17 0.0000000 0.0000000 0.0000000 0.0000000 0.3651484 0.0000000 0.0000000 0.0000000 0.3651484 #&gt; 18 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.2721655 0.3333333 #&gt; 19 0.0000000 0.0000000 0.0000000 0.2540003 0.4016097 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 20 0.0000000 0.3592106 0.0000000 0.0000000 0.0000000 0.3592106 0.0000000 0.0000000 0.0000000 #&gt; Poaprat Poatriv Ranuflam Rumeacet Sagiproc Salirepe Scorautu Trifprat Trifrepe #&gt; 1 0.4714045 0.3333333 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 2 0.3086067 0.4082483 0.0000000 0.0000000 0.0000000 0.0000000 0.3450328 0.0000000 0.3450328 #&gt; 3 0.3535534 0.3872983 0.0000000 0.0000000 0.0000000 0.0000000 0.2236068 0.0000000 0.2236068 #&gt; 4 0.2981424 0.3333333 0.0000000 0.0000000 0.3333333 0.0000000 0.2108185 0.0000000 0.1490712 #&gt; 5 0.2156655 0.3735437 0.0000000 0.3409972 0.0000000 0.0000000 0.2641353 0.2156655 0.2156655 #&gt; 6 0.2500000 0.2886751 0.0000000 0.3535534 0.0000000 0.0000000 0.2500000 0.3227486 0.3227486 #&gt; 7 0.3162278 0.3535534 0.0000000 0.2738613 0.0000000 0.0000000 0.2738613 0.2236068 0.2236068 #&gt; 8 0.3162278 0.3162278 0.2236068 0.0000000 0.2236068 0.0000000 0.2738613 0.0000000 0.2236068 #&gt; 9 0.3086067 0.3450328 0.0000000 0.2182179 0.2182179 0.0000000 0.2182179 0.0000000 0.2672612 #&gt; 10 0.3049971 0.3049971 0.0000000 0.0000000 0.0000000 0.0000000 0.2641353 0.0000000 0.3735437 #&gt; 11 0.3535534 0.0000000 0.0000000 0.0000000 0.2500000 0.0000000 0.3952847 0.0000000 0.3061862 #&gt; 12 0.0000000 0.3380617 0.0000000 0.2390457 0.3380617 0.0000000 0.2390457 0.0000000 0.2927700 #&gt; 13 0.2461830 0.5222330 0.2461830 0.0000000 0.2461830 0.0000000 0.2461830 0.0000000 0.2461830 #&gt; 14 0.0000000 0.0000000 0.2886751 0.0000000 0.0000000 0.0000000 0.2886751 0.0000000 0.5000000 #&gt; 15 0.0000000 0.0000000 0.2948839 0.0000000 0.0000000 0.0000000 0.2948839 0.0000000 0.2085144 #&gt; 16 0.0000000 0.2461830 0.2461830 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; 17 0.2581989 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.3651484 0.0000000 0.0000000 #&gt; 18 0.3333333 0.0000000 0.0000000 0.0000000 0.0000000 0.3333333 0.4303315 0.0000000 0.2721655 #&gt; 19 0.0000000 0.0000000 0.0000000 0.0000000 0.3110855 0.3110855 0.4399413 0.0000000 0.2540003 #&gt; 20 0.0000000 0.0000000 0.3592106 0.0000000 0.0000000 0.4016097 0.2540003 0.0000000 0.0000000 #&gt; Vicilath Bracruta Callcusp #&gt; 1 0.0000000 0.0000000 0.0000000 #&gt; 2 0.0000000 0.0000000 0.0000000 #&gt; 3 0.0000000 0.2236068 0.0000000 #&gt; 4 0.0000000 0.2108185 0.0000000 #&gt; 5 0.0000000 0.2156655 0.0000000 #&gt; 6 0.0000000 0.3535534 0.0000000 #&gt; 7 0.0000000 0.2236068 0.0000000 #&gt; 8 0.0000000 0.2236068 0.0000000 #&gt; 9 0.0000000 0.2182179 0.0000000 #&gt; 10 0.1524986 0.2156655 0.0000000 #&gt; 11 0.2500000 0.3535534 0.0000000 #&gt; 12 0.0000000 0.3380617 0.0000000 #&gt; 13 0.0000000 0.0000000 0.0000000 #&gt; 14 0.0000000 0.0000000 0.4082483 #&gt; 15 0.0000000 0.4170288 0.0000000 #&gt; 16 0.0000000 0.3481553 0.3015113 #&gt; 17 0.0000000 0.0000000 0.0000000 #&gt; 18 0.1924501 0.4714045 0.0000000 #&gt; 19 0.0000000 0.3110855 0.0000000 #&gt; 20 0.0000000 0.3592106 0.3110855 Geralmente a mensagem de erro será de que a função não foi encontrada ou algo nesse sentido. Carregando o pacote, esse erro é contornado. ## Carregar o pacote library(vegan) ## Carregar dados data(dune) ## Função do pacote vegan decostand(dune[1:6, 1:6], &quot;hell&quot;) #&gt; Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere #&gt; 1 1.0000000 0.0000000 0 0.0000000 0.0000000 0.0000000 #&gt; 2 0.6123724 0.0000000 0 0.5000000 0.0000000 0.6123724 #&gt; 3 0.0000000 0.5547002 0 0.7337994 0.0000000 0.3922323 #&gt; 4 0.0000000 0.8164966 0 0.4082483 0.0000000 0.4082483 #&gt; 5 0.5000000 0.0000000 0 0.0000000 0.7071068 0.5000000 #&gt; 6 0.6324555 0.0000000 0 0.0000000 0.7745967 0.0000000 5. Usar o nome da função de forma errônea Esse erro não é tão comum, mas pode ser incômodo às vezes. Algumas funções possuem nomes no padrão Camel Case, i.e., com letras maiúsculas para no meio do nome da função. Isso às vezes pode confundir, ou ainda, as funções podem ou não ser separadas com ., como row.names() e rownames(). No Capítulo de tidyverse 5, veremos que houve uma tentativa de padronização nos nomes das funções para Snake Case, i.e, todas as funções possuem letras minúsculas, com palavras separadas por underscore _. ## Soma das colunas colsums(dune) #&gt; Error in colsums(dune): could not find function &quot;colsums&quot; ## Soma das colunas colSums(dune) #&gt; Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere Bromhord Chenalbu Cirsarve Comapalu Eleopalu #&gt; 16 48 5 36 21 13 15 1 2 4 25 #&gt; Elymrepe Empenigr Hyporadi Juncarti Juncbufo Lolipere Planlanc Poaprat Poatriv Ranuflam Rumeacet #&gt; 26 2 9 18 13 58 26 48 63 14 18 #&gt; Sagiproc Salirepe Scorautu Trifprat Trifrepe Vicilath Bracruta Callcusp #&gt; 20 11 54 9 47 4 49 10 6. Atentar para o diretório correto Muitas vezes o erro é simplesmente porque o usuário(a) não definiu o diretório correto onde está o arquivo a ser importado. Por isso é fundamental sempre verificar se o diretório foi definido corretamente, geralmente com as funções dir() ou list.files() para listar no console a lista de arquivos no diretório. Podemos ainda usar o argumento pattern para listar arquivos por um padrão textual. ## Listar os arquivos do diretório definido dir() list.files() ## Listar os arquivos do diretório definido por um padrão dir(pattern = &quot;.csv&quot;) Além disso, é fundamental ressaltar a importância de verificar se o nome do arquivo que importaremos foi digitado corretamente, atentando-se também para a extensão: .csv, .txt, .xlsx, etc. 4.4 Estrutura e manipulação de objetos O conhecimento sobre a estrutura e manipulação de objetos é fundamental para ter domínio e entendimento do funcionamento da linguagem R. Nesta seção, trataremos da estrutura e manipulação de dados no R, no que ficou conhecido como modo R Base, em contrapartida ao tidyverse, tópico do Capítulo 5. Abordaremos aqui temas chaves: 1) atributos de objetos, 2) manipulação de objetos unidimensionais e multidimensionais, 3) valores faltantes e especiais, 4) diretório de trabalho, e 5) importar, conferir e exportar dados. 4.4.1 Atributo dos objetos Quando fazemos atribuições de dados no R (&lt;-), os objetos gerados possuem três características. Nome: palavra que o R reconhece os dados atribuídos Conteúdo: dados em si Atributos: modos (natureza) e estruturas (organização) dos elementos Vamos explorar mais a fundo os modos e estruturas dos objetos. Vale ressaltar que isso é uma simplificação, pois há muitas classes de objetos, como funções e saídas de funções que possuem outros atributos. Podemos verificar os atributos dos objetos com a função attributes(). ## Atributos attributes(dune) #&gt; $names #&gt; [1] &quot;Achimill&quot; &quot;Agrostol&quot; &quot;Airaprae&quot; &quot;Alopgeni&quot; &quot;Anthodor&quot; &quot;Bellpere&quot; &quot;Bromhord&quot; &quot;Chenalbu&quot; #&gt; [9] &quot;Cirsarve&quot; &quot;Comapalu&quot; &quot;Eleopalu&quot; &quot;Elymrepe&quot; &quot;Empenigr&quot; &quot;Hyporadi&quot; &quot;Juncarti&quot; &quot;Juncbufo&quot; #&gt; [17] &quot;Lolipere&quot; &quot;Planlanc&quot; &quot;Poaprat&quot; &quot;Poatriv&quot; &quot;Ranuflam&quot; &quot;Rumeacet&quot; &quot;Sagiproc&quot; &quot;Salirepe&quot; #&gt; [25] &quot;Scorautu&quot; &quot;Trifprat&quot; &quot;Trifrepe&quot; &quot;Vicilath&quot; &quot;Bracruta&quot; &quot;Callcusp&quot; #&gt; #&gt; $row.names #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; #&gt; [20] &quot;20&quot; #&gt; #&gt; $class #&gt; [1] &quot;data.frame&quot; 4.4.1.1 Modo dos objetos A depender da natureza dos elementos que compõem os dados e que foram atribuídos aos objetos, esses objetos podem ser, de forma simples um dos cinco modos: numérico do tipo inteiro (integer), numérico do tipo flutuante (double), texto (character), lógico (logical) ou complexo (complex). A atribuição de números no R podem gerar dois tipos de modos: integer para números inteiros e double para números flutuantes ou com decimais. ## Numérico double obj_numerico_double &lt;- 1 ## Modo mode(obj_numerico_double) #&gt; [1] &quot;numeric&quot; ## Tipo typeof(obj_numerico_double) #&gt; [1] &quot;double&quot; A título de praticidade, ambos são incorporados como o modo numeric, com o tipo double, a menos que especifiquemos que seja inteiro com a letra L depois do número. ## Numérico integer obj_numerico_inteiro &lt;- 1L ## Modo mode(obj_numerico_inteiro) #&gt; [1] &quot;numeric&quot; ## Tipo typeof(obj_numerico_inteiro) #&gt; [1] &quot;integer&quot; Além de números, podemos atribuir textos, utilizando para isso aspas \"\". ## Caracter ou string obj_caracter &lt;- &quot;a&quot; # atencao para as aspas ## Modo mode(obj_caracter) #&gt; [1] &quot;character&quot; Em algumas situações, precisamos indicar a ocorrência ou não de um evento ou operação. Para isso, utilizamos as palavras reservadas (TRUE e FALSE), chamadas de variáveis booleanas, pois assumem apenas duas possibilidades: 0 ou 1. Devemos nos ater para o fato dessas palavras serem escritas com letras maiúsculas e sem aspas. ## Lógico obj_logico &lt;- TRUE # maiusculas e sem aspas ## Modo mode(obj_logico) #&gt; [1] &quot;logical&quot; Por fim, existe um modo pouco utilizado que cria números complexos (raiz de números negativos). ## Complexo obj_complexo &lt;- 1+1i ## Modo mode(obj_complexo) #&gt; [1] &quot;complex&quot; Podemos verificar o modo dos objetos ou fazer a conversão entre esses modos com diversas funções. ## Verificar o modo dos objetos is.numeric() is.integer() is.character() is.logical() is.complex() ## Conversões entre modos as.numeric() as.integer() as.character() as.logical() as.complex() 4.4.1.2 Estrutura dos objetos Uma vez entendido a natureza dos modos dos elementos dos objetos no R, podemos passar para o passo seguinte e entender como esses elementos são estruturados dentro dos objetos. Essa estruturação irá nos contar sobre a organização dos elementos, com relação aos modos e dimensionalidade da disposição dos elementos (Figura 4.3). De modo bem simples, os elementos podem ser estruturados em cinco tipos: Vetores e fatores: homogêneo (um modo) e unidimensional (uma dimensão). Um tipo especial de vetor são os fatores, usados para designar variáveis categóricas Matrizes: homogêneo (um modo) e bidimensional (duas dimensões) Arrays: homogêneo (um modo) e multidimensional (mais de duas dimensões) Data frames: heterogêneo (mais de um modo) e bidimensional (duas dimensões) Listas: heterogêneo (mais de um modo) e unidimensional (uma dimensão) Figura 4.3: Estruturas de dados mais comuns de R: vetores, matrizes, arrays, listas e data frames. Adaptado de: Grolemund (2014). 4.4.1.2.1 Vetor Vetores representam o encadeamento de elementos numa sequência unidimensional. No Capítulo 3, vimos o conceito de variável aleatória e seus tipos. No R, essas variáveis podem ser operacionalizadas como vetores. Dessa forma, essa estrutura de dados pode ser traduzida como medidas de uma variável numérica (discretas ou contínuas), variável binária (booleana - TRUE e FALSE) ou descrição (informações em texto). Há diversas formas de se criar um vetor no R: Concatenando elementos com a função c() Criando sequências unitárias : ou com a função seq() Criando repetições com a função rep() Colar palavras com uma sequência numérica com a função paste() ou paste0() Amostrando aleatoriamente elementos com a função sample() ## Concatenar elementos numéricos concatenar &lt;- c(15, 18, 20, 22, 18) concatenar #&gt; [1] 15 18 20 22 18 ## Sequência unitária (x1:x2) sequencia &lt;- 1:10 sequencia #&gt; [1] 1 2 3 4 5 6 7 8 9 10 ## Sequência com diferentes espaçamentos sequencia_esp &lt;- seq(from = 0, to = 100, by = 10) sequencia_esp #&gt; [1] 0 10 20 30 40 50 60 70 80 90 100 ## Repetição repeticao &lt;- rep(x = c(TRUE, FALSE), times = 5) repeticao #&gt; [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE ## Cola palavra e sequência numérica colar &lt;- paste(&quot;amostra&quot;, 1:5) colar #&gt; [1] &quot;amostra 1&quot; &quot;amostra 2&quot; &quot;amostra 3&quot; &quot;amostra 4&quot; &quot;amostra 5&quot; ## Amostragem aleatória amostragem &lt;- sample(x = 1:100, size = 10) amostragem #&gt; [1] 45 23 76 63 47 31 68 73 69 5 Como os vetores são homogêneos, i.e., só comportam um modo, quando combinamos mais de um modo no mesmo objeto ocorre uma dominância de modos. Existe, dessa forma, uma coerção dos elementos combinados para que todos fiquem iguais. Essa dominância segue essa ordem: DOMINANTE character &gt; double &gt; integer &gt; logical RECESSIVO Além disso, podemos utilizar as conversões listadas anteriormente para alterar os modos. Vamos exemplificar combinando os vetores criados anteriormente e convertendo-os. ## Coerção c(colar, amostragem) #&gt; [1] &quot;amostra 1&quot; &quot;amostra 2&quot; &quot;amostra 3&quot; &quot;amostra 4&quot; &quot;amostra 5&quot; &quot;45&quot; &quot;23&quot; #&gt; [8] &quot;76&quot; &quot;63&quot; &quot;47&quot; &quot;31&quot; &quot;68&quot; &quot;73&quot; &quot;69&quot; #&gt; [15] &quot;5&quot; ## Conversão as.numeric(repeticao) #&gt; [1] 1 0 1 0 1 0 1 0 1 0 4.4.1.2.2 Fator O fator representa medidas de uma variável categórica, podendo ser nominal ou ordinal. É fundamental destacar que fatores no R devem ser entendidos como um vetor de integer, i.e., ele é composto por números inteiros representando os níveis da variável categórica. Para criar um fator no R usamos uma função específica factor(), na qual podemos especificar os níveis com o argumento level, ou fazemos uma conversão usando a função as.factor(). Trabalhar com fatores no R Base não é das tarefas mais agradáveis, sendo assim, no Capítulo ?? usamos a versão tidyverse usando o pacote forcats. ## Fator nominal fator_nominal &lt;- factor(x = sample(x = c(&quot;floresta&quot;, &quot;pastagem&quot;, &quot;cerrado&quot;), size = 20, replace = TRUE), levels = c(&quot;floresta&quot;, &quot;pastagem&quot;, &quot;cerrado&quot;)) fator_nominal #&gt; [1] cerrado cerrado floresta pastagem pastagem cerrado cerrado pastagem cerrado floresta #&gt; [11] floresta floresta pastagem pastagem cerrado cerrado pastagem cerrado floresta pastagem #&gt; Levels: floresta pastagem cerrado ## Fator ordinal fator_ordinal &lt;- factor(x = sample(x = c(&quot;baixa&quot;, &quot;media&quot;, &quot;alta&quot;), size = 20, replace = TRUE), levels = c(&quot;baixa&quot;, &quot;media&quot;, &quot;alta&quot;), ordered = TRUE) fator_ordinal #&gt; [1] alta alta baixa media baixa media alta media baixa media baixa media alta baixa media #&gt; [16] media alta media baixa baixa #&gt; Levels: baixa &lt; media &lt; alta ## Conversão fator &lt;- as.factor(x = sample(x = c(&quot;floresta&quot;, &quot;pastagem&quot;, &quot;cerrado&quot;), size = 20, replace = TRUE)) fator #&gt; [1] cerrado pastagem floresta floresta cerrado cerrado pastagem cerrado pastagem cerrado #&gt; [11] pastagem cerrado pastagem pastagem floresta cerrado pastagem pastagem cerrado floresta #&gt; Levels: cerrado floresta pastagem 4.4.1.2.3 Matriz A matriz representa dados no formato de tabela, com linhas e colunas. As linhas representam unidades amostrais (locais, transectos, parcelas) e as colunas representam variáveis numéricas (discretas ou contínuas), variáveis binárias (TRUE ou FALSE) ou descrições (informações em texto). Podemos criar matrizes no R de duas formas. A primeira delas dispondo elementos de um vetor em um certo número de linhas e colunas com a função matrix(), podendo preencher essa matriz com os elementos do vetor por linhas ou por colunas alterando o argumento byrow. ## Vetor ve &lt;- 1:12 ## Matrix - preenchimento por linhas - horizontal ma_row &lt;- matrix(data = ve, nrow = 4, ncol = 3, byrow = TRUE) ma_row #&gt; [,1] [,2] [,3] #&gt; [1,] 1 2 3 #&gt; [2,] 4 5 6 #&gt; [3,] 7 8 9 #&gt; [4,] 10 11 12 ## Matrix - preenchimento por colunas - vertical ma_col &lt;- matrix(data = ve, nrow = 4, ncol = 3, byrow = FALSE) ma_col #&gt; [,1] [,2] [,3] #&gt; [1,] 1 5 9 #&gt; [2,] 2 6 10 #&gt; [3,] 3 7 11 #&gt; [4,] 4 8 12 A segundo forma, combinando vetores, utilizando a função rbind() para combinar vetores por linha, i.e., vetor embaixo do outro, e cbind() para combinar vetores por coluna, i.e., vetor ao lado do outro. ## Criar dois vetores vec_1 &lt;- c(1, 2, 3) vec_2 &lt;- c(4, 5, 6) ## Combinar por linhas - vertical - um embaixo do outro ma_rbind &lt;- rbind(vec_1, vec_2) ma_rbind #&gt; [,1] [,2] [,3] #&gt; vec_1 1 2 3 #&gt; vec_2 4 5 6 ## Combinar por colunas - horizontal - um ao lado do outro ma_cbind &lt;- cbind(vec_1, vec_2) ma_cbind #&gt; vec_1 vec_2 #&gt; [1,] 1 4 #&gt; [2,] 2 5 #&gt; [3,] 3 6 4.4.1.2.4 Array O array representa combinação de tabelas, com linhas, colunas e dimensões. Essa combinação pode ser feita em múltiplas dimensões, mas apesar disso, geralmente é mais comum o uso em Ecologia para três dimensões, por exemplo: linhas (unidades amostrais), colunas (espécies) e dimensão (tempo). Isso gera um cubo mágico ou cartas de um baralho, onde podemos comparar, nesse caso, comunidades ao longo do tempo. Além disso, arrays também são muito comuns em morfometria geométrica ou sensoriamento remoto. Podemos criar arrays no R dispondo elementos de um vetor em um certo número de linhas, colunas e dimensões com a função array(). Em nosso exemplo, vamos compor cinco comunidades de cinco espécies ao longo de três períodos. ## Array ar &lt;- array(data = sample(x = c(0, 1), size = 75, rep = TRUE), dim = c(5, 5, 3)) ar #&gt; , , 1 #&gt; #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 0 1 1 0 #&gt; [2,] 1 0 1 1 0 #&gt; [3,] 1 1 1 1 1 #&gt; [4,] 0 1 0 1 0 #&gt; [5,] 1 0 1 0 0 #&gt; #&gt; , , 2 #&gt; #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 0 0 0 1 0 #&gt; [2,] 0 1 0 1 0 #&gt; [3,] 1 1 1 1 0 #&gt; [4,] 0 0 0 0 1 #&gt; [5,] 0 0 0 0 0 #&gt; #&gt; , , 3 #&gt; #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 0 1 1 1 1 #&gt; [2,] 0 0 0 0 0 #&gt; [3,] 0 1 1 1 1 #&gt; [4,] 0 0 1 0 0 #&gt; [5,] 0 1 0 1 0 4.4.1.2.5 Data frame O data frame também representa dados no formato de tabela, com linhas e colunas, muito semelhante à matriz. Mas diferentemente das matrizes, os data frames comportam mais de um modo em suas colunas. Dessa forma, as linhas do data frame ainda representam unidades amostrais (locais, transectos, parcelas), mas as colunas agora podem representar descrições (informações em texto), variáveis numéricas (discretas ou contínuas), variáveis binárias (TRUE ou FALSE) e variáveis categóricas (nominais ou ordinais). A forma mais simples de criar data frames no R é através da combinação de vetores. Essa combinação é feita com a função data.frame() e ocorre de forma horizontal, semelhante à função cbind(). Sendo assim, todos os vetores precisam ter o mesmo número de elementos, ou seja, o mesmo comprimento. Podemos ainda nomear as colunas de cada vetor. ## Criar três vetores vec_ch &lt;- c(&quot;sp1&quot;, &quot;sp2&quot;, &quot;sp3&quot;) vec_nu &lt;- c(4, 5, 6) vec_fa &lt;- factor(c(&quot;campo&quot;, &quot;floresta&quot;, &quot;floresta&quot;)) ## Data frame - combinar por colunas - horizontal - um ao lado do outro df &lt;- data.frame(vec_ch, vec_nu, vec_fa) df #&gt; vec_ch vec_nu vec_fa #&gt; 1 sp1 4 campo #&gt; 2 sp2 5 floresta #&gt; 3 sp3 6 floresta ## Data frame - nomear as colunas df &lt;- data.frame(especies = vec_ch, abundancia = vec_nu, vegetacao = vec_fa) df #&gt; especies abundancia vegetacao #&gt; 1 sp1 4 campo #&gt; 2 sp2 5 floresta #&gt; 3 sp3 6 floresta 4.4.1.2.6 Lista A lista é um tipo especial de vetor que aceita objetos como elementos. Ela é a estrutura de dados utilizada para agrupar objetos, e é geralmente a saída de muitas funções. Podemos criar listas através da função list(). Essa função funciona de forma semelhante à função c() para a criação de vetores, mas agora estamos concatenando objetos. Podemos ainda nomear os elementos (objetos) que estamos combinando. Um ponto interessante para entender data frames, é que eles são listas, em que todos os elementos (colunas) possuem o mesmo número de elementos, ou seja, mesmo comprimento. ## Lista lista &lt;- list(rep(1, 20), # vector factor(1, 1), # factor cbind(c(1, 2), c(1, 2))) # matrix lista #&gt; [[1]] #&gt; [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; #&gt; [[2]] #&gt; [1] 1 #&gt; Levels: 1 #&gt; #&gt; [[3]] #&gt; [,1] [,2] #&gt; [1,] 1 1 #&gt; [2,] 2 2 ## Lista - nomear os elementos lista_nome &lt;- list(vector = rep(1, 20), # vector factor = factor(1, 1), # factor matrix = cbind(c(1, 2), c(1, 2))) # matrix lista_nome #&gt; $vector #&gt; [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; #&gt; $factor #&gt; [1] 1 #&gt; Levels: 1 #&gt; #&gt; $matrix #&gt; [,1] [,2] #&gt; [1,] 1 1 #&gt; [2,] 2 2 4.4.1.2.7 Funções Uma última estrutura de objetos criados no R são as funções. Elas são objetos criados pelo usuário e reutilizados para fazer operações específicas. A criação de funções geralmente é um tópico tratado num segundo momento, quando o usuário de R adquire certo conhecimento da linguagem. Aqui abordaremos apenas seu funcionamento básico, diferenciando sua estrutura para entendimento e sua diferenciação das demais estruturas. Vamos criar uma função simples que retorna a multiplicação de dois termos. Criaremos a função com o nome multi, à qual será atribuída uma função com o nome function(), com dois argumentos x e y. Depois disso abrimos chaves {}, que é onde iremos incluir nosso bloco de código. Nosso bloco de código é composto por duas linhas, a primeira contendo a operação de multiplicação dos argumento com a atribuição ao objeto mu e a sugunda contendo a função return() para retornar o valor da multiplicação. ## Criar uma função multi &lt;- function(x, y){ mu &lt;- (x * y) return(mu) } multi #&gt; function(x, y){ #&gt; #&gt; mu &lt;- (x * y) #&gt; return(mu) #&gt; #&gt; } ## Uso da função multi(42, 42) #&gt; [1] 1764 4.4.2 Manipulação de objetos unidimensionais Vamos agora explorar formas de manipular elementos de objetos unidimensionais, ou seja, vetores, fatores e listas. A primeira forma de manipulação é através da indexação, utilizando os operadores []. Com a indexação podemos acessar elementos de vetores e fatores por sua posição. Utilizaremos números, sequência de números ou operações booleanas para retornar partes dos vetores ou fatores. Podemos ainda retirar elementos dessas estruturas com o operador aritmético -. No exemplo a seguir, iremos fixar o ponto de partida da amostragem da função sample(), utilizando a função set.seed(42) (usamos 42 porque é a resposta para a vida, o universo e tudo mais - O Guia do Mochileiro das Galáxias, mas poderia ser outro número qualquer). Isso permite que o resultado da amostragem aleatório seja igual em diferentes computadores. ## Fixar a amostragem set.seed(42) ## Amostrar 10 elementos de uma sequência ve &lt;- sample(x = seq(0, 2, .05), size = 10) ve #&gt; [1] 1.80 0.00 1.20 0.45 1.75 0.85 1.15 0.30 1.90 0.20 ## Seleciona o quinto elemento ve[5] #&gt; [1] 1.75 ## Seleciona os elementos de 1 a 5 ve[1:5] #&gt; [1] 1.80 0.00 1.20 0.45 1.75 ## Retira o decimo elemento ve[-10] #&gt; [1] 1.80 0.00 1.20 0.45 1.75 0.85 1.15 0.30 1.90 ## Retira os elementos 2 a 9 ve[-(2:9)] #&gt; [1] 1.8 0.2 Podemos ainda fazer uma seleção condicional do vetor. Ao utilizarmos operadores relacionais, teremos como resposta um vetor lógico. Esse vetor lógico pode ser utilizado dentro da indexação para seleção de elementos. ## Quais valores sao maiores que 1? ve &gt; 1 #&gt; [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE ## Valores acima de 1 ve[ve &gt; 1] #&gt; [1] 1.80 1.20 1.75 1.15 1.90 Além da indexação, temos algumas funções que nos auxiliam em algumas operações com objetos unidimensionais, listadas na Tabela 4.2. Tabela 4.2: Funções para verificação e resumo de dados unidimensionais. Função Descrição max() Valor máximo min() Valor mínimo range() Amplitude length() Comprimento sum() Soma cumsum() Soma cumulativa prod() Produto sqrt() Raiz quadrada abs() Valor absoluto exp() Expoente log() Logaritmo natural log1p() Logaritmo natural mais 1 log(x + 1) log2() Logaritmo base 2 log10() Logaritmo base 10 mean() Média mean.weighted() Média ponderada var() Variância sd() Desvio Padrão mediam() Mediana quantile() Quantil quarters() Quartil IQR() Amplitude interquartil round() Arredondamento sort() Ordenação order() Posição ordenada rev() Reverso unique() Únicos summary() Resumo estatístico cut() Divide variável contínua em fator pretty() Divide variável contínua em intervalos scale() Padronização e centralização sub() Substitui caracteres grep() Posição de caracteres any() Algum valor? all() Todos os valores? which() Quais valores? subset() Subconjunto ifelse() Operação condicional Para listas, também podemos usar a indexação [] para acessar ou retirar elementos. ## Lista li &lt;- list(elem1 = 1, elem2 = 2, elem3 = 3) ## Acessar o primeiro elemento li[1] #&gt; $elem1 #&gt; [1] 1 ## Retirar o primeiro elemento li[-1] #&gt; $elem2 #&gt; [1] 2 #&gt; #&gt; $elem3 #&gt; [1] 3 Podemos ainda usar a indexação dupla [[]] para acessar os valores desses elementos. ## Acessar o valor do primeiro elemento li[[1]] #&gt; [1] 1 ## Acessar o valor do segundo elemento li[[2]] #&gt; [1] 2 Para listas nomeadas, podemos ainda utilizar o operador $ para acessar elementos pelo nome. ## Acessar o primeiro elemento li$elem1 #&gt; [1] 1 E ainda podemos utilizar funções para medir o comprimento dessa lista, listar os nomes dos elementos ou ainda renomear os elementos: length() e names(). ## Comprimento length(li) #&gt; [1] 3 ## Nomes names(li) #&gt; [1] &quot;elem1&quot; &quot;elem2&quot; &quot;elem3&quot; ## Renomear names(li) &lt;- paste0(&quot;elemento0&quot;, 1:3) li #&gt; $elemento01 #&gt; [1] 1 #&gt; #&gt; $elemento02 #&gt; [1] 2 #&gt; #&gt; $elemento03 #&gt; [1] 3 4.4.3 Manipulação de objetos multidimensionais Da mesma forma que para objetos unidimensionais, podemos manipular elementos de objetos multidimensionais, ou seja, matrizes, data frames e arrays. Novamente, a primeira forma de manipulação é através da indexação, utilizando os operadores []. Com a indexação podemos acessar elementos de matrizes, data frames e arrays por sua posição. Podemos ainda retirar elementos dessas estruturas com o operador aritmético -. Entretanto, agora temos mais de uma dimensão na estruturação dos elementos dentro dos objetos. Assim, utilizamos números, sequência de números ou operação booleanas para retornar partes desses objetos, mas as dimensões têm de ser explicitadas e separadas por vírgulas para acessar linhas e colunas. Essa indexação funciona para matrizes e data frames. Para arrays, especificamos também as dimensões, também separadas por vírgulas para acessar essas dimensões. ## Matriz ma &lt;- matrix(1:12, 4, 3) ma #&gt; [,1] [,2] [,3] #&gt; [1,] 1 5 9 #&gt; [2,] 2 6 10 #&gt; [3,] 3 7 11 #&gt; [4,] 4 8 12 ## Indexação ma[3, ] # linha 3 #&gt; [1] 3 7 11 ma[, 2] # coluna 2 #&gt; [1] 5 6 7 8 ma[1, 2] # elemento da linha 1 e coluna 2 #&gt; [1] 5 ma[1, 1:2] # elementos da linha 1 e coluna 1 e 2 #&gt; [1] 1 5 ma[1, c(1, 3)] # elementos da linha 1 e coluna 1 e 3 #&gt; [1] 1 9 ma[-1, ] # retirar a linha 1 #&gt; [,1] [,2] [,3] #&gt; [1,] 2 6 10 #&gt; [2,] 3 7 11 #&gt; [3,] 4 8 12 ma[, -3] # retirar a coluna 3 #&gt; [,1] [,2] #&gt; [1,] 1 5 #&gt; [2,] 2 6 #&gt; [3,] 3 7 #&gt; [4,] 4 8 Para data frames, além de utilizar números e/ou sequências de números dentro do operador [] simples, assim como podemos utilizar o operador [[]] duplo para retornar apenas os valores de uma linha ou uma coluna. Se as colunas estiverem nomeadas, podemos utilizar o nome da coluna de interesse entre aspas dentro dos operadores [] (retornar coluna) e [[]] (retornar apenas os valores), assim como ainda podemos utilizar o operador $ para data frames. Essas últimas operações retornam um vetor, para o qual podemos fazer operações de vetores ou ainda atualizar o valor dessa coluna selecionada ou adicionar outra coluna. ## Criar três vetores sp &lt;- paste(&quot;sp&quot;, 1:10, sep = &quot;&quot;) abu &lt;- 1:10 flo &lt;- factor(rep(c(&quot;campo&quot;, &quot;floresta&quot;), each = 5)) ## data frame df &lt;- data.frame(sp, abu, flo) df #&gt; sp abu flo #&gt; 1 sp1 1 campo #&gt; 2 sp2 2 campo #&gt; 3 sp3 3 campo #&gt; 4 sp4 4 campo #&gt; 5 sp5 5 campo #&gt; 6 sp6 6 floresta #&gt; 7 sp7 7 floresta #&gt; 8 sp8 8 floresta #&gt; 9 sp9 9 floresta #&gt; 10 sp10 10 floresta ## [] - números df[, 1] #&gt; [1] &quot;sp1&quot; &quot;sp2&quot; &quot;sp3&quot; &quot;sp4&quot; &quot;sp5&quot; &quot;sp6&quot; &quot;sp7&quot; &quot;sp8&quot; &quot;sp9&quot; &quot;sp10&quot; ## [] - nome das colunas - retorna coluna df[&quot;flo&quot;] #&gt; flo #&gt; 1 campo #&gt; 2 campo #&gt; 3 campo #&gt; 4 campo #&gt; 5 campo #&gt; 6 floresta #&gt; 7 floresta #&gt; 8 floresta #&gt; 9 floresta #&gt; 10 floresta ## [[]] - nome das colunas - retorna apenas os valores df[[&quot;flo&quot;]] #&gt; [1] campo campo campo campo campo floresta floresta floresta floresta floresta #&gt; Levels: campo floresta ## $ funciona apenas para data frame df$sp #&gt; [1] &quot;sp1&quot; &quot;sp2&quot; &quot;sp3&quot; &quot;sp4&quot; &quot;sp5&quot; &quot;sp6&quot; &quot;sp7&quot; &quot;sp8&quot; &quot;sp9&quot; &quot;sp10&quot; ## Operação de vetors length(df$abu) #&gt; [1] 10 ## Converter colunas df$abu &lt;- as.character(df$abu) mode(df$abu) #&gt; [1] &quot;character&quot; ## Adicionar colunas set.seed(42) df$abu2 &lt;- sample(x = 0:1, size = nrow(df), rep = TRUE) df #&gt; sp abu flo abu2 #&gt; 1 sp1 1 campo 0 #&gt; 2 sp2 2 campo 0 #&gt; 3 sp3 3 campo 0 #&gt; 4 sp4 4 campo 0 #&gt; 5 sp5 5 campo 1 #&gt; 6 sp6 6 floresta 1 #&gt; 7 sp7 7 floresta 1 #&gt; 8 sp8 8 floresta 1 #&gt; 9 sp9 9 floresta 0 #&gt; 10 sp10 10 floresta 1 Podemos ainda fazer seleções condicionais para retornar linhas com valores que temos interesse, semelhante ao uso de filtro de uma planilha eletrônica. ## Selecionar linhas de uma matriz ou data frame df[df$abu &gt; 4, ] #&gt; sp abu flo abu2 #&gt; 5 sp5 5 campo 1 #&gt; 6 sp6 6 floresta 1 #&gt; 7 sp7 7 floresta 1 #&gt; 8 sp8 8 floresta 1 #&gt; 9 sp9 9 floresta 0 df[df$flo == &quot;floresta&quot;, ] #&gt; sp abu flo abu2 #&gt; 6 sp6 6 floresta 1 #&gt; 7 sp7 7 floresta 1 #&gt; 8 sp8 8 floresta 1 #&gt; 9 sp9 9 floresta 0 #&gt; 10 sp10 10 floresta 1 Além disso, há uma série de funções para conferência e manipulação de dados que listamos na Tabela 4.3. Tabela 4.3: Funções para verificação e resumo de dados multidimensionais. Função Descrição head() Mostra as primeiras 6 linhas tail() Mostra as últimas 6 linhas nrow() Mostra o número de linhas ncol() Mostra o número de colunas dim() Mostra o número de linhas e de colunas rownames() Mostra os nomes das linhas (locais) colnames() Mostra os nomes das colunas (variáveis) str() Mostra as classes de cada coluna (estrutura) summary() Mostra um resumo dos valores de cada coluna rowSums() Calcula a soma das linhas (horizontal) colSums() Calcula a soma das colunas (vertical) rowMeans() Calcula a média das linhas (horizontal) colMeans() Calcula a média das colunas (vertical) str() Mostra a estrutura dos dados table() Tabulação cruzada t() Matriz ou data frame transposto 4.4.4 Valores faltantes e especiais Valores faltantes e especiais são valores reservados que representam dados faltantes, indefinições matemáticas, infinitos e objetos nulos. NA (Not Available): significa dado faltante ou indisponível NaN (Not a Number): representa indefinições matemáticas Inf (Infinito): é um número muito grande ou um limite matemático NULL (Nulo): representa um objeto nulo, sendo útil para preenchimento em aplicações de programação ## Data frame com elemento NA df &lt;- data.frame(var1 = c(1, 4, 2, NA), var2 = c(1, 4, 5, 2)) df #&gt; var1 var2 #&gt; 1 1 1 #&gt; 2 4 4 #&gt; 3 2 5 #&gt; 4 NA 2 ## Resposta booleana para elementos NA is.na(df) #&gt; var1 var2 #&gt; [1,] FALSE FALSE #&gt; [2,] FALSE FALSE #&gt; [3,] FALSE FALSE #&gt; [4,] TRUE FALSE ## Algum elemento é NA? any(is.na(df)) #&gt; [1] TRUE ## Remover as linhas com NAs df_sem_na &lt;- na.omit(df) df_sem_na #&gt; var1 var2 #&gt; 1 1 1 #&gt; 2 4 4 #&gt; 3 2 5 ## Substituir NAs por 0 df[is.na(df)] &lt;- 0 df #&gt; var1 var2 #&gt; 1 1 1 #&gt; 2 4 4 #&gt; 3 2 5 #&gt; 4 0 2 ## Desconsiderar os NAs em funções com o argumento rm.na = TRUE sum(1, 2, 3, 4, NA, na.rm = TRUE) #&gt; [1] 10 ## NaN - not a number 0/0 #&gt; [1] NaN log(-1) #&gt; [1] NaN ## Limite matemático 1/0 #&gt; [1] Inf ## Número grande 10^310 #&gt; [1] Inf ## Objeto nulo nulo &lt;- NULL nulo #&gt; NULL 4.4.5 Diretório de trabalho O diretório de trabalho é o endereço da pasta (ou diretório) de onde o R importará ou exportar nossos dados. Podemos utilizar o próprio RStudio para tal tarefa, indo em Session &gt; Set Work Directory &gt; Choose Directory... ou simplesmente utilizar o atalho Ctrl + Shift + H. Podemos ainda utilizar as funções do R para definir o diretório. Para tanto, podemos navegar com o aplicativo de gerenciador de arquivos (e.g., Windows Explorer) até nosso diretório de interesse e copiar o endereço na barra superior. Voltamos para o R e colamos esse endereço entre aspas como um argumento da função setwd(). É fundamental destacar que em Sistemas Operacionais Windows é necessário inverter as barras (\\ por /). Aconselhamos ainda utilizar as funções getwd() para retornar o diretório definido na sessão do R, assim como as funções dir() ou list.files() para listagem dos arquivos no diretório, ambas medidas de conferência do diretório correto. ## Definir o diretório de trabalho setwd(&quot;/home/mude/data/github/livro_r_ecologia/dados&quot;) ## Verificar o diretório getwd() ## Listar os arquivos no diretório dir() list.files() Outra forma de definir o diretório é digitar a tecla tab dentro da função setwd(\"tab\"). Quando apertamos a tab dentro das aspas conseguimos selecionar o diretório manualmente, pois abre-se uma lista de diretório que podemos ir selecionando até chegar no diretório de interesse. ## Mudar o diretório com a tecla tab setwd(&quot;`tab`&quot;) 4.4.6 Importar dados Uma das operações mais corriqueiras do R, antes de realizar alguma análise ou plotar um gráfico, é a de importar dados que foram tabulados numa planilha eletrônica e salvos no formato .csv, .txt ou .xlsx. Ao importar esse tipo de dado para o R, o formato que o mesmo assume, se nenhum parâmetro for especificado, é o da classe data frame, prevendo que a planilha de dados possua colunas com diferentes modos. Existem diversas formas de importar dados para o R. Podemos importar utilizando o RStudio, indo na janela Environment (Figura 4.2 (3)) e clicar em Importar Dataset. Entretanto, aconselhamos o uso de funções que fiquem salvas em um script para aumentar a reprodutibilidade do mesmo. Dessa forma, as três principais funções para importar os arquivos nos três principais extensões (.csv, .txt ou .xlsx) são, respectivamente: read.csv(), read.table() e openxlsx::read.xlsx(), sendo o último do pacote openxlsx. ## Instalar o pacote openxlsx install.packages(&quot;openxlsx&quot;) library(openxlsx) Para exemplificar como importar dados vamos usar os dados de comunidades de anfíbios da Mata Atlântica (Atlantic Amphibians, Vancine et al. (2018)). Faremos o download diretamente do site da fonte dos dados. Vamos antes escolher um diretório de trabalho com a função setwd(), e em seguida criar um diretório com a função dir.create() chamado dados. Em seguida, vamos mudar nosso diretório para essa pasta e criar mais um diretório chamado tabelas, e por fim, definir esse diretório para que o conteúdo do download seja armazenado ali. ## Escolher um diretório setwd(&quot;/home/mude/data/github/livro_r_ecologia&quot;) ## Criar um diretório &#39;dados&#39; dir.create(&quot;dados&quot;) ## Escolher diretório &#39;dados&#39; setwd(&quot;dados&quot;) ## Criar um diretório &#39;tabelas&#39; dir.create(&quot;tabelas&quot;) ## Escolher diretório &#39;tabelas&#39; setwd(&quot;tabelas&quot;) Agora podemos fazer o download do arquivo .zip e extrair as tabelas usando a função unzip() nesse mesmo diretório. ## Download download.file(url = &quot;https://esajournals.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1002%2Fecy.2392&amp;file=ecy2392-sup-0001-DataS1.zip&quot;, destfile = &quot;atlantic_amphibians.zip&quot;, mode = &quot;wb&quot;) ## Unzip unzip(zipfile = &quot;atlantic_amphibians.zip&quot;) Agora podemos importar a tabela de dados com a função read.csv(), atribuindo ao objeto intror_anfibios_locais. Devemos atentar para o argumento encoding, que selecionamos aqui como latin1 para corrigir um erro do autor dos dados que publicou esse data paper com erros ## Importar a tabela de locais intror_anfibios_locais &lt;- read.csv(&quot;dados/tabelas/ATLANTIC_AMPHIBIANS_sites.csv&quot;, encoding = &quot;latin1&quot;) Esse arquivo foi criado com separador de decimais sendo . e separador de colunas sendo ,. Caso tivesse sido criado com separador de decimais sendo , e separador de colunas sendo ;, usaríamos a função read.csv2(). Para outros formatos, basta usar as outras funções apresentadas, atentando-se para os argumentos específicos de cada função. Outra forma de importar dados, principalmente quando não sabemos exatamente o nome do arquivo e também para evitar erros de digitação, é utilizar a tecla tab dentro das aspas da função de importação. Dessa forma, conseguimos ter acesso aos arquivos do nosso diretório e temos a possibilidade de selecioná-los sem erros de digitação. ## Importar usando a tecla tab intror_anfibios_locais &lt;- read.csv(&quot;`tab`&quot;) intror_anfibios_locais Caso o download não funcione ou haja problemas com a importação, disponibilizamos os dados também no pacote ecodados. ## Importar os dados pelo pacote ecodados data(intror_anfibios_locais) head(intror_anfibios_locais) #&gt; id reference_number species_number record sampled_habitat active_methods passive_methods #&gt; 1 amp1001 1001 19 ab fo,ll as pt #&gt; 2 amp1002 1002 16 co fo,la,ll as pt #&gt; 3 amp1003 1002 14 co fo,la,ll as pt #&gt; 4 amp1004 1002 13 co fo,la,ll as pt #&gt; 5 amp1005 1003 30 co fo,ll,br as &lt;NA&gt; #&gt; 6 amp1006 1004 42 co tp,pp,la,ll,is &lt;NA&gt; &lt;NA&gt; #&gt; complementary_methods period month_start year_start month_finish year_finish effort_months #&gt; 1 &lt;NA&gt; mo,da,tw,ni 9 2000 1 2002 16 #&gt; 2 &lt;NA&gt; mo,da,tw,ni 12 2007 5 2009 17 #&gt; 3 &lt;NA&gt; mo,da,tw,ni 12 2007 5 2009 17 #&gt; 4 &lt;NA&gt; mo,da,tw,ni 12 2007 5 2009 17 #&gt; 5 &lt;NA&gt; mo,da,ni 7 1988 8 2001 157 #&gt; 6 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA #&gt; country state state_abbreviation municipality site #&gt; 1 Brazil Piauí BR-PI Canto do Buriti Parque Nacional Serra das Confusões #&gt; 2 Brazil Ceará BR-CE São Gonçalo do Amarante Dunas #&gt; 3 Brazil Ceará BR-CE São Gonçalo do Amarante Jardim Botânico Municipal de Bauru #&gt; 4 Brazil Ceará BR-CE São Gonçalo do Amarante Taíba #&gt; 5 Brazil Ceará BR-CE Baturité Serra de Baturité #&gt; 6 Brazil Ceará BR-CE Quebrangulo Reserva Biológica de Pedra Talhada #&gt; latitude longitude coordinate_precision altitude temperature precipitation #&gt; 1 -8.680000 -43.42194 gm 543 24.98 853 #&gt; 2 -3.545527 -38.85783 dd 15 26.53 1318 #&gt; 3 -3.574194 -38.88869 dd 29 26.45 1248 #&gt; 4 -3.515250 -38.91880 dd 25 26.55 1376 #&gt; 5 -4.280556 -38.91083 gm 750 21.35 1689 #&gt; 6 -9.229167 -36.42806 &lt;NA&gt; 745 20.45 1249 4.4.7 Conferência dos dados importados Uma vez importados os dados para o R, geralmente antes de iniciarmos qualquer manipulação, visualização ou análise de dados, fazemos a conferência desses dados. Para isso, podemos utilizar as funções listadas na Tabela 4.3. Dentre todas essas funções de verificação, destacamos a importância destas funções apresentadas abaixo para saber se as variáveis foram importadas e interpretadas corretamente e reconhecer erros de digitação, por exemplo. ## Primeiras linhas head(intror_anfibios_locais) #&gt; id reference_number species_number record sampled_habitat active_methods passive_methods #&gt; 1 amp1001 1001 19 ab fo,ll as pt #&gt; 2 amp1002 1002 16 co fo,la,ll as pt #&gt; 3 amp1003 1002 14 co fo,la,ll as pt #&gt; 4 amp1004 1002 13 co fo,la,ll as pt #&gt; 5 amp1005 1003 30 co fo,ll,br as &lt;NA&gt; #&gt; 6 amp1006 1004 42 co tp,pp,la,ll,is &lt;NA&gt; &lt;NA&gt; #&gt; complementary_methods period month_start year_start month_finish year_finish effort_months #&gt; 1 &lt;NA&gt; mo,da,tw,ni 9 2000 1 2002 16 #&gt; 2 &lt;NA&gt; mo,da,tw,ni 12 2007 5 2009 17 #&gt; 3 &lt;NA&gt; mo,da,tw,ni 12 2007 5 2009 17 #&gt; 4 &lt;NA&gt; mo,da,tw,ni 12 2007 5 2009 17 #&gt; 5 &lt;NA&gt; mo,da,ni 7 1988 8 2001 157 #&gt; 6 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA #&gt; country state state_abbreviation municipality site #&gt; 1 Brazil Piauí BR-PI Canto do Buriti Parque Nacional Serra das Confusões #&gt; 2 Brazil Ceará BR-CE São Gonçalo do Amarante Dunas #&gt; 3 Brazil Ceará BR-CE São Gonçalo do Amarante Jardim Botânico Municipal de Bauru #&gt; 4 Brazil Ceará BR-CE São Gonçalo do Amarante Taíba #&gt; 5 Brazil Ceará BR-CE Baturité Serra de Baturité #&gt; 6 Brazil Ceará BR-CE Quebrangulo Reserva Biológica de Pedra Talhada #&gt; latitude longitude coordinate_precision altitude temperature precipitation #&gt; 1 -8.680000 -43.42194 gm 543 24.98 853 #&gt; 2 -3.545527 -38.85783 dd 15 26.53 1318 #&gt; 3 -3.574194 -38.88869 dd 29 26.45 1248 #&gt; 4 -3.515250 -38.91880 dd 25 26.55 1376 #&gt; 5 -4.280556 -38.91083 gm 750 21.35 1689 #&gt; 6 -9.229167 -36.42806 &lt;NA&gt; 745 20.45 1249 ## Últimas linhas tail(intror_anfibios_locais) #&gt; id reference_number species_number record sampled_habitat active_methods passive_methods #&gt; 1158 amp2158 1389 3 co &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 1159 amp2159 1389 9 co &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 1160 amp2160 1389 6 co &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 1161 amp2161 1389 1 co &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 1162 amp2162 1389 2 co &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 1163 amp2163 1389 2 co &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; complementary_methods period month_start year_start month_finish year_finish effort_months #&gt; 1158 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA #&gt; 1159 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA #&gt; 1160 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA #&gt; 1161 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA #&gt; 1162 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA #&gt; 1163 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA #&gt; country state state_abbreviation municipality site latitude #&gt; 1158 Argentina Misiones AR-N Manuel Belgrano Comandante Andresito -25.66944 #&gt; 1159 Argentina Misiones AR-N Posadas Posadas -27.45333 #&gt; 1160 Argentina Misiones AR-N Montecarlo Montecarlo -26.56889 #&gt; 1161 Argentina Misiones AR-N San Pedro Refugio Moconá -27.14083 #&gt; 1162 Argentina Misiones AR-N Cainguás Balneario Municipal Cuñá Pirú -27.08722 #&gt; 1163 Argentina Misiones AR-N Oberá Chacra San Juan de Dios -27.47333 #&gt; longitude coordinate_precision altitude temperature precipitation #&gt; 1158 -54.04556 gms 251 19.94 1780 #&gt; 1159 -55.89250 gms 105 21.30 1768 #&gt; 1160 -53.60889 gms 597 18.35 1954 #&gt; 1161 -53.92611 gms 202 19.92 1850 #&gt; 1162 -54.95278 gms 213 21.04 1553 #&gt; 1163 -55.17194 gms 254 20.67 1683 ## Número de linhas e colunas nrow(intror_anfibios_locais) #&gt; [1] 1163 ncol(intror_anfibios_locais) #&gt; [1] 25 dim(intror_anfibios_locais) #&gt; [1] 1163 25 ## Nome das linhas e colunas rownames(intror_anfibios_locais) colnames(intror_anfibios_locais) ## Estrutura dos dados str(intror_anfibios_locais) ## Resumo dos dados summary(intror_anfibios_locais) ## Verificar NAs any(is.na(intror_anfibios_locais)) which(is.na(intror_anfibios_locais)) ## Remover as linhas com NAs intror_anfibios_locais_na &lt;- na.omit(intror_anfibios_locais) 4.4.8 Exportar dados Uma vez realizado as operações de manipulação ou tendo dados que foram analisados e armazenados num objeto no formato de data frame ou matriz, podemos exportar esses dados do R para o diretório que definimos anteriormente. Para tanto, podemos utilizar funções de escrita de dados, como write.csv(), write.table() e openxlsx::write.xlsx(). Dois pontos são fundamentais: 1. o nome do arquivo tem de estar entre aspas e no final dele deve constar a extensão que pretendemos que o arquivo tenha; 2. é interessante utilizar os argumentos row.names = FALSE e quote=FALSE, para que o arquivo escrito não tenha o nome das linhas ou aspas em todas as células, respectivamente. ## Exportar dados na extensão .csv write.csv(intror_anfibios_locais_na, &quot;ATLANTIC_AMPHIBIAN_sites_na.csv&quot;, row.names = FALSE, quote = FALSE) ## Exportar dados na extensão .txt write.table(intror_anfibios_locais_na, &quot;ATLANTIC_AMPHIBIAN_sites_na.txt&quot;, row.names = FALSE, quote = FALSE) ## Exportar dados na extensão .xlsx openxlsx::write.xlsx(intror_anfibios_locais_na, &quot;ATLANTIC_AMPHIBIAN_sites_na.xlsx&quot;, row.names = FALSE, quote = FALSE) 4.5 Exercícios Use o R para verificar o resultado da operação 7 + 7 ÷ 7 + 7 x 7 - 7. Verifique através do R se 3x2³ é maior que 2x3². Crie dois objetos (qualquer nome) com os valores 100 e 300. Multiplique esses objetos (função prod()) e atribuam ao objeto mult. Faça o logaritmo natural (função log()) do objeto mult e atribuam ao objeto ln. Quantos pacotes existem no CRAN nesse momento? Execute essa combinação no Console: nrow(available.packages(repos = \"http://cran.r-project.org\")). Instale o pacote tidyverse do CRAN. Escolha números para jogar na mega-sena usando o R, nomeando o objeto como mega. Lembrando: são 6 valores de 1 a 60 e atribuam a um objeto. Crie um fator chamado tr, com dois níveis (cont e trat) para descrever 100 locais de amostragem, 50 de cada tratamento. O fator deve ser dessa forma cont, cont, cont, ...., cont, trat, trat, ...., trat. Crie uma matriz chamada ma, resultante da disposição de um vetor composto por 10000 valores aleatórios entre 0 e 10. A matriz deve conter 100 linhas e ser disposta por colunas. Crie um data frame chamado df, resultante da composição desses vetores: id: 1:50 sp: sp01, sp02, ..., sp49, sp50 ab: 50 valores aleatórios entre 0 a 5 Crie uma lista com os objetos criados anteriormente: mega, tr, ma e df. Selecione os elementos ímpares do objeto tr, e atribua ao objeto tr_impar. Selecione as linhas com ids pares do objeto df, e atribua ao objeto df_ids_par. Faça uma amostragem de 10 linhas do objeto df, e atribua ao objeto df_amos10. 4.6 Para se aprofundar Listamos a seguir livros e links com material que recomendamos para seguir com sua aprendizagem em R Base. 4.6.1 Livros Mayer F. P., Bonat W. H., Zeviani W. M., Krainski E. T., Ribeiro Jr. P. J. 2018. Estatística Computacional com R. [http://cursos.leg.ufpr.br/ecr/index.html] Zeviani W. M. 2019. Manual de Planejamento e Análise de Experimentos com R. [http://leg.ufpr.br/~walmes/mpaer/] Curso-R. 2021. Ciência de Dados em R. [https://livro.curso-r.com/] Adler J. 2012. R in a Nutshell: A Desktop Quick Reference. 2 ed. OReilly Media. Burns P. 2011. The R Inferno. [https://www.burns-stat.com/pages/Tutor/R_inferno.pdf] Cotton R. 2013. Learning R: A Step-by-Step Function Guide to Data Analysis. OReilly Media. Crawley MJ. 2012. The R Book. 2 ed. Wiley. Davies TM. 2016. The Book of R: A First Course in Programming and Statistics. No Starch Press. Douglas A, Roos D, Mancini F, Couto A, Lusseau D. An Introduction to R. 2021 [https://intro2r.com/] Engel C. 2019. Introduction to R. [https://cengel.github.io/R-intro/] Gillespie C., Lovelace R, 2016. Efficient R programming. OReilly Media. [https://bookdown.org/csgillespie/efficientR/] Grolemund G. 2014. Hands-On Programming with R. OReilly Media. Holmes S, Huber W. 2019. Modern Statistics for Modern Biology. Cambridge University Press. [https://www.huber.embl.de/msmb/] Irizarry RA, Love MI. 2016. Data Analysis for the Life Sciences with R. Chapman and Hall/CRC. James G, Witten D, Hastie T, Tibshirani R. 2013. An Introduction to Statistical Learning: with Applications in R. 2 ed. Springer. [http://faculty.marshall.usc.edu/gareth-james/ISL/] Kabacoff RI. 2015. R in Action: Data analysis and graphics with R. 2 ed. Manning. Lander JP. 2017. R for Everyone: Advanced Analytics and Graphics. Addison-Wesley Professional. Matloff N. 2011. The Art of R Programming: A Tour of Statistical Software Design. No Starch Press. Long JD, Teetor P. 2019. R Cookbook.2 ed. OReilly Media. [https://rc2e.com/] Wickham H. 2019. Advanced R. 2 ed. Chapman and Hall/CRC. [https://adv-r.hadley.nz/] Wickham H. 2015. R Packages: Organize, Test, Document, and Share Your Code. OReilly Media. [https://r-pkgs.org/] 4.6.2 Links Materiais sobre R R resources (free courses, books, tutorials, &amp; cheat sheets) Data Science for Ecologists and Environmental Scientists Data Analysis and Visualization in R for Ecologists A (very) shortintroduction to R - Paul Torfs &amp; Claudia Brauer R forBeginners - Emmanuel Paradis Referências "],["cap5.html", "Capítulo 5 Tidyverse Pré-requisitos do capítulo 5.1 Contextualização 5.2 tidyverse 5.3 here 5.4 readr, readxl e writexl 5.5 tibble 5.6 magrittr (pipe - %&gt;%) 5.7 tidyr 5.8 dplyr 5.9 stringr 5.10 forcats 5.11 lubridate 5.12 purrr 5.13 Exercícios 5.14 Para se aprofundar", " Capítulo 5 Tidyverse Pré-requisitos do capítulo Pacotes que serão utilizados nesse capítulo. ## Pacotes library(tidyverse) library(here) library(ggplot2) library(purrr) library(tibble) library(dplyr) library(tidyr) library(stringr) library(readr) library(forcats) library(palmerpenguins) library(lubridate) 5.1 Contextualização Como todo idioma, a linguagem R vem passando por transformações nos últimos anos. Grande parte dessas mudanças estão dentro do paradigma da Ciência de Dados (Data Science), uma nova área de conhecimento que vem se moldando a partir do desenvolvimento da sociedade em torno da era digital e da grande quantidade de dados gerados e disponíveis pela internet, de onde advém os pilares das inovações tecnológicas: Big Data, Machine Learning e Internet of Things. A grande necessidade de computação para desenvolver esse novo paradigma colocaram o R e o python como as principais linguagens de programação frente a esses novos desafios. Apesar de não serem as únicas ferramentas utilizadas para esse propósito, elas rapidamente se tornaram uma das melhores escolhas, dado vários fatores como: são de código-aberto e gratuitas, possuírem grandes comunidades contribuidoras, serem linguagens de interpretação (orientadas a objeto) e relativamente fáceis de serem aprendidas e aplicadas. Essas mudanças e expanções na utilização da linguagem R para a Ciência de Dados começaram a ser implementadas principalmente devido a um pesquisador: Hadley Wickham, que iniciou sua contribuição à comunidade R com o desenvolvimento do já consagrado pacote ggplot2 (Wickham 2016) para a composição de gráficos no R (ver mais no Capítulo 6), baseado na gramática de gráficos (Wilkinson and Wills 2005). Depois disso, Wickham dedicou-se ao desenvolvimento do pensamento de uma nova abordagem dentro da manipulação de dados, denominada Tidy Data (Dados organizados) (Wickham 2014), na qual focou na limpeza e organização de dados. A ideia postula que dados estão tidy quando: 1) variáveis estão nas colunas, 2) observações estão nas linhas e 3) valores estão nas células, sendo que para esse último, não deve haver mais de um valor por célula (Figura 5.2). A partir dessas ideias, o tidyverse foi operacionalizado no R como uma coleção de pacotes que atuam no workflow comum da ciência de dados: importação, manipulação, exploração, visualização, análise e comunicação de dados e análises (Wickham et al. 2019) (Figura 5.1). O principal objetivo do tidyverse é aproximar a linguagem para melhorar a interação entre ser humano e computador sobre dados, de modo que os pacotes compartilham uma filosofia de design de alto nível e gramática, além da estrutura de dados de baixo nível (Wickham et al. 2019). As principais leituras sobre o tema no R são os artigos (Wickham 2014) e (Wickham et al. 2019), e o livro (Wickham and Grolemund 2017), disponível on-line neste link, além do site que possui muito mais informações. Figura 5.1: Modelo das ferramentas necessárias em um projeto típico de ciência de dados: importar, organizar, entender (transformar, visualizar, modelar) e comunicar, envolto à essas ferramentas está a programação. Adaptado de: Wickham and Grolemund (2017). 5.2 tidyverse Uma vez instalado e carregado, o pacote tidyverse disponibiliza um conjunto de ferramentas através de vários pacotes. Esses pacotes compartilham uma filosofia de design, gramática e estruturas. Podemos entender o tidyverse como um dialeto novo para a linguagem R, onde tidy quer dizer organizado, arrumado, ordenado, e verse é universo. A seguir, listamos os principais pacotes e suas especificações. readr: importa dados tabulares (e.g. .csv e .txt) tibble: implementa a classe tibble tidyr: transformação de dados para tidy dplyr: manipulação de dados stringr: manipulação de caracteres forcats: manipulação de fatores ggplot2: possibilita a visualização de dados purrr: disponibiliza ferramentas para programação funcional Além dos pacotes principais, fazemos também menção a outros pacotes que estão dentro dessa abordagem e que trataremos ainda neste capítulo, em outro momento do livro, ou que você leitor(a) deve se familiarizar. Alguns pacotes compõem o tidyverse outros são mais gerais, entretanto, todos estão envolvidos de alguma forma com Data Science. readxl e writexl: importa e exporta dados tabulares (.xlsx) janitor: examinar e limpar dados sujos DBI: interface de banco de dados R haven: importa e exporta dados do SPSS, Stata e SAS httr: ferramentas para trabalhar com URLs e HTTP rvest: coletar facilmente (raspe) páginas da web xml2: trabalhar com arquivos XML jsonlite: um analisador e gerador JSON simples e robusto para R hms: hora do dia lubridate: facilita o tratamento de datas magrittr: provê os operadores pipe (%&gt;%, %$%, %&lt;&gt;%) glue: facilita combinar dados e caracteres rmarkdown: cria documentos de análise dinâmica que combinam código, saída renderizada (como figuras) e texto knitr: projetado para ser um mecanismo transparente para geração de relatórios dinâmicos com R shiny: framework de aplicativo Web para R flexdashboard: painéis interativos para R here: facilita a definição de diretórios usethis: automatiza tarefas durante a configuração e desenvolvimento de projetos (Git, GitHub e Projetos RStudio) data.table: pacote que fornece uma versão de alto desempenho do data.frame (importar, manipular e expotar) reticulate: pacote que fornece ferramentas para integrar Python e R sparklyr: interface R para Apache Spark broom: converte objetos estatísticos em tibbles organizados modelr: funções de modelagem que funcionam com o pipe tidymodels: coleção de pacotes para modelagem e aprendizado de máquina usando os princípios do tidyverse Destacamos a grande expansão e aplicabilidade dos pacotes rmarkdown, knitr e bookdown, que permitiram a escrita desse livro usando essas ferramentas. Para instalar os principais pacotes que integram o tidyverse podemos instalar o pacote tidyverse. ## Instalar o pacote tidyverse install.packages(&quot;tidyverse&quot;) Quando carregamos o pacote tidyverse podemos notar uma mensagem indicando quais pacotes foram carregados, suas respectivas versões e os conflitos com outros pacotes. ## Carregar o pacote tidyverse library(tidyverse) Podemos ainda listar todos os pacotes do tidyverse com a função tidyverse::tidyverse_packages(). ## Listar todos os pacotes do tidyverse tidyverse::tidyverse_packages() #&gt; [1] &quot;broom&quot; &quot;cli&quot; &quot;crayon&quot; &quot;dbplyr&quot; &quot;dplyr&quot; #&gt; [6] &quot;dtplyr&quot; &quot;forcats&quot; &quot;googledrive&quot; &quot;googlesheets4&quot; &quot;ggplot2&quot; #&gt; [11] &quot;haven&quot; &quot;hms&quot; &quot;httr&quot; &quot;jsonlite&quot; &quot;lubridate&quot; #&gt; [16] &quot;magrittr&quot; &quot;modelr&quot; &quot;pillar&quot; &quot;purrr&quot; &quot;readr&quot; #&gt; [21] &quot;readxl&quot; &quot;reprex&quot; &quot;rlang&quot; &quot;rstudioapi&quot; &quot;rvest&quot; #&gt; [26] &quot;stringr&quot; &quot;tibble&quot; &quot;tidyr&quot; &quot;xml2&quot; &quot;tidyverse&quot; Também podemos verificar se os pacotes estão atualizados, senão, podemos atualizá-los com a função tidyverse::tidyverse_update(). ## Verificar e atualizar os pacotes do tidyverse tidyverse::tidyverse_update(repos = &quot;http://cran.us.r-project.org&quot;) #&gt; The following packages are out of date: #&gt; #&gt; * dplyr (1.0.5 -&gt; 1.0.7) #&gt; * haven (2.3.1 -&gt; 2.4.3) #&gt; * hms (1.1.0 -&gt; 1.1.1) #&gt; * lubridate (1.7.10 -&gt; 1.8.0) #&gt; * readr (1.4.0 -&gt; 2.0.2) #&gt; * rlang (0.4.11 -&gt; 0.4.12) #&gt; * rvest (1.0.1 -&gt; 1.0.2) #&gt; * tibble (3.1.0 -&gt; 3.1.5) #&gt; * tidyr (1.1.3 -&gt; 1.1.4) #&gt; #&gt; Start a clean R session then run: #&gt; install.packages(c(&quot;dplyr&quot;, &quot;haven&quot;, &quot;hms&quot;, &quot;lubridate&quot;, &quot;readr&quot;, &quot;rlang&quot;, &quot;rvest&quot;, #&gt; &quot;tibble&quot;, &quot;tidyr&quot;)) Apesar de podermos fazer a instalação e carregamento de todos os pacotes juntos, usando o pacote tidyverse, podemos instalar e carregar os pacotes individualmente. ## Instalar os pacotes do tidyverse individualmente install.packages(c(&quot;ggplot2&quot;, &quot;purrr&quot;, &quot;tibble&quot;, &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;stringr&quot;, &quot;readr&quot;, &quot;forcats&quot;), dependencies = TRUE) ## Carregar os pacotes do tidyverse individualmente library(ggplot2) library(purrr) library(tibble) library(dplyr) library(tidyr) library(stringr) library(readr) library(forcats) Todas as funções dos pacotes tidyverse usam fonte minúscula e _ (underscore) para separar os nomes internos das funções, seguindo a mesma sintaxe do python (Snake Case). Neste sentido de padronização, é importante destacar ainda que existe um guia próprio para que os scripts sigam a recomendação de padronização, o The tidyverse style guide, criado pelo Hadley Wickham. Para pessoas que desenvolvem existe o Tidyverse design guide criado pelo Tidyverse team. ## Funções no formato snake case read_csv() read_xlsx() as_tibble() left_join() group_by() Por fim, para evitar possíveis conflitos de funções com o mesmo nome entre pacotes, recomendamos fortemente o hábito de usar as funções precedidas do operador :: e o respectivo pacote. Assim, garante-se que a função utilizada é referente ao pacote daquela função. Segue um exemplo com as funções apresentadas anteriormente. ## Funções seguidas de seus respectivos pacotes readr::read_csv() readxl::read_xlsx() tibble::as_tibble() dplyr::left_join() dplyr::group_by() Seguindo essas ideias do novo paradigma da Ciência de Dados, outro conjunto de pacotes foi desenvolvido, chamado de tidymodels que atuam no workflow da análise de dados em ciência de dados: separação e reamostragem, pré-processamento, ajuste de modelos e métricas de performasse de ajustes. Por razões de espaço e especificidade, não entraremos em detalhes desse pacote. Seguindo o workflow da Figura 5.1, iremos ver nos itens das próximas seções como esses passos são realizados com funções de cada pacote. 5.3 here Dentro do workflow do tidyverse, devemos sempre trabalhar com Projetos do RStudio. Junto com o projeto, também podemos fazer uso do pacote here. Ele permite construir caminhos para os arquivos do projeto de forma mais simples e com maior reprodutibilidade. Esse pacote cobre o ponto que discutimos no capítulo 4, dado que muitas vezes mudar o diretório com a função setwd() tende a ser demorado, principalmente quando se trata de um script em que várias pessoas estão trabalhando em diferentes computadores e sistemas operacionais. Além disso, ele elimina a questão da fragilidade dos scripts, pois geralmente um script está com os diretórios conectados exatamente a um lugar e a um momento. Por fim, ele também simplifica o trabalho com subdiretórios, facilitando importar ou exportar arquivos para subpastas. Seu uso é relativamente simples: uma vez criado e aberto o RStudio pelo Projeto do RStudio, o diretório automaticamente é definido para o diretório do projeto. Depois disso, podemos usar a função here::here() para definir os subdiretórios onde estão os dados. O exemplo da aplicação fica para a seção seguinte, quando iremos de fato importar um arquivo para o R. Logo abaixo, mostramos como instalar e carregar o pacote here. ## Instalar install.packages(&quot;here&quot;) ## Carregar library(here) 5.4 readr, readxl e writexl Dado que possuímos um conjunto de dados e que geralmente esse conjunto de dados estará no formato tabular com umas das extensões: .csv, .txt ou .xlsx, usaremos o pacote readr ou readxl para importar esses dados para o R. Esses pacotes leem e escrevem grandes arquivos de forma mais rápida, além de fornecerem medidores de progresso de importação e exportação, e imprimir a informação dos modos das colunas quando faz a importação. Outro ponto bastante positivo é que também classificam automaticamente o modo dos dados de cada coluna, i.e., se uma coluna possui dados numéricos ou apenas texto, essa informação será considerada para classificar o modo da coluna toda. A classe do objeto atribuído quando lido por esses pacotes é automaticamente um tibble, que veremos melhor na seção seguinte. Todas as funções deste pacote são listadas na página de referência do pacote. Usamos as funções readr::read_csv() e readr::write_csv() para importar e exportar arquivos .csv do R, respectivamente. Para dados com a extensão .txt, podemos utilizar as funções readr::read_tsv() ou ainda readr::read_delim(). Para arquivos tabulares com a extensão .xlsx, temos de instalar e carregar dois pacotes adicionais: readxl e writexl, dos quais usaremos as funções readxl::read_excel(), readxl::read_xlsx() ou readxl::read_xls() para importar dados, atentado para o fato de podermos indicar a aba com os dados com o argumento sheet, e writexl::write_xlsx() para exportar. ## Instalar install.packages(&quot;writexl&quot;) ## Carregar library(readxl) library(writexl) Se o arquivo .csv foi criado com separador de decimais sendo . e separador de colunas sendo ,, usamos as funções normalmente. Caso seja criado com separador de decimais sendo , e separador de colunas sendo ;, usaríamos a função readr::read_csv2() para importar e readr::write_csv2() para exportar nesse formato, que é mais comum no Brasil. Para exemplificar como essas funções funcionam, vamos importar novamente os dados de comunidades de anfíbios da Mata Atlântica (Atlantic Amphibians, Vancine et al. (2018)), que fizemos o download no Capítulo 4. Estamos usando a função readr::read_csv(), indicando os diretórios com a função here::here(), e a classe do arquivo é tibble. Devemos atentar para o argumento locale = readr::locale(encoding = \"latin1\", que selecionamos aqui como latin1 para corrigir um erro do autor dos dados que publicou esse data paper com erros ## Importar locais tidy_anfibios_locais &lt;- readr::read_csv( here::here(&quot;dados&quot;, &quot;tabelas&quot;, &quot;ATLANTIC_AMPHIBIANS_sites.csv&quot;), locale = readr::locale(encoding = &quot;latin1&quot;) ) #&gt; #&gt; -- Column specification --------------------------------------------------------------------------- #&gt; cols( #&gt; .default = col_character(), #&gt; reference_number = col_double(), #&gt; species_number = col_double(), #&gt; month_start = col_double(), #&gt; year_start = col_double(), #&gt; month_finish = col_double(), #&gt; year_finish = col_double(), #&gt; effort_months = col_double(), #&gt; latitude = col_double(), #&gt; longitude = col_double(), #&gt; altitude = col_double(), #&gt; temperature = col_double(), #&gt; precipitation = col_double() #&gt; ) #&gt; i Use `spec()` for the full column specifications. Caso o download não funcione ou haja problemas com a importação, disponibilizamos os dados também no pacote ecodados. ## Importar os dados pelo pacote ecodados data(tidy_anfibios_locais) head(tidy_anfibios_locais) Para se aprofundar no tema, recomendamos a leitura do Capítulo 11 Data import de Wickham and Grolemund (2017). 5.5 tibble O tibble (tbl_sf) é uma versão aprimorada do data frame (data.frame). Ele é a classe aconselhada para que as funções do tidyverse funcionem melhor sobre conjuntos de dados tabulares importados para o R. Geralmente, quando utilizamos funções tidyverse para importar dados para o R, é essa classe que os dados adquirem depois de importados. Além da importação de dados, podemos criar um tibble no R usando a função tibble::tibble(), semelhante ao uso da função data.frame(). Podemos ainda converter um data.frame para um tibble usando a função tibble::as_tibble(). Entretanto, em alguns momentos precisaremos da classe data.frame para algumas funções específicas, e podemos converter um tibble para data.frame usando a função tibble::as_data_frame(). Existem duas diferenças principais no uso do tibble e do data.frame: impressão e subconjunto. Objetos da classe tibbles possuem um método de impressão que mostra a contagem do número de linhas e colunas, e apenas as primeiras 10 linhas e todas as colunas que cabem na tela no console, além dos modos ou tipos das colunas. Dessa forma, cada coluna ou variável, pode ser do modo numbers (int ou dbl), character (chr), logical (lgl), factor (fctr), date + time (dttm) e date (date), além de outras inúmeras possibilidades. Todas as funções deste pacote são listadas na página de referência do pacote. ## Tibble - impressão tidy_anfibios_locais #&gt; # A tibble: 1,163 x 25 #&gt; id reference_number species_number record sampled_habitat active_methods passive_methods #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 amp1001 1001 19 ab fo,ll as pt #&gt; 2 amp1002 1002 16 co fo,la,ll as pt #&gt; 3 amp1003 1002 14 co fo,la,ll as pt #&gt; 4 amp1004 1002 13 co fo,la,ll as pt #&gt; 5 amp1005 1003 30 co fo,ll,br as &lt;NA&gt; #&gt; 6 amp1006 1004 42 co tp,pp,la,ll,is &lt;NA&gt; &lt;NA&gt; #&gt; 7 amp1007 1005 23 co sp as &lt;NA&gt; #&gt; 8 amp1008 1005 19 co sp,la,sw as,sb,tr &lt;NA&gt; #&gt; 9 amp1009 1005 13 ab fo &lt;NA&gt; pt #&gt; 10 amp1010 1006 1 ab fo &lt;NA&gt; pt #&gt; # ... with 1,153 more rows, and 18 more variables: complementary_methods &lt;chr&gt;, period &lt;chr&gt;, #&gt; # month_start &lt;dbl&gt;, year_start &lt;dbl&gt;, month_finish &lt;dbl&gt;, year_finish &lt;dbl&gt;, #&gt; # effort_months &lt;dbl&gt;, country &lt;chr&gt;, state &lt;chr&gt;, state_abbreviation &lt;chr&gt;, municipality &lt;chr&gt;, #&gt; # site &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, coordinate_precision &lt;chr&gt;, altitude &lt;dbl&gt;, #&gt; # temperature &lt;dbl&gt;, precipitation &lt;dbl&gt; Para o subconjunto, como vimos anteriormente, para selecionar colunas e linhas de objetos bidimensionais podemos utilizar os operadores [] ou [[]], associado com números separados por vírgulas ou o nome da coluna entre aspas, e o operador $ para extrair uma coluna pelo seu nome. Comparando um data.frame a um tibble, o último é mais rígido na seleção das colunas: eles nunca fazem correspondência parcial e gerarão um aviso se a coluna que você está tentando acessar não existir. ## Tibble - subconjunto tidy_anfibios_locais$ref #&gt; Warning: Unknown or uninitialised column: `ref`. #&gt; NULL Por fim, podemos espiar os dados utilizando a função tibble::glimpse() para ter uma noção geral de número de linhas, colunas, e conteúdo de todas as colunas. Essa é função tidyverse da função R Base str(). ## Espiar os dados tibble::glimpse(tidy_anfibios_locais[, 1:10]) #&gt; Rows: 1,163 #&gt; Columns: 10 #&gt; $ id &lt;chr&gt; &quot;amp1001&quot;, &quot;amp1002&quot;, &quot;amp1003&quot;, &quot;amp1004&quot;, &quot;amp1005&quot;, &quot;amp1006&quot;, &quot;~ #&gt; $ reference_number &lt;dbl&gt; 1001, 1002, 1002, 1002, 1003, 1004, 1005, 1005, 1005, 1006, 1006, 1~ #&gt; $ species_number &lt;dbl&gt; 19, 16, 14, 13, 30, 42, 23, 19, 13, 1, 1, 2, 4, 4, 6, 5, 8, 2, 5, 1~ #&gt; $ record &lt;chr&gt; &quot;ab&quot;, &quot;co&quot;, &quot;co&quot;, &quot;co&quot;, &quot;co&quot;, &quot;co&quot;, &quot;co&quot;, &quot;co&quot;, &quot;ab&quot;, &quot;ab&quot;, &quot;ab&quot;, &quot;~ #&gt; $ sampled_habitat &lt;chr&gt; &quot;fo,ll&quot;, &quot;fo,la,ll&quot;, &quot;fo,la,ll&quot;, &quot;fo,la,ll&quot;, &quot;fo,ll,br&quot;, &quot;tp,pp,la,~ #&gt; $ active_methods &lt;chr&gt; &quot;as&quot;, &quot;as&quot;, &quot;as&quot;, &quot;as&quot;, &quot;as&quot;, NA, &quot;as&quot;, &quot;as,sb,tr&quot;, NA, NA, NA, NA,~ #&gt; $ passive_methods &lt;chr&gt; &quot;pt&quot;, &quot;pt&quot;, &quot;pt&quot;, &quot;pt&quot;, NA, NA, NA, NA, &quot;pt&quot;, &quot;pt&quot;, &quot;pt&quot;, &quot;pt&quot;, &quot;pt~ #&gt; $ complementary_methods &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~ #&gt; $ period &lt;chr&gt; &quot;mo,da,tw,ni&quot;, &quot;mo,da,tw,ni&quot;, &quot;mo,da,tw,ni&quot;, &quot;mo,da,tw,ni&quot;, &quot;mo,da,~ #&gt; $ month_start &lt;dbl&gt; 9, 12, 12, 12, 7, NA, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ~ Para se aprofundar no tema, recomendamos a leitura do Capítulo 10 Tibbles de Wickham and Grolemund (2017). 5.6 magrittr (pipe - %&gt;%) O operador pipe %&gt;% permite o encadeamento de várias funções, eliminando a necessidade de criar objetos para armazenar resultados intermediários. Dessa forma, pipes são uma ferramenta poderosa para expressar uma sequência de múltiplas operações. O operador pipe %&gt;% vem do pacote magrittr, entretanto, todos os pacotes no tidyverse automaticamente tornam o pipe disponível. Essa função torna os códigos em R mais simples, pois realizamos múltiplas operações em uma única linha. Ele captura o resultado de uma declaração e o torna a entrada da próxima declaração, então podemos pensar como EM SEGUIDA FAÇA ao final de cada linha de código. Todas as funções deste pacote são listadas na página de referência do pacote. A principal vantagem do uso dos pipes é facilitar o debuging (achar erros) nos códigos, porque seu uso torna a linguagem R mais próxima do que falamos e pensamos, uma vez que evita o uso de funções dentro de funções (funções compostas, lembra-se do fog e gof? Evitamos eles aqui também). Digitar %&gt;% é um pouco chato, dessa forma, existe um atalho para sua inserção nos scripts: Ctrl + Shift + M. Para deixar esse tópico menos estranho a quem possa ver essa operação pela primeira vez, vamos fazer alguns exemplos. ## Base R - sem pipe sqrt(sum(1:100)) #&gt; [1] 71.06335 ## Tidyverse - com pipe 1:100 %&gt;% sum() %&gt;% sqrt() #&gt; [1] 71.06335 Essas operações ainda estão simples, vamos torná-las mais complexas com várias funções compostas. É nesses casos que a propriedade organizacional do uso do pipe emerge: podemos facilmente ver o encadeamento de operações, onde cada função é disposta numa linha. Apenas um adendo: a função set.seed() que fixa a amostragem de funções que geram valores aleatório, como é o caso da função rpois(). ## Fixar amostragem set.seed(42) ## Base R - sem pipe ve &lt;- sum(sqrt(sort(log10(rpois(100, 10))))) ve #&gt; [1] 99.91426 ## Fixar amostragem set.seed(42) ## Tidyverse - com pipe ve &lt;- rpois(100, 10) %&gt;% log10() %&gt;% sort() %&gt;% sqrt() %&gt;% sum() ve #&gt; [1] 99.91426 O uso do pipe vai se tornar especialmente útil quando seguirmos para os pacotes das próximas duas seções: tidyr e dplyr. Com esses pacotes faremos operações em linhas e colunas de nossos dados tabulares, então podemos encadear uma série de funções para manipulação, limpeza e análise de dados. Há ainda três outras variações do pipe que podem ser úteis em alguns momentos, mas que para funcionar precisam que o pacotemagrittr seja carregado: %T&gt;%: retorna o lado esquerdo em vez do lado direito %$%: explode as variáveis em um quadro de dados %&lt;&gt;%: permite atribuição usando pipes Para se aprofundar no tema, recomendamos a leitura do Capítulo 18 Pipes de Wickham and Grolemund (2017). Observação: A partir da versão do R 4.1+ (18/05/2021), o operador pipe se tornou nativo do R. Entretanto, o operador foi atualizado para |&gt;, podendo ser inserido com o mesmo atalho Ctrl + Shift + M, mas necessitando uma mudança de opção em Tools &gt; Global Options &gt; Code &gt; [x] Use native pipe operator, |&gt; (requires R 4.1+), necessitando que o RStudio esteja numa versão igual ou superior a 1.4.17+. 5.7 tidyr Os conjuntos de dados tidy (organizados) são mais fáceis de manipular, modelar e visualizar. Um conjunto de dados está no formato tidy ou não, dependendo de como linhas, colunas e células são combinadas com observações, variáveis e valores. Nos dados tidy, as variáveis estão nas colunas, observações estão nas linhas e valores estão nas células, sendo que para esse último, não deve haver mais de um valor por célula (Figura 5.2). Cada variável em uma coluna Cada observação em uma linha Cada valor como uma célula Figura 5.2: As três regras que tornam um conjunto de dados tidy. Adaptado de: Wickham and Grolemund (2017). Todas as funções deste pacote são listadas na página de referência do pacote. Para realizar diversas transformações nos dados, a fim de ajustá-los ao formato tidy existe uma série de funções, para: unir colunas, separar colunas, lidar com valores faltantes (NA), transformar a base de dados de formato longo para largo (ou vice-e-versa), além de outras funções específicas. unite(): junta dados de múltiplas colunas em uma coluna separate(): separa caracteres em múltiplas colunas separate_rows(): separa caracteres em múltiplas colunas e linhas drop_na(): retira linhas com NA do conjunto de dados replace_na(): substitui NA do conjunto de dados pivot_wider(): transforma um conjunto de dados longo (long) para largo (wide) pivot_longer(): transforma um conjunto de dados largo (wide) para longo (long) 5.7.1 palmerpenguins Para exemplificar o funcionamento dessas funções, usaremos os dados de medidas de pinguins chamados palmerpenguins, disponíveis no pacote palmerpenguins. ## Instalar o pacote install.packages(&quot;palmerpenguins&quot;) Esses dados foram coletados e disponibilizados pela Dra. Kristen Gorman e pela Palmer Station, Antarctica LTER, membro da Long Term Ecological Research Network. O pacote palmerpenguins contém dois conjuntos de dados. Um é chamado de penguins, e é uma versão simplificada dos dados brutos. O segundo conjunto de dados é penguins_raw e contém todas as variáveis e nomes originais baixados. Ambos os conjuntos de dados contêm dados para 344 pinguins, de três espécies diferentes, coletados em três ilhas no arquipélago de Palmer, na Antártica. Destacamos também a versão traduzida desses dados para o português, disponível no pacote dados. Vamos utilizar principalmente o conjunto de dados penguins_raw, que é a versão dos dados brutos. ## Carregar o pacote palmerpenguins library(palmerpenguins) ## Ajuda dos dados ?penguins ?penguins_raw 5.7.2 glimpse() Primeiramente, vamos observar os dados e utilizar a função tibble::glimpse() para ter uma noção geral. ## Visualizar os dados penguins_raw #&gt; # A tibble: 344 x 17 #&gt; studyName `Sample Number` Species Region Island Stage `Individual ID` `Clutch Complet~ #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 PAL0708 1 Adelie Pengu~ Anvers Torger~ Adult, ~ N1A1 Yes #&gt; 2 PAL0708 2 Adelie Pengu~ Anvers Torger~ Adult, ~ N1A2 Yes #&gt; 3 PAL0708 3 Adelie Pengu~ Anvers Torger~ Adult, ~ N2A1 Yes #&gt; 4 PAL0708 4 Adelie Pengu~ Anvers Torger~ Adult, ~ N2A2 Yes #&gt; 5 PAL0708 5 Adelie Pengu~ Anvers Torger~ Adult, ~ N3A1 Yes #&gt; 6 PAL0708 6 Adelie Pengu~ Anvers Torger~ Adult, ~ N3A2 Yes #&gt; 7 PAL0708 7 Adelie Pengu~ Anvers Torger~ Adult, ~ N4A1 No #&gt; 8 PAL0708 8 Adelie Pengu~ Anvers Torger~ Adult, ~ N4A2 No #&gt; 9 PAL0708 9 Adelie Pengu~ Anvers Torger~ Adult, ~ N5A1 Yes #&gt; 10 PAL0708 10 Adelie Pengu~ Anvers Torger~ Adult, ~ N5A2 Yes #&gt; # ... with 334 more rows, and 9 more variables: Date Egg &lt;date&gt;, Culmen Length (mm) &lt;dbl&gt;, #&gt; # Culmen Depth (mm) &lt;dbl&gt;, Flipper Length (mm) &lt;dbl&gt;, Body Mass (g) &lt;dbl&gt;, Sex &lt;chr&gt;, #&gt; # Delta 15 N (o/oo) &lt;dbl&gt;, Delta 13 C (o/oo) &lt;dbl&gt;, Comments &lt;chr&gt; ## Espiar os dados dplyr::glimpse(penguins_raw) #&gt; Rows: 344 #&gt; Columns: 17 #&gt; $ studyName &lt;chr&gt; &quot;PAL0708&quot;, &quot;PAL0708&quot;, &quot;PAL0708&quot;, &quot;PAL0708&quot;, &quot;PAL0708&quot;, &quot;PAL0708&quot;, &quot;~ #&gt; $ `Sample Number` &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ~ #&gt; $ Species &lt;chr&gt; &quot;Adelie Penguin (Pygoscelis adeliae)&quot;, &quot;Adelie Penguin (Pygoscelis ~ #&gt; $ Region &lt;chr&gt; &quot;Anvers&quot;, &quot;Anvers&quot;, &quot;Anvers&quot;, &quot;Anvers&quot;, &quot;Anvers&quot;, &quot;Anvers&quot;, &quot;Anvers~ #&gt; $ Island &lt;chr&gt; &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;T~ #&gt; $ Stage &lt;chr&gt; &quot;Adult, 1 Egg Stage&quot;, &quot;Adult, 1 Egg Stage&quot;, &quot;Adult, 1 Egg Stage&quot;, &quot;~ #&gt; $ `Individual ID` &lt;chr&gt; &quot;N1A1&quot;, &quot;N1A2&quot;, &quot;N2A1&quot;, &quot;N2A2&quot;, &quot;N3A1&quot;, &quot;N3A2&quot;, &quot;N4A1&quot;, &quot;N4A2&quot;, &quot;N5~ #&gt; $ `Clutch Completion` &lt;chr&gt; &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;,~ #&gt; $ `Date Egg` &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 2007-11-16, 2007-1~ #&gt; $ `Culmen Length (mm)` &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.~ #&gt; $ `Culmen Depth (mm)` &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.~ #&gt; $ `Flipper Length (mm)` &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191~ #&gt; $ `Body Mass (g)` &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 370~ #&gt; $ Sex &lt;chr&gt; &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;FEMALE&quot;, NA, &quot;FEMALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;,~ #&gt; $ `Delta 15 N (o/oo)` &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718, 9.46060, NA, 9~ #&gt; $ `Delta 13 C (o/oo)` &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, -25.21799, -24.~ #&gt; $ Comments &lt;chr&gt; &quot;Not enough blood for isotopes.&quot;, NA, NA, &quot;Adult not sampled.&quot;, NA,~ 5.7.3 unite() Primeiramente, vamos exemplificar como juntar e separar colunas. Vamos utilizar a função tidyr::unite() para unir as colunas. Há diversos parâmetros para alterar como essa função funciona, entretanto, é importante destacar três deles: col nome da coluna que vai receber as colunas unidas, sep indicando o caracter separador das colunas unidas, e remove para uma resposta lógica se as colunas unidas são removidas ou não. Vamos unir as colunas Region e Island na nova coluna region_island. ## Unir colunas penguins_raw_unir &lt;- tidyr::unite(data = penguins_raw, col = &quot;region_island&quot;, Region:Island, sep = &quot;, &quot;, remove = FALSE) head(penguins_raw_unir[, c(&quot;Region&quot;, &quot;Island&quot;, &quot;region_island&quot;)]) #&gt; # A tibble: 6 x 3 #&gt; Region Island region_island #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Anvers Torgersen Anvers, Torgersen #&gt; 2 Anvers Torgersen Anvers, Torgersen #&gt; 3 Anvers Torgersen Anvers, Torgersen #&gt; 4 Anvers Torgersen Anvers, Torgersen #&gt; 5 Anvers Torgersen Anvers, Torgersen #&gt; 6 Anvers Torgersen Anvers, Torgersen 5.7.4 separate() De forma contrária, podemos utilizar as funções tidyr::separate() e tidyr::separate_rows() para separar elementos de uma coluna em mais colunas. Respectivamente, a primeira função separa uma coluna em novas colunas conforme a separação, e a segunda função separa uma coluna, distribuindo os elementos também nas linhas. Novamente, há diversos parâmetros para mudar o comportamento dessas funções, mas destacaremos aqui quatro deles: col coluna a ser separada, into os nomes das novas colunas, sep indicando o caractere separador das colunas, e remove para uma resposta lógica se as colunas separadas são removidas ou não. Vamos separar a coluna Stage nas colunas stage e egg_stage. ## Separar colunas penguins_raw_separar &lt;- tidyr::separate(data = penguins_raw, col = Stage, into = c(&quot;stage&quot;, &quot;egg_stage&quot;), sep = &quot;, &quot;, remove = FALSE) head(penguins_raw_separar[, c(&quot;Stage&quot;, &quot;stage&quot;, &quot;egg_stage&quot;)]) #&gt; # A tibble: 6 x 3 #&gt; Stage stage egg_stage #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Adult, 1 Egg Stage Adult 1 Egg Stage #&gt; 2 Adult, 1 Egg Stage Adult 1 Egg Stage #&gt; 3 Adult, 1 Egg Stage Adult 1 Egg Stage #&gt; 4 Adult, 1 Egg Stage Adult 1 Egg Stage #&gt; 5 Adult, 1 Egg Stage Adult 1 Egg Stage #&gt; 6 Adult, 1 Egg Stage Adult 1 Egg Stage ## Separar colunas em novas linhas penguins_raw_separar_linhas &lt;- tidyr::separate_rows(data = penguins_raw, Stage, sep = &quot;, &quot;) head(penguins_raw_separar_linhas[, c(&quot;studyName&quot;, &quot;Sample Number&quot;, &quot;Species&quot;, &quot;Region&quot;, &quot;Island&quot;, &quot;Stage&quot;)]) #&gt; # A tibble: 6 x 6 #&gt; studyName `Sample Number` Species Region Island Stage #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 PAL0708 1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen Adult #&gt; 2 PAL0708 1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen 1 Egg Stage #&gt; 3 PAL0708 2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen Adult #&gt; 4 PAL0708 2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen 1 Egg Stage #&gt; 5 PAL0708 3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen Adult #&gt; 6 PAL0708 3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen 1 Egg Stage 5.7.5 drop_na() e replace_na() Valores faltantes (NA) é um tipo especial de elemento que discutimos no Capítulo 4, e são relativamente comuns em conjuntos de dados. Em Base R, vimos algumas formas de lidar com esse tipo de elemento. No formato tidyverse, existem várias formas de lidar com eles, mas aqui focaremos nas funções tidyr::drop_na() e tidyr::replace_na(), para retirar linhas e substitui-los, respectivamente. ## Remover todas as linhas com NAs penguins_raw_todas_na &lt;- tidyr::drop_na(data = penguins_raw) head(penguins_raw_todas_na) #&gt; [90m# A tibble: 6 × 17[39m #&gt; [1mstudyName[22m [1m`Sample Number`[22m [1mSpecies[22m [1mRegion[22m [1mIsland[22m [1mStage[22m [1m`Individual ID`[22m [1m`Clutch Complet[22m [1m`Date Egg`[22m #&gt; [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;date&gt;[39m[23m #&gt; [90m1[39m PAL0708 7 Adelie Peng Anvers Torger Adult, N4A1 No 2007-11-15 #&gt; [90m2[39m PAL0708 8 Adelie Peng Anvers Torger Adult, N4A2 No 2007-11-15 #&gt; [90m3[39m PAL0708 29 Adelie Peng Anvers Biscoe Adult, N18A1 No 2007-11-10 #&gt; [90m4[39m PAL0708 30 Adelie Peng Anvers Biscoe Adult, N18A2 No 2007-11-10 #&gt; [90m5[39m PAL0708 39 Adelie Peng Anvers Dream Adult, N25A1 No 2007-11-13 #&gt; [90m6[39m PAL0809 69 Adelie Peng Anvers Torger Adult, N32A1 No 2008-11-11 #&gt; [90m#  with 8 more variables: [1mCulmen Length (mm)[22m &lt;dbl&gt;, [1mCulmen Depth (mm)[22m &lt;dbl&gt;, [1mFlipper Length (mm)[22m &lt;dbl&gt;,[39m #&gt; [90m# [1mBody Mass (g)[22m &lt;dbl&gt;, [1mSex[22m &lt;chr&gt;, [1mDelta 15 N (o/oo)[22m &lt;dbl&gt;, [1mDelta 13 C (o/oo)[22m &lt;dbl&gt;, [1mComments[22m &lt;chr&gt;[39m ## Remover linhas de colunas específicas com NAs penguins_raw_colunas_na &lt;- tidyr::drop_na(data = penguins_raw, any_of(&quot;Comments&quot;)) head(penguins_raw_colunas_na[, &quot;Comments&quot;]) #&gt; [90m# A tibble: 6 × 1[39m #&gt; [1mComments[22m #&gt; [3m[90m&lt;chr&gt;[39m[23m #&gt; [90m1[39m Not enough blood for isotopes. #&gt; [90m2[39m Adult not sampled. #&gt; [90m3[39m Nest never observed with full clutch. #&gt; [90m4[39m Nest never observed with full clutch. #&gt; [90m5[39m No blood sample obtained. #&gt; [90m6[39m No blood sample obtained for sexing. ## Substituir NAs por outro valor penguins_raw_subs_na &lt;- tidyr::replace_na(data = penguins_raw, list(Comments = &quot;Unknown&quot;)) head(penguins_raw_subs_na[, &quot;Comments&quot;]) #&gt; [90m# A tibble: 6 × 1[39m #&gt; [1mComments[22m #&gt; [3m[90m&lt;chr&gt;[39m[23m #&gt; [90m1[39m Not enough blood for isotopes. #&gt; [90m2[39m Unknown #&gt; [90m3[39m Unknown #&gt; [90m4[39m Adult not sampled. #&gt; [90m5[39m Unknown #&gt; [90m6[39m Unknown 5.7.6 pivot_longer() e pivot_wider() Por fim, trataremos da pivotagem ou remodelagem de dados. Veremos como mudar o formato do nosso conjunto de dados de longo (long) para largo (wide) e vice-versa. Essa é uma operação semelhante à Tabela Dinâmica das planilhas eletrônicas. Consiste em usar uma coluna para distribuir seus valores em outras colunas, de modo que os valores dos elementos são preenchidos corretamente, reduzindo assim o número de linhas. Essa operação é bastante comum em Ecologia de Comunidades, quando queremos transformar uma lista de espécies em uma matriz de comunidades, com várias espécies nas colunas. Para realizar essa operação, usarmos a função tidyr::pivot_wider(). Dos diversos parâmetros que podem compor essa função, dois deles são fundamentais: names_from que indica a coluna de onde os nomes serão usados e values_from a coluna com os valores. ## Selecionar colunas penguins_raw_sel_col &lt;- penguins_raw[, c(2, 3, 13)] head(penguins_raw_sel_col) #&gt; # A tibble: 6 x 3 #&gt; `Sample Number` Species `Body Mass (g)` #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 Adelie Penguin (Pygoscelis adeliae) 3750 #&gt; 2 2 Adelie Penguin (Pygoscelis adeliae) 3800 #&gt; 3 3 Adelie Penguin (Pygoscelis adeliae) 3250 #&gt; 4 4 Adelie Penguin (Pygoscelis adeliae) NA #&gt; 5 5 Adelie Penguin (Pygoscelis adeliae) 3450 #&gt; 6 6 Adelie Penguin (Pygoscelis adeliae) 3650 ## Pivotar para largo penguins_raw_pivot_wider &lt;- tidyr::pivot_wider(data = penguins_raw_sel_col, names_from = Species, values_from = `Body Mass (g)`) head(penguins_raw_pivot_wider) #&gt; # A tibble: 6 x 4 #&gt; `Sample Number` `Adelie Penguin (Pygoscelis adeliae)` `Gentoo penguin (Pyg~ `Chinstrap penguin (~ #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 3750 4500 3500 #&gt; 2 2 3800 5700 3900 #&gt; 3 3 3250 4450 3650 #&gt; 4 4 NA 5700 3525 #&gt; 5 5 3450 5400 3725 #&gt; 6 6 3650 4550 3950 De modo oposto, podemos partir de um conjunto de dados largo (wide), ou seja, com várias colunas, e queremos que essas colunas preencham uma única coluna, e que os valores antes espalhados nessas várias colunas sejam adicionados um embaixo do outro, numa única coluna. Para essa operação, podemos utilizar a função tidyr::pivot_longer(). Novamente, dos diversos parâmetros que podem compor essa função, três deles são fundamentais: cols indicando as colunas que serão usadas para serem pivotadas, names_to que indica a coluna de onde os nomes serão usados e values_to a coluna com os valores. ## Selecionar colunas penguins_raw_sel_col &lt;- penguins_raw[, c(2, 3, 10:13)] head(penguins_raw_sel_col) #&gt; # A tibble: 6 x 6 #&gt; `Sample Number` Species `Culmen Length (~ `Culmen Depth (m~ `Flipper Length~ `Body Mass (g)` #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Adelie Peng~ 39.1 18.7 181 3750 #&gt; 2 2 Adelie Peng~ 39.5 17.4 186 3800 #&gt; 3 3 Adelie Peng~ 40.3 18 195 3250 #&gt; 4 4 Adelie Peng~ NA NA NA NA #&gt; 5 5 Adelie Peng~ 36.7 19.3 193 3450 #&gt; 6 6 Adelie Peng~ 39.3 20.6 190 3650 ## Pivotar para largo penguins_raw_pivot_longer &lt;- tidyr::pivot_longer(data = penguins_raw_sel_col, cols = `Culmen Length (mm)`:`Body Mass (g)`, names_to = &quot;medidas&quot;, values_to = &quot;valores&quot;) head(penguins_raw_pivot_longer) #&gt; # A tibble: 6 x 4 #&gt; `Sample Number` Species medidas valores #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 Adelie Penguin (Pygoscelis adeliae) Culmen Length (mm) 39.1 #&gt; 2 1 Adelie Penguin (Pygoscelis adeliae) Culmen Depth (mm) 18.7 #&gt; 3 1 Adelie Penguin (Pygoscelis adeliae) Flipper Length (mm) 181 #&gt; 4 1 Adelie Penguin (Pygoscelis adeliae) Body Mass (g) 3750 #&gt; 5 2 Adelie Penguin (Pygoscelis adeliae) Culmen Length (mm) 39.5 #&gt; 6 2 Adelie Penguin (Pygoscelis adeliae) Culmen Depth (mm) 17.4 Para se aprofundar no tema, recomendamos a leitura do Capítulo 12 Tidy data de Wickham and Grolemund (2017). 5.8 dplyr O dplyr é um pacote que facilita a manipulação de dados, com uma gramática simples e flexível (por exemplo, como filtragem, reordenamento, seleção, entre outras). Ele foi construído com o intuito de obter uma forma mais rápida e expressiva de manipular dados tabulares. O tibble é a versão de data frame mais conveniente para se usar com pacote dplyr. Todas as funções deste pacote são listadas na página de referência do pacote. 5.8.1 Gramática Sua gramática simples contém funções verbais para manipulação de dados, baseada em: Verbos: mutate(), select(), filter(), arrange(), summarise(), slice(), rename(), etc. Replicação: across(), if_any(), if_all(), where(), starts_with(), ends_with(), contains(), etc. Agrupamento: group_by() e ungroup() Junções: inner_join(), full_join(), left_join(), right_join(), etc. Combinações: bind_rows() e bind_cols() Resumos, contagem e seleção: n(), n_distinct(), first(), last(), nth(), etc. Existe uma série de funções para realizar a manipulação dos dados, com diversas finalidades: manipulação de uma tabela, manipulação de duas tabelas, replicação, agrupamento, funções de vetores, além de muitas outras funções específicas. relocate(): muda a ordem das colunas rename(): muda o nome das colunas select(): seleciona colunas pelo nome ou posição pull(): seleciona uma coluna como vetor mutate(): adiciona novas colunas ou resultados em colunas existentes arrange(): reordena as linhas com base nos valores de colunas filter(): seleciona linhas com base em valores de colunas slice(): seleciona linhas de diferente formas distinct(): remove linhas com valores repetidos com base nos valores de colunas count(): conta observações para um grupo group_by(): agrupa linhas pelos valores das colunas summarise(): resume os dados através de funções considerando valores das colunas *_join(): funções que juntam dados de duas tabelas através de uma coluna chave 5.8.2 Sintaxe As funções do dplyr podem seguir uma mesma sintaxe: o tibble será sempre o primeiro argumento dessas funções, seguido de um pipe e pelo nome da função que irá fazer a manipulação nesses dados. Isso permite o encadeamento de várias operações consecutivas mantendo a estrutura do dado original e acrescentando mudanças num encadeamento lógico. Sendo assim, as funções verbais não precisam modificar necessariamente o tibble original, sendo que as operações de manipulações podem e devem ser atribuídas a um novo objeto. ## Sintaxe tb_dplyr &lt;- tb %&gt;% funcao_verbal1(argumento1, argumento2, ...) %&gt;% funcao_verbal2(argumento1, argumento2, ...) %&gt;% funcao_verbal3(argumento1, argumento2, ...) Além de data.frames e tibbles, a manipulação pelo formato dplyr torna o trabalho com outros formatos de classes e dados acessíveis e eficientes como data.table, SQL e Apache Spark, para os quais existem pacotes específicos. dtplyr: manipular conjuntos de dados data.table dbplyr: manipular conjuntos de dados SQL sparklyr: manipular conjuntos de dados no Apache Spark 5.8.3 palmerpenguins Para nossos exemplos, vamos utilizar novamente os dados de pinguins palmerpenguins. Esses dados estão disponíveis no pacote palmerpenguins. Vamos utilizar principalmente o conjunto de dados penguins, que é a versão simplificada dos dados brutos penguins_raw. ## Carrega o pacote palmerpenguins library(palmerpenguins) 5.8.4 relocate() Primeiramente, vamos reordenar as colunas com a função dplyr::relocate(), onde simplesmente listamos as colunas que queremos mudar de posição e para onde elas devem ir. Para esse último passo há dois argumentos: .before que indica qual a coluna que as colunas realocadas devem se mover antes, e o argumento .after indicando onde devem se mover depois. Ambos podem ser informados com os nomes ou posições dessas colunas com números. ## Reordenar colunas - nome penguins_relocate_col &lt;- penguins %&gt;% dplyr::relocate(sex, year, .after = island) head(penguins_relocate_col) #&gt; # A tibble: 6 x 8 #&gt; species island sex year bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen male 2007 39.1 18.7 181 3750 #&gt; 2 Adelie Torgersen female 2007 39.5 17.4 186 3800 #&gt; 3 Adelie Torgersen female 2007 40.3 18 195 3250 #&gt; 4 Adelie Torgersen &lt;NA&gt; 2007 NA NA NA NA #&gt; 5 Adelie Torgersen female 2007 36.7 19.3 193 3450 #&gt; 6 Adelie Torgersen male 2007 39.3 20.6 190 3650 ## Reordenar colunas - posição penguins_relocate_ncol &lt;- penguins %&gt;% dplyr::relocate(sex, year, .after = 2) head(penguins_relocate_ncol) #&gt; # A tibble: 6 x 8 #&gt; species island sex year bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen male 2007 39.1 18.7 181 3750 #&gt; 2 Adelie Torgersen female 2007 39.5 17.4 186 3800 #&gt; 3 Adelie Torgersen female 2007 40.3 18 195 3250 #&gt; 4 Adelie Torgersen &lt;NA&gt; 2007 NA NA NA NA #&gt; 5 Adelie Torgersen female 2007 36.7 19.3 193 3450 #&gt; 6 Adelie Torgersen male 2007 39.3 20.6 190 3650 5.8.5 rename() Podemos ainda renomear colunas facilmente com a função dplyr::rename(), onde primeiramente informamos o nome que queremos que a coluna tenha, seguido do operador = e a coluna do nosso dado (nova_coluna = antiga_coluna). Também podemos utilizar a função dplyr::rename_with(), que faz a mudança do nome em múltiplas colunas, que pode depender ou não de resultados booleanos. ## Renomear as colunas penguins_rename &lt;- penguins %&gt;% dplyr::rename(bill_length = bill_length_mm, bill_depth = bill_depth_mm, flipper_length = flipper_length_mm, body_mass = body_mass_g) head(penguins_rename) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length bill_depth flipper_length body_mass sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## mudar o nome de todas as colunas penguins_rename_with &lt;- penguins %&gt;% dplyr::rename_with(toupper) head(penguins_rename_with) #&gt; # A tibble: 6 x 8 #&gt; SPECIES ISLAND BILL_LENGTH_MM BILL_DEPTH_MM FLIPPER_LENGTH_MM BODY_MASS_G SEX YEAR #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 5.8.6 select() Outra operação bastante usual dentro da manipulação de dados tabulares é a seleção de colunas. Podemos fazer essa operação com a função dplyr::select(), que seleciona colunas pelo nome ou pela sua posição. Aqui há uma série de possibilidades de seleção de colunas, desde utilizar operadores como : para selecionar intervalos de colunas, ! para tomar o complemento (todas menos as listadas), além de funções como dplyr::starts_with(), dplyr::ends_with(), dplyr::contains() para procurar colunas com um padrão de texto. ## Selecionar colunas por posição penguins_select_position &lt;- penguins %&gt;% dplyr::select(3:6) head(penguins_select_position) #&gt; # A tibble: 6 x 4 #&gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 39.1 18.7 181 3750 #&gt; 2 39.5 17.4 186 3800 #&gt; 3 40.3 18 195 3250 #&gt; 4 NA NA NA NA #&gt; 5 36.7 19.3 193 3450 #&gt; 6 39.3 20.6 190 3650 ## Selecionar colunas por nomes penguins_select_names &lt;- penguins %&gt;% dplyr::select(bill_length_mm:body_mass_g) head(penguins_select_names) #&gt; # A tibble: 6 x 4 #&gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 39.1 18.7 181 3750 #&gt; 2 39.5 17.4 186 3800 #&gt; 3 40.3 18 195 3250 #&gt; 4 NA NA NA NA #&gt; 5 36.7 19.3 193 3450 #&gt; 6 39.3 20.6 190 3650 ## Selecionar colunas por padrão penguins_select_contains &lt;- penguins %&gt;% dplyr::select(contains(&quot;_mm&quot;)) head(penguins_select_contains) #&gt; # A tibble: 6 x 3 #&gt; bill_length_mm bill_depth_mm flipper_length_mm #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 39.1 18.7 181 #&gt; 2 39.5 17.4 186 #&gt; 3 40.3 18 195 #&gt; 4 NA NA NA #&gt; 5 36.7 19.3 193 #&gt; 6 39.3 20.6 190 5.8.7 pull() Quando usamos a função dplyr::select(), mesmo que para uma coluna, o retorno é sempre um tibble. Caso precisemos que essa coluna se torne um vetor dentro do encadeamento dos pipes, usamos a função dplyr::pull() que extrai uma única coluna como vetor. ## Coluna como vetor penguins_select_pull &lt;- penguins %&gt;% dplyr::pull(bill_length_mm) head(penguins_select_pull, 15) #&gt; [1] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42.0 37.8 37.8 41.1 38.6 34.6 5.8.8 mutate() Uma das operações mais úteis dentre as operações para colunas é adicionar ou atualizar os valores de colunas. Para essa operação, usaremos a função dplyr::mutate(). Podemos ainda usar os argumentos .before e .after para indicar onde a nova coluna deve ficar, além do parâmetro .keep com diversas possibilidades de manter colunas depois de usar a função dplyr::mutate(). Por fim, é fundamental destacar o uso das funções de replicação: dplyr::across(), dplyr::if_any() e dplyr::if_all(), para os quais a função fará alterações em múltiplas colunas de uma vez, dependendo de resultados booleanos. ## Adicionar colunas penguins_mutate &lt;- penguins %&gt;% dplyr::mutate(body_mass_kg = body_mass_g/1e3, .before = sex) head(penguins_mutate) #&gt; # A tibble: 6 x 9 #&gt; species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g body_mass_kg sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torge~ 39.1 18.7 181 3750 3.75 male 2007 #&gt; 2 Adelie Torge~ 39.5 17.4 186 3800 3.8 fema~ 2007 #&gt; 3 Adelie Torge~ 40.3 18 195 3250 3.25 fema~ 2007 #&gt; 4 Adelie Torge~ NA NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torge~ 36.7 19.3 193 3450 3.45 fema~ 2007 #&gt; 6 Adelie Torge~ 39.3 20.6 190 3650 3.65 male 2007 ## Modificar várias colunas penguins_mutate_across &lt;- penguins %&gt;% dplyr::mutate(across(where(is.factor), as.character)) head(penguins_mutate_across) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 5.8.9 arrange() Além de operações em colunas, podemos fazer operações em linhas. Vamos começar com a reordenação das linhas com base nos valores das colunas. Para essa operação, usamos a função dplyr::arrange(). Podemos reordenar por uma ou mais colunas de forma crescente ou de forma decrescente usando a função desc() ou o operador -. Da mesma forma que na função dplyr::mutate(), podemos usar as funções de replicação para ordenar as linhas para várias colunas de uma vez, dependendo de resultados booleanos. ## Reordenar linhas - crescente penguins_arrange &lt;- penguins %&gt;% dplyr::arrange(body_mass_g) head(penguins_arrange) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Chinstrap Dream 46.9 16.6 192 2700 female 2008 #&gt; 2 Adelie Biscoe 36.5 16.6 181 2850 female 2008 #&gt; 3 Adelie Biscoe 36.4 17.1 184 2850 female 2008 #&gt; 4 Adelie Biscoe 34.5 18.1 187 2900 female 2008 #&gt; 5 Adelie Dream 33.1 16.1 178 2900 female 2008 #&gt; 6 Adelie Torgersen 38.6 17 188 2900 female 2009 ## Reordenar linhas - decrescente penguins_arrange_desc &lt;- penguins %&gt;% dplyr::arrange(desc(body_mass_g)) head(penguins_arrange_desc) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Gentoo Biscoe 49.2 15.2 221 6300 male 2007 #&gt; 2 Gentoo Biscoe 59.6 17 230 6050 male 2007 #&gt; 3 Gentoo Biscoe 51.1 16.3 220 6000 male 2008 #&gt; 4 Gentoo Biscoe 48.8 16.2 222 6000 male 2009 #&gt; 5 Gentoo Biscoe 45.2 16.4 223 5950 male 2008 #&gt; 6 Gentoo Biscoe 49.8 15.9 229 5950 male 2009 ## Reordenar linhas - decrescente penguins_arrange_desc_m &lt;- penguins %&gt;% dplyr::arrange(-body_mass_g) head(penguins_arrange_desc_m) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Gentoo Biscoe 49.2 15.2 221 6300 male 2007 #&gt; 2 Gentoo Biscoe 59.6 17 230 6050 male 2007 #&gt; 3 Gentoo Biscoe 51.1 16.3 220 6000 male 2008 #&gt; 4 Gentoo Biscoe 48.8 16.2 222 6000 male 2009 #&gt; 5 Gentoo Biscoe 45.2 16.4 223 5950 male 2008 #&gt; 6 Gentoo Biscoe 49.8 15.9 229 5950 male 2009 ## Reordenar linhas - multiplas colunas penguins_arrange_across &lt;- penguins %&gt;% dplyr::arrange(across(where(is.numeric))) head(penguins_arrange_across) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Dream 32.1 15.5 188 3050 female 2009 #&gt; 2 Adelie Dream 33.1 16.1 178 2900 female 2008 #&gt; 3 Adelie Torgersen 33.5 19 190 3600 female 2008 #&gt; 4 Adelie Dream 34 17.1 185 3400 female 2008 #&gt; 5 Adelie Torgersen 34.1 18.1 193 3475 &lt;NA&gt; 2007 #&gt; 6 Adelie Torgersen 34.4 18.4 184 3325 female 2007 5.8.10 filter() Uma das principais e mais usuais operações que podemos realizar em linhas é a seleção de linhas através do filtro por valores de uma ou mais colunas, utilizando a função dplyr::filter(). Para realizar os filtros utilizaremos grande parte dos operadores relacionais e lógicos que listamos na Tabela 4.1, especialmente os lógicos para combinações de filtros em mais de uma coluna. Além desses operadores, podemos utilizar a função is.na() para filtros em elementos faltantes, e as funções dplyr::between() e dplyr::near() para filtros entre valores, e para valores próximos com certa tolerância, respectivamente. Por fim, podemos usar as funções de replicação para filtro das linhas para mais de uma coluna, dependendo de resultados booleanos. ## Filtrar linhas penguins_filter &lt;- penguins %&gt;% dplyr::filter(species == &quot;Adelie&quot;) head(penguins_filter) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## Filtrar linhas penguins_filter_two &lt;- penguins %&gt;% dplyr::filter(species == &quot;Adelie&quot; &amp; sex == &quot;female&quot;) head(penguins_filter_two) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 2 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 3 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 4 Adelie Torgersen 38.9 17.8 181 3625 female 2007 #&gt; 5 Adelie Torgersen 41.1 17.6 182 3200 female 2007 #&gt; 6 Adelie Torgersen 36.6 17.8 185 3700 female 2007 ## Filtrar linhas penguins_filter_in &lt;- penguins %&gt;% dplyr::filter(species %in% c(&quot;Adelie&quot;, &quot;Gentoo&quot;), sex == &quot;female&quot;) head(penguins_filter_in) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 2 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 3 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 4 Adelie Torgersen 38.9 17.8 181 3625 female 2007 #&gt; 5 Adelie Torgersen 41.1 17.6 182 3200 female 2007 #&gt; 6 Adelie Torgersen 36.6 17.8 185 3700 female 2007 ## Filtrar linhas - NA penguins_filter_na &lt;- penguins %&gt;% dplyr::filter(!is.na(sex) == TRUE) head(penguins_filter_na) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 5 Adelie Torgersen 39.3 20.6 190 3650 male 2007 #&gt; 6 Adelie Torgersen 38.9 17.8 181 3625 female 2007 ## Filtrar linhas - intervalos penguins_filter_between &lt;- penguins %&gt;% dplyr::filter(between(body_mass_g, 3000, 4000)) head(penguins_filter_between) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 5 Adelie Torgersen 39.3 20.6 190 3650 male 2007 #&gt; 6 Adelie Torgersen 38.9 17.8 181 3625 female 2007 ## Filtrar linhas por várias colunas penguins_filter_if &lt;- penguins %&gt;% dplyr::filter(if_all(where(is.integer), ~ . &gt; 200)) head(penguins_filter_if) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Dream 35.7 18 202 3550 female 2008 #&gt; 2 Adelie Dream 41.1 18.1 205 4300 male 2008 #&gt; 3 Adelie Dream 40.8 18.9 208 4300 male 2008 #&gt; 4 Adelie Biscoe 41 20 203 4725 male 2009 #&gt; 5 Adelie Torgersen 41.4 18.5 202 3875 male 2009 #&gt; 6 Adelie Torgersen 44.1 18 210 4000 male 2009 5.8.11 slice() Além da seleção de linhas por filtros, podemos fazer a seleção das linhas por intervalos, indicando quais linhas desejamos, usando a função dplyr::slice(), e informando o argumento n para o número da linha ou intervalo das linhas. Essa função possui variações no sufixo muito interessantes: dplyr::slice_head() e dplyr::slice_tail() seleciona as primeiras e últimas linhas, dplyr::slice_min() e dplyr::slice_max() seleciona linhas com os maiores e menores valores de uma coluna, e dplyr::slice_sample() seleciona linhas aleatoriamente. ## Seleciona linhas penguins_slice &lt;- penguins %&gt;% dplyr::slice(n = c(1, 3, 300:n())) head(penguins_slice) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 3 Chinstrap Dream 50.6 19.4 193 3800 male 2007 #&gt; 4 Chinstrap Dream 46.7 17.9 195 3300 female 2007 #&gt; 5 Chinstrap Dream 52 19 197 4150 male 2007 #&gt; 6 Chinstrap Dream 50.5 18.4 200 3400 female 2008 ## Seleciona linhas - head penguins_slice_head &lt;- penguins %&gt;% dplyr::slice_head(n = 5) head(penguins_slice_head) #&gt; # A tibble: 5 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 ## Seleciona linhas - max penguins_slice_max &lt;- penguins %&gt;% dplyr::slice_max(body_mass_g, n = 5) head(penguins_slice_max) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Gentoo Biscoe 49.2 15.2 221 6300 male 2007 #&gt; 2 Gentoo Biscoe 59.6 17 230 6050 male 2007 #&gt; 3 Gentoo Biscoe 51.1 16.3 220 6000 male 2008 #&gt; 4 Gentoo Biscoe 48.8 16.2 222 6000 male 2009 #&gt; 5 Gentoo Biscoe 45.2 16.4 223 5950 male 2008 #&gt; 6 Gentoo Biscoe 49.8 15.9 229 5950 male 2009 ## Seleciona linhas - sample penguins_slice_sample &lt;- penguins %&gt;% dplyr::slice_sample(n = 30) head(penguins_slice_sample) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Biscoe 41.3 21.1 195 4400 male 2008 #&gt; 2 Gentoo Biscoe 44.5 15.7 217 4875 &lt;NA&gt; 2009 #&gt; 3 Adelie Torgersen 41.4 18.5 202 3875 male 2009 #&gt; 4 Adelie Biscoe 37.6 17 185 3600 female 2008 #&gt; 5 Adelie Dream 36 17.9 190 3450 female 2007 #&gt; 6 Adelie Biscoe 35.7 16.9 185 3150 female 2008 5.8.12 distinct() A última operação que apresentaremos para linhas é a retirada de linhas com valores repetidos com base nos valores de colunas, utilizando a função dplyr::distinct(). Essa função por padrão retorna apenas a coluna utilizada para retirar as linhas com valores repetidos, sendo necessário acrescentar o argumento .keep_all = TRUE para retornar todas as colunas. Por fim, podemos usar as funções de replicação para retirar linhas com valores repetidos para mais de uma coluna, dependendo de resultados booleanos. ## Retirar linhas com valores repetidos penguins_distinct &lt;- penguins %&gt;% dplyr::distinct(body_mass_g) head(penguins_distinct) #&gt; # A tibble: 6 x 1 #&gt; body_mass_g #&gt; &lt;int&gt; #&gt; 1 3750 #&gt; 2 3800 #&gt; 3 3250 #&gt; 4 NA #&gt; 5 3450 #&gt; 6 3650 ## Retirar linhas com valores repetidos - manter as outras colunas penguins_distinct_keep_all &lt;- penguins %&gt;% dplyr::distinct(body_mass_g, .keep_all = TRUE) head(penguins_distinct_keep_all) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## Retirar linhas com valores repetidos para várias colunas penguins_distinct_keep_all_across &lt;- penguins %&gt;% dplyr::distinct(across(where(is.integer)), .keep_all = TRUE) head(penguins_distinct_keep_all_across) #&gt; # A tibble: 6 x 8 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 5.8.13 count() Agora entraremos no assunto de resumo das observações. Podemos fazer contagens resumos dos nossos dados, utilizando para isso a função dplyr::count(). Essa função contará valores de uma ou mais colunas, geralmente para variáveis categóricas, semelhante à função Base R table(), mas num contexto tidyverse. ## Contagens de valores para uma coluna penguins_count &lt;- penguins %&gt;% dplyr::count(species) penguins_count #&gt; # A tibble: 3 x 2 #&gt; species n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie 152 #&gt; 2 Chinstrap 68 #&gt; 3 Gentoo 124 ## Contagens de valores para mais de uma coluna penguins_count_two &lt;- penguins %&gt;% dplyr::count(species, island) penguins_count_two #&gt; # A tibble: 5 x 3 #&gt; species island n #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Biscoe 44 #&gt; 2 Adelie Dream 56 #&gt; 3 Adelie Torgersen 52 #&gt; 4 Chinstrap Dream 68 #&gt; 5 Gentoo Biscoe 124 5.8.14 group_by() Uma grande parte das operações feitas nos dados são realizadas em grupos definidos por valores de colunas ou variáveis categóricas. A função dplyr::group_by() transforma um tibble em um tibble agrupado, onde as operações são realizadas por grupo. Essa função é utilizada geralmente junto com a função dplyr::summarise(), que veremos logo em seguida. O agrupamento não altera a aparência dos dados (além de informar como estão agrupados). A função dplyr::ungroup() remove o agrupamento. Podemos ainda usar funções de replicação para fazer os agrupamentos para mais de uma coluna, dependendo de resultados booleanos. ## Agrupamento penguins_group_by &lt;- penguins %&gt;% dplyr::group_by(species) head(penguins_group_by) #&gt; # A tibble: 6 x 8 #&gt; # Groups: species [1] #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## Agrupamento de várias colunas penguins_group_by_across &lt;- penguins %&gt;% dplyr::group_by(across(where(is.factor))) head(penguins_group_by_across) #&gt; # A tibble: 6 x 8 #&gt; # Groups: species, island, sex [3] #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 5.8.15 summarise() Como dissemos, muitas vezes queremos resumir nossos dados, principalmente para ter uma noção geral das variáveis (colunas) ou mesmo começar a análise exploratória resumindo variáveis contínuas por grupos de variáveis categóricas. Dessa forma, ao utilizar a função dplyr::summarise() teremos um novo tibble com os dados resumidos, que é a agregação ou resumo dos dados através de funções. Da mesma forma que outras funções, podemos usar funções de replicação para resumir valores para mais de uma coluna, dependendo de resultados booleanos. ## Resumo penguins_summarise &lt;- penguins %&gt;% dplyr::group_by(species) %&gt;% dplyr::summarize(body_mass_g_mean = mean(body_mass_g, na.rm = TRUE), body_mass_g_sd = sd(body_mass_g, na.rm = TRUE)) penguins_summarise #&gt; # A tibble: 3 x 3 #&gt; species body_mass_g_mean body_mass_g_sd #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Adelie 3701. 459. #&gt; 2 Chinstrap 3733. 384. #&gt; 3 Gentoo 5076. 504. ## Resumo para várias colunas penguins_summarise_across &lt;- penguins %&gt;% dplyr::group_by(species) %&gt;% dplyr::summarize(across(where(is.numeric), ~ mean(.x, na.rm = TRUE))) penguins_summarise_across #&gt; # A tibble: 3 x 6 #&gt; species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g year #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Adelie 38.8 18.3 190. 3701. 2008. #&gt; 2 Chinstrap 48.8 18.4 196. 3733. 2008. #&gt; 3 Gentoo 47.5 15.0 217. 5076. 2008. 5.8.16 bind_rows() e bind_cols() Muitas vezes teremos de combinar duas ou mais tabelas de dados. Podemos utilizar as funções Base R rbind() e cbind(), como vimos. Entretanto, pode ser interessante avançar para as funções dplyr::bind_rows() e dplyr::bind_cols() do formato tidyverse. A ideia é muito semelhante: a primeira função combina dados por linhas e a segunda por colunas. Entretanto, há vantagens no uso dessas funções, como a identificação das linhas pelo argumento .id para a primeira função, e a conferência do nome das colunas pelo argumento .name_repair para a segunda função. ## Selecionar as linhas para dois tibbles penguins_01 &lt;- dplyr::slice(penguins, 1:5) penguins_02 &lt;- dplyr::slice(penguins, 51:55) ## Combinar as linhas penguins_bind_rows &lt;- dplyr::bind_rows(penguins_01, penguins_02, .id = &quot;id&quot;) head(penguins_bind_rows) #&gt; # A tibble: 6 x 9 #&gt; id species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year #&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 #&gt; 2 1 Adelie Torgersen 39.5 17.4 186 3800 female 2007 #&gt; 3 1 Adelie Torgersen 40.3 18 195 3250 female 2007 #&gt; 4 1 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 #&gt; 5 1 Adelie Torgersen 36.7 19.3 193 3450 female 2007 #&gt; 6 2 Adelie Biscoe 39.6 17.7 186 3500 female 2008 ## Combinar as colunas penguins_bind_cols &lt;- dplyr::bind_cols(penguins_01, penguins_02, .name_repair = &quot;unique&quot;) head(penguins_bind_cols) #&gt; # A tibble: 5 x 16 #&gt; species...1 island...2 bill_length_mm.~ bill_depth_mm..~ flipper_length_~ body_mass_g...6 sex...7 #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female #&gt; # ... with 9 more variables: year...8 &lt;int&gt;, species...9 &lt;fct&gt;, island...10 &lt;fct&gt;, #&gt; # bill_length_mm...11 &lt;dbl&gt;, bill_depth_mm...12 &lt;dbl&gt;, flipper_length_mm...13 &lt;int&gt;, #&gt; # body_mass_g...14 &lt;int&gt;, sex...15 &lt;fct&gt;, year...16 &lt;int&gt; 5.8.17 *_join() Finalmente, veremos o último conjunto de funções do pacote dplyr, a junção de tabelas. Nessa operação, fazemos a combinação de pares de conjunto de dados tabulares por uma ou mais colunas chaves. Há dois tipos de junções: junção de mutação e junção de filtragem. A junção de mutação primeiro combina as observações por suas chaves e, em seguida, copia as variáveis (colunas) de uma tabela para a outra. É fundamental destacar a importância da coluna chave, que é indicada pelo argumento by. Essa coluna deve conter elementos que sejam comuns às duas tabelas para que haja a combinação dos elementos. Existem quatro tipos de junções, que são realizadas pelas funções: dplyr::inner_join(), dplyr::left_join(), dplyr::full_join() e dplyr::right_join(), e que podem ser representadas na Figura 5.3. Figura 5.3: Diferentes tipos de joins, representados com um diagrama de Venn. Adaptado de: Wickham and Grolemund (2017). Considerando a nomenclatura de duas tabelas de dados por x e y, temos: inner_join(x, y): mantém apenas as observações em x e em y left_join(x, y): mantém todas as observações em x right_join(x, y): mantém todas as observações em y full_join(x, y): mantém todas as observações em x e em y Aqui, vamos demostrar apenas a função dplyr::left_join(), combinando um tibble de coordenadas geográficas das ilhas com o conjunto de dados do penguins. ## Adicionar uma coluna chave de ids penguin_islands &lt;- tibble( island = c(&quot;Torgersen&quot;, &quot;Biscoe&quot;, &quot;Dream&quot;, &quot;Alpha&quot;), longitude = c(-64.083333, -63.775636, -64.233333, -63), latitude = c(-64.766667, -64.818569, -64.733333, -64.316667)) ## Junção - left penguins_left_join &lt;- dplyr::left_join(penguins, penguin_islands, by = &quot;island&quot;) head(penguins_left_join) #&gt; # A tibble: 6 x 10 #&gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year longitude #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 -64.1 #&gt; 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 -64.1 #&gt; 3 Adelie Torgersen 40.3 18 195 3250 female 2007 -64.1 #&gt; 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 -64.1 #&gt; 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 -64.1 #&gt; 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 -64.1 #&gt; # ... with 1 more variable: latitude &lt;dbl&gt; Já a junção de filtragem combina as observações da mesma maneira que as junções de mutação, mas afetam as observações (linhas), não as variáveis (colunas). Existem dois tipos. semi_join(x, y): mantém todas as observações em x que têm uma correspondência em y anti_join(x, y): elimina todas as observações em x que têm uma correspondência em y Semi-joins são úteis para corresponder tabelas de resumo filtradas de volta às linhas originais, removendo as linhas que não estavam antes do join. Anti-joins são úteis para diagnosticar incompatibilidades de junção, por exemplo, ao verificar os elementos que não combinam entre duas tabelas de dados. 5.8.18 Operações de conjuntos e comparação de dados Temos ainda operações de conjuntos e comparação de dados. union(x, y): retorna todas as linhas que aparecem em x, y ou mais dos conjuntos de dados interesect(x, y): retorna apenas as linhas que aparecem em x e em y setdiff(x, y): retorna as linhas que aparecem x, mas não em y setequal(x, y): retorna se x e y são iguais e quais suas diferenças Para se aprofundar no tema, recomendamos a leitura do Capítulo 13 Relational data de Wickham and Grolemund (2017). 5.9 stringr O pacote stringr fornece um conjunto de funções para a manipulação de caracteres ou strings. O pacote concentra-se nas funções de manipulação mais importantes e comumente usadas. Para funções mais específicas, recomenda-se usar o pacote stringi, que fornece um conjunto mais abrangente de funções. As funções do stringr podem ser agrupadas em algumas operações para tarefas específicas como correspondência de padrões, retirar e acrescentar espaços em branco, mudar maiúsculas e minúsculas, além de outras operações. Todas as funções deste pacote são listadas na página de referência do pacote. Demonstraremos algumas funções para algumas operações mais comuns, utilizando um vetor de um elemento, com o string penguins. Podemos explorar o comprimento de strings com a função stringr::str_length(). ## Comprimento stringr::str_length(string = &quot;penguins&quot;) #&gt; [1] 8 Extrair um string por sua posição usando a função stringr::str_sub() ou por um padrão com stringr::str_extract(). ## Extrair pela posição stringr::str_sub(string = &quot;penguins&quot;, end = 3) #&gt; [1] &quot;pen&quot; ## Extrair por padrão stringr::str_extract(string = &quot;penguins&quot;, pattern = &quot;p&quot;) #&gt; [1] &quot;p&quot; Substituir strings por outros strings com stringr::str_replace(). ## Substituir stringr::str_replace(string = &quot;penguins&quot;, pattern = &quot;i&quot;, replacement = &quot;y&quot;) #&gt; [1] &quot;penguyns&quot; Separar strings por um padrão com a função stringr::str_split(). ## Separar stringr::str_split(string = &quot;p-e-n-g-u-i-n-s&quot;, pattern = &quot;-&quot;, simplify = TRUE) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; [1,] &quot;p&quot; &quot;e&quot; &quot;n&quot; &quot;g&quot; &quot;u&quot; &quot;i&quot; &quot;n&quot; &quot;s&quot; Inserir espaços em brancos pela esquerda, direita ou ambos com a função stringr::str_pad(). ## Inserir espacos em branco stringr::str_pad(string = &quot;penguins&quot;, width = 10, side = &quot;left&quot;) #&gt; [1] &quot; penguins&quot; stringr::str_pad(string = &quot;penguins&quot;, width = 10, side = &quot;right&quot;) #&gt; [1] &quot;penguins &quot; stringr::str_pad(string = &quot;penguins&quot;, width = 10, side = &quot;both&quot;) #&gt; [1] &quot; penguins &quot; Também podemos remover espaços em branco da esquerda, direita ou ambos, utilizando stringr::str_trim(). ## Remover espacos em branco stringr::str_trim(string = &quot; penguins &quot;, side = &quot;left&quot;) #&gt; [1] &quot;penguins &quot; stringr::str_trim(string = &quot; penguins &quot;, side = &quot;right&quot;) #&gt; [1] &quot; penguins&quot; stringr::str_trim(string = &quot; penguins &quot;, side = &quot;both&quot;) #&gt; [1] &quot;penguins&quot; Podemos também alterar minúsculas e maiúsculas em diferentes posições do string, com várias funções. ## Alterar minúsculas e maiúsculas stringr::str_to_lower(string = &quot;Penguins&quot;) #&gt; [1] &quot;penguins&quot; stringr::str_to_upper(string = &quot;penguins&quot;) #&gt; [1] &quot;PENGUINS&quot; stringr::str_to_sentence(string = &quot;penGuins&quot;) #&gt; [1] &quot;Penguins&quot; stringr::str_to_title(string = &quot;penGuins&quot;) #&gt; [1] &quot;Penguins&quot; Podemos ainda ordenar os elementos de um vetor por ordem alfabética de forma crescente ou decrescente, usando stringr::str_sort(). ## Ordenar stringr::str_sort(x = letters) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; #&gt; [24] &quot;x&quot; &quot;y&quot; &quot;z&quot; stringr::str_sort(x = letters, dec = TRUE) #&gt; [1] &quot;z&quot; &quot;y&quot; &quot;x&quot; &quot;w&quot; &quot;v&quot; &quot;u&quot; &quot;t&quot; &quot;s&quot; &quot;r&quot; &quot;q&quot; &quot;p&quot; &quot;o&quot; &quot;n&quot; &quot;m&quot; &quot;l&quot; &quot;k&quot; &quot;j&quot; &quot;i&quot; &quot;h&quot; &quot;g&quot; &quot;f&quot; &quot;e&quot; &quot;d&quot; #&gt; [24] &quot;c&quot; &quot;b&quot; &quot;a&quot; Podemos ainda utilizar essas funções em complemento com o pacote dplyr, para alterar os strings de colunas ou nome das colunas. ## Alterar valores das colunas penguins_stringr_valores &lt;- penguins %&gt;% dplyr::mutate(species = stringr::str_to_lower(species)) ## Alterar nome das colunas penguins_stringr_nomes &lt;- penguins %&gt;% dplyr::rename_with(stringr::str_to_title) Para se aprofundar no tema, recomendamos a leitura do Capítulo 14 Strings de Wickham and Grolemund (2017). 5.10 forcats O pacote forcats fornece um conjunto de ferramentas úteis para facilitar a manipulação de fatores. Como dito anteriormente, usamos fatores geralmente quando temos dados categóricos, que são variáveis que possuem um conjunto de valores fixos e conhecidos. As funções são utilizadas principalmente para: mudar a ordem dos níveis, mudar os valores dos níveis, adicionar e remover níveis, combinar múltiplos níveis, além de outras operações. Todas as funções deste pacote são listadas na página de referência do pacote. Vamos utilizar ainda os dados penguins e penguins_raw para exemplificar o uso do pacote forcats. ## Carregar o pacote palmerpenguins library(palmerpenguins) Primeiramente, vamos converter dados de string para fator, utilizando a função forcats::as_factor(). ## String forcats::as_factor(penguins_raw$Species) %&gt;% head() #&gt; [1] Adelie Penguin (Pygoscelis adeliae) Adelie Penguin (Pygoscelis adeliae) #&gt; [3] Adelie Penguin (Pygoscelis adeliae) Adelie Penguin (Pygoscelis adeliae) #&gt; [5] Adelie Penguin (Pygoscelis adeliae) Adelie Penguin (Pygoscelis adeliae) #&gt; 3 Levels: Adelie Penguin (Pygoscelis adeliae) ... Chinstrap penguin (Pygoscelis antarctica) Podemos facilmente mudar o nome dos níveis utilizando a função forcats::fct_recode(). ## Mudar o nome dos níveis forcats::fct_recode(penguins$species, a = &quot;Adelie&quot;, c = &quot;Chinstrap&quot;, g = &quot;Gentoo&quot;) %&gt;% head() #&gt; [1] a a a a a a #&gt; Levels: a c g Para inverter os níveis, usamos a função forcats::fct_rev(). ## Inverter os níveis forcats::fct_rev(penguins$species) %&gt;% head() #&gt; [1] Adelie Adelie Adelie Adelie Adelie Adelie #&gt; Levels: Gentoo Chinstrap Adelie Uma operação muito comum com fatores é mudar a ordem dos níveis. Quando precisamos especificar a ordem dos níveis, podemos fazer essa operação manualmente com a função forcats::fct_relevel(). ## Especificar a ordem dos níveis forcats::fct_relevel(penguins$species, &quot;Chinstrap&quot;, &quot;Gentoo&quot;, &quot;Adelie&quot;) %&gt;% head() #&gt; [1] Adelie Adelie Adelie Adelie Adelie Adelie #&gt; Levels: Chinstrap Gentoo Adelie Como vimos, a reordenação dos níveis pode ser feita manualmente. Mas existem outras formas automáticas de reordenação seguindo algumas regras, para as quais existem funções específicas. forcats::fct_inorder(): pela ordem em que aparecem pela primeira vez forcats::fct_infreq(): por número de observações com cada nível (decrescente, i.e., o maior primeiro) forcats::fct_inseq(): pelo valor numérico do nível ## Níveis pela ordem em que aparecem forcats::fct_inorder(penguins$species) %&gt;% head() #&gt; [1] Adelie Adelie Adelie Adelie Adelie Adelie #&gt; Levels: Adelie Gentoo Chinstrap ## Ordem (decrescente) de frequência forcats::fct_infreq(penguins$species) %&gt;% head() #&gt; [1] Adelie Adelie Adelie Adelie Adelie Adelie #&gt; Levels: Adelie Gentoo Chinstrap Por fim, podemos fazer a agregação de níveis raros em um nível utilizando a função forcats::fct_lump(). ## Agregação de níveis raros em um nível forcats::fct_lump(penguins$species) %&gt;% head() #&gt; [1] Adelie Adelie Adelie Adelie Adelie Adelie #&gt; Levels: Adelie Gentoo Other Podemos ainda utilizar essas funções em complemento com o pacote dplyr para fazer manipulações de fatores nas colunas de tibbles. ## Transformar várias colunas em fator penguins_raw_multi_factor &lt;- penguins_raw %&gt;% dplyr::mutate(across(where(is.character), forcats::as_factor)) Para se aprofundar no tema, recomendamos a leitura do Capítulo 15 Factors de Wickham and Grolemund (2017). 5.11 lubridate O pacote lubridate fornece um conjunto de funções para a manipulação de dados de data e horário. Dessa forma, esse pacote facilita a manipulação dessa classe de dado no R, pois geralmente esses dados não são intuitivos e mudam dependendo do tipo de objeto de data e horário. Além disso, os métodos que usam datas e horários devem levar em consideração fusos horários, anos bissextos, horários de verão, além de outras particularidades. Existem diversas funções nesse pacote, sendo as mesmas focadas em: transformações de data/horário, componentes, arredondamentos, durações, períodos, intervalos, além de muitas outras funções específicas. Todas as funções deste pacote são listadas na página de referência do pacote. Apesar de estar inserido no escopo do tidyverse, este pacote não é carregado com os demais, requisitando seu carregamento solo. ## Carregar library(lubridate) Existem três tipos de dados data/horário: Data: tempo em dias, meses e anos &lt;date&gt; Horário: tempo dentro de um dia &lt;time&gt; Data-horário: tempo em um instante (data mais tempo) &lt;dttm&gt; Para trabalhar exclusivamente com horários, podemos utilizar o pacote hms. É fundamental também destacar que algumas letras terão um significado temporal, sendo abreviações de diferentes períodos em inglês: year (ano), month (mês), weak (semana), day (dia), hour (hora), minute (minuto), e second (segundo). Para acessar a informação da data e horários atuais podemos utilizar as funções lubridate::today() e lubridate::now(). ## Extrair a data nesse instante lubridate::today() #&gt; [1] &quot;2021-10-26&quot; ## Extrair a data e tempo nesse instante lubridate::now() #&gt; [1] &quot;2021-10-26 11:46:09 CEST&quot; Além dessas informações instantâneas, existem três maneiras de criar um dado de data/horário. De um string De componentes individuais de data e horário De um objeto de data/horário existente Os dados de data/horário geralmente estão no formato de strings. Podemos transformar os dados especificando a ordem dos seus componentes, ou seja, a ordem em que ano, mês e dia aparecem no string, usando as letras y (ano), m (mês) e d (dia) na mesma ordem, por exemplo, lubridate::dmy(). ## Strings e números para datas lubridate::dmy(&quot;03-03-2021&quot;) #&gt; [1] &quot;2021-03-03&quot; Essas funções também aceitam números sem aspas, além de serem muito versáteis e funcionarem em outros diversos formatos. ## Strings e números para datas lubridate::dmy(&quot;03-Mar-2021&quot;) lubridate::dmy(03032021) lubridate::dmy(&quot;03032021&quot;) lubridate::dmy(&quot;03/03/2021&quot;) lubridate::dmy(&quot;03.03.2021&quot;) Além da data, podemos especificar horários atrelados a essas datas. Para criar uma data com horário adicionamos um underscore (_) e os h (hora), m (minuto) e s (segundo) ao nome da função, além do argumento tz para especificar o fuso horário (tema tratado mais adiante nessa seção). ## Especificar horários e fuso horário lubridate::dmy_h(&quot;03-03-2021 13&quot;) #&gt; [1] &quot;2021-03-03 13:00:00 UTC&quot; lubridate::dmy_hm(&quot;03-03-2021 13:32&quot;) #&gt; [1] &quot;2021-03-03 13:32:00 UTC&quot; lubridate::dmy_hms(&quot;03-03-2021 13:32:01&quot;) #&gt; [1] &quot;2021-03-03 13:32:01 UTC&quot; lubridate::dmy_hms(&quot;03-03-2021 13:32:01&quot;, tz = &quot;America/Sao_Paulo&quot;) #&gt; [1] &quot;2021-03-03 13:32:01 -03&quot; Podemos ainda ter componentes individuais de data/horário em múltiplas colunas. Para realizar essa transformação, podemos usar as funções lubridate::make_date() e lubridate::make_datetime(). ## Dados com componentes individuais dados &lt;- tibble::tibble( ano = c(2021, 2021, 2021), mes = c(1, 2, 3), dia = c(12, 20, 31), hora = c(2, 14, 18), minuto = c(2, 44, 55)) ## Data de componentes individuais dados %&gt;% dplyr::mutate(data = lubridate::make_datetime(ano, mes, dia, hora, minuto)) #&gt; # A tibble: 3 x 6 #&gt; ano mes dia hora minuto data #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 2021 1 12 2 2 2021-01-12 02:02:00 #&gt; 2 2021 2 20 14 44 2021-02-20 14:44:00 #&gt; 3 2021 3 31 18 55 2021-03-31 18:55:00 Por fim, podemo criar datas modificando entre data/horário e data, utilizando as funções lubridate::as_datetime() e lubridate::as_date(). ## Data para data-horário lubridate::as_datetime(today()) #&gt; [1] &quot;2021-10-26 UTC&quot; ## Data-horário para data lubridate::as_date(now()) #&gt; [1] &quot;2021-10-26&quot; Uma vez que entendemos como podemos criar dados de data/horário, podemos explorar funções para acessar e definir componentes individuais. Para essa tarefa existe uma grande quantidade de funções para acessar de partes específicas de datas e horários. year(): acessa o ano month(): acessa o mês month(): acessa o dia yday(): acessa o dia do ano mday(): acessa o dia do mês wday(): acessa o dia da semana hour(): acessa as horas minute(): acessa os minutos second(): acessa os segundos ## Extrair lubridate::year(now()) #&gt; [1] 2021 lubridate::month(now()) #&gt; [1] 10 lubridate::month(now(), label = TRUE) #&gt; [1] Oct #&gt; Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; Oct &lt; Nov &lt; Dec lubridate::day(now()) #&gt; [1] 26 lubridate::wday(now()) #&gt; [1] 3 lubridate::wday(now(), label = TRUE) #&gt; [1] Tue #&gt; Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat lubridate::second(now()) #&gt; [1] 9.436133 Além de acessar componentes de datas e horários, podemos usar essas funções para fazer a inclusão de informações de datas e horários. ## Data data &lt;- dmy_hms(&quot;04-03-2021 01:04:56&quot;) ## Incluir lubridate::year(data) &lt;- 2020 lubridate::month(data) &lt;- 01 lubridate::hour(data) &lt;- 13 Mais convenientemente, podemos utilizar a função update() para alterar vários valores de uma vez. ## Incluir vários valores update(data, year = 2020, month = 1, mday = 1, hour = 1) #&gt; [1] &quot;2020-01-01 01:04:56 UTC&quot; Muitas vezes precisamos fazer operações com datas, como a aritmética com datas: subtração, adição e divisão. Para tanto, é preciso entender três classes importantes que representam intervalos de tempo. Durações: representam um número exato de segundos Períodos: representam unidades humanas como semanas e meses Intervalos: representam um ponto inicial e final Quando fazemos uma subtração de datas, criamos um objeto da classe difftime. Essa classe pode ser um pouco complicada de trabalhar, então dentro do lubridate, podemos usar funções que convertem essa classe em duração, da classe Duration. As durações sempre registram o intervalo de tempo em segundos, com alguma unidade de tempo maior entre parênteses. Há uma série de funções para tratar dessa classe. duration(): cria data em duração as.duration(): converte datas em duração dyears(): duração de anos dmonths(): duração de meses dweeks(): duração de semanas ddays(): duração de dias dhours(): duração de horas dminutes(): duração de minutos dseconds(): duração de segundos ## Subtração de datas tempo_estudando_r &lt;- lubridate::today() - lubridate::dmy(&quot;30-11-2011&quot;) ## Conversão para duração tempo_estudando_r_dur &lt;- lubridate::as.duration(tempo_estudando_r) ## Criando durações lubridate::duration(90, &quot;seconds&quot;) #&gt; [1] &quot;90s (~1.5 minutes)&quot; lubridate::duration(1.5, &quot;minutes&quot;) #&gt; [1] &quot;90s (~1.5 minutes)&quot; lubridate::duration(1, &quot;days&quot;) #&gt; [1] &quot;86400s (~1 days)&quot; ## Transformação da duração lubridate::dseconds(100) #&gt; [1] &quot;100s (~1.67 minutes)&quot; lubridate::dminutes(100) #&gt; [1] &quot;6000s (~1.67 hours)&quot; lubridate::dhours(100) #&gt; [1] &quot;360000s (~4.17 days)&quot; lubridate::ddays(100) #&gt; [1] &quot;8640000s (~14.29 weeks)&quot; lubridate::dweeks(100) #&gt; [1] &quot;60480000s (~1.92 years)&quot; lubridate::dyears(100) #&gt; [1] &quot;3155760000s (~100 years)&quot; Podemos ainda utilizar as durações para fazer operações aritméticas com datas como adição, subtração e multiplicação. ## Somando durações a datas lubridate::today() + lubridate::ddays(1) #&gt; [1] &quot;2021-10-27&quot; ## Subtraindo durações de datas lubridate::today() - lubridate::dyears(1) #&gt; [1] &quot;2020-10-25 18:00:00 UTC&quot; ## Multiplicando durações 2 * dyears(2) #&gt; [1] &quot;126230400s (~4 years)&quot; Além das durações, podemos usar períodos, que são extensões de tempo não fixados em segundos como as durações, mas flexíveis, com o tempo em dias, semanas, meses ou anos, permitindo uma interpretação mais intuitiva das datas. Novamente, há uma série de funções para realizar essas operações. period(): cria data em período as.period(): converte datas em período seconds(): período em segundos minutes(): período em minutos hours(): período em horas days(): período em dias weeks(): período em semanas months(): período em meses years(): período em anos ## Criando períodos period(c(90, 5), c(&quot;second&quot;, &quot;minute&quot;)) #&gt; [1] &quot;5M 90S&quot; period(c(3, 1, 2, 13, 1), c(&quot;second&quot;, &quot;minute&quot;, &quot;hour&quot;, &quot;day&quot;, &quot;week&quot;)) #&gt; [1] &quot;20d 2H 1M 3S&quot; ## Transformação de períodos lubridate::seconds(100) #&gt; [1] &quot;100S&quot; lubridate::minutes(100) #&gt; [1] &quot;100M 0S&quot; lubridate::hours(100) #&gt; [1] &quot;100H 0M 0S&quot; lubridate::days(100) #&gt; [1] &quot;100d 0H 0M 0S&quot; lubridate::weeks(100) #&gt; [1] &quot;700d 0H 0M 0S&quot; lubridate::years(100) #&gt; [1] &quot;100y 0m 0d 0H 0M 0S&quot; Além disso, podemos fazer operações com os períodos, somando e subtraindo. ## Somando datas lubridate::today() + lubridate::weeks(10) #&gt; [1] &quot;2022-01-04&quot; ## Subtraindo datas lubridate::today() - lubridate::weeks(10) #&gt; [1] &quot;2021-08-17&quot; ## Criando datas recorrentes lubridate::today() + lubridate::weeks(0:10) #&gt; [1] &quot;2021-10-26&quot; &quot;2021-11-02&quot; &quot;2021-11-09&quot; &quot;2021-11-16&quot; &quot;2021-11-23&quot; &quot;2021-11-30&quot; &quot;2021-12-07&quot; #&gt; [8] &quot;2021-12-14&quot; &quot;2021-12-21&quot; &quot;2021-12-28&quot; &quot;2022-01-04&quot; Por fim, intervalos são períodos de tempo limitados por duas datas, possuindo uma duração com um ponto de partida, que o faz preciso para determinar uma duração. Intervalos são objetos da classe Interval. Da mesma forma que para duração e períodos, há uma série de funções para realizar essas operações. interval(): cria data em intervalo %--%: cria data em intervalo as.interval(): converte datas em intervalo int_start(): acessa ou atribui data inicial de um intervalo int_end(): acessa ou atribui data final de um intervalo int_length(): comprimento de um intervalo em segundos int_flip(): inverte a ordem da data de início e da data de término em um intervalo int_shift(): desloca as datas de início e término de um intervalo int_aligns(): testa se dois intervalos compartilham um ponto final int_standardize(): garante que todos os intervalos sejam positivos int_diff(): retorna os intervalos que ocorrem entre os elementos de data/horário int_overlaps(): testa se dois intervalos se sobrepõem %within%: testa se o primeiro intervalo está contido no segundo ## Criando duas datas - início de estudos do R e nascimento do meu filho r_inicio &lt;- lubridate::dmy(&quot;30-11-2011&quot;) filho_nascimento &lt;- lubridate::dmy(&quot;26-09-2013&quot;) r_hoje &lt;- lubridate::today() ## Criando intervalos - interval r_intervalo &lt;- lubridate::interval(r_inicio, r_hoje) ## Criando intervalos - interval %--% filho_intervalo &lt;- filho_nascimento %--% lubridate::today() ## Operações com intervalos lubridate::int_start(r_intervalo) #&gt; [1] &quot;2011-11-30 UTC&quot; lubridate::int_end(r_intervalo) #&gt; [1] &quot;2021-10-26 UTC&quot; lubridate::int_length(r_intervalo) #&gt; [1] 312595200 lubridate::int_flip(r_intervalo) #&gt; [1] 2021-10-26 UTC--2011-11-30 UTC lubridate::int_shift(r_intervalo, duration(days = 30)) #&gt; [1] 2011-12-30 UTC--2021-11-25 UTC Uma operação de destaque é verificar a sobreposição entre dois intervalos. ## Verificar sobreposição - int_overlaps lubridate::int_overlaps(r_intervalo, filho_intervalo) #&gt; [1] TRUE ## Verificar se intervalo está contido r_intervalo %within% filho_intervalo #&gt; [1] FALSE filho_intervalo %within% r_intervalo #&gt; [1] TRUE Podemos ainda calcular quantos períodos existem dentro de um intervalo, utilizando as operações de / e %/%. ## Períodos dentro de um intervalo - anos r_intervalo / lubridate::years() #&gt; [1] 9.90411 r_intervalo %/% lubridate::years() #&gt; [1] 9 ## Períodos dentro de um intervalo - dias e semandas filho_intervalo / lubridate::days() #&gt; [1] 2952 filho_intervalo / lubridate::weeks() #&gt; [1] 421.7143 Ainda podemos fazer transformações dos dados para períodos e ter todas as unidades de data e tempo que o intervalo compreende. ## Tempo total estudando R lubridate::as.period(r_intervalo) #&gt; [1] &quot;9y 10m 26d 0H 0M 0S&quot; ## Idade do meu filho lubridate::as.period(filho_intervalo) #&gt; [1] &quot;8y 1m 0d 0H 0M 0S&quot; Por fim, fusos horários tendem a ser um fator complicador quando precisamos analisar informações instantâneas de tempo (horário) de outras partes do planeta, ou mesmo fazer conversões dos horários. No lubridate há funções para ajudar nesse sentido. Para isso, podemos utilizar a função lubridate::with_tz(), e no argumento tzone informar o fuso horário para a transformação do horário. Podemos descobrir o fuso horário que o R está considerando com a função Sys.timezone(). ## Fuso horário no R Sys.timezone() #&gt; [1] &quot;Europe/Berlin&quot; No R há uma listagem dos nomes dos fusos horários que podemos utilizar no argumento tzone para diferentes fusos horários. ## Verificar os fuso horários length(OlsonNames()) #&gt; [1] 593 head(OlsonNames()) #&gt; [1] &quot;Africa/Abidjan&quot; &quot;Africa/Accra&quot; &quot;Africa/Addis_Ababa&quot; &quot;Africa/Algiers&quot; #&gt; [5] &quot;Africa/Asmara&quot; &quot;Africa/Asmera&quot; Podemos nos perguntar que horas são em outra parte do globo ou fazer as conversões facilmente no lubridate. ## Que horas são em... lubridate::with_tz(lubridate::now(), tzone = &quot;America/Sao_Paulo&quot;) #&gt; [1] &quot;2021-10-26 06:46:10 -03&quot; lubridate::with_tz(lubridate::now(), tzone = &quot;GMT&quot;) #&gt; [1] &quot;2021-10-26 09:46:10 GMT&quot; lubridate::with_tz(lubridate::now(), tzone = &quot;Europe/Berlin&quot;) #&gt; [1] &quot;2021-10-26 11:46:10 CEST&quot; ## Altera o fuso sem mudar a hora lubridate::force_tz(lubridate::now(), tzone = &quot;GMT&quot;) #&gt; [1] &quot;2021-10-26 11:46:10 GMT&quot; Para se aprofundar no tema, recomendamos a leitura do Capítulo 16 Dates and times de Wickham and Grolemund (2017). 5.12 purrr O pacote purrr implementa a Programação Funcional no R, fornecendo um conjunto completo e consistente de ferramentas para trabalhar com funções e vetores. A programação funcional é um assunto bastante extenso, sendo mais conhecido no R pela família de funções purrr::map(), que permite substituir muitos loops for por um código mais sucinto e fácil de ler. Não focaremos aqui nas outras funções. Todas as funções deste pacote são listadas na página de referência do pacote. Um loop for pode ser entendido como uma iteração: um bloco de códigos é repetido mudando um contador de uma lista de possibilidades. Vamos exemplificar com uma iteração bem simples, onde imprimiremos no console os valores de 1 a 10, utilizando a função for(), um contador i em um vetor de dez números 1:10 que será iterado, no bloco de códigos definido entre {}, usando a função print() para imprimir os valores. A ideia é bastante simples: a função for() vai atribuir o primeiro valor da lista ao contador i, esse contador será utilizado em todo o bloco de códigos. Quando o bloco terminar, o segundo valor é atribuído ao contador i e entra no bloco de códigos, repetindo esse processo até que todos os elementos da lista tenham sido atribuídos ao contador. ## Loop for for(i in 1:10){ print(i) } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 #&gt; [1] 6 #&gt; [1] 7 #&gt; [1] 8 #&gt; [1] 9 #&gt; [1] 10 Com essa ideia em mente, a programação funcional utilizando a função purrr::map(). O mesmo for ficaria dessa forma. ## Loop for com map purrr::map(.x = 1:10, .f = print) #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 #&gt; [1] 6 #&gt; [1] 7 #&gt; [1] 8 #&gt; [1] 9 #&gt; [1] 10 #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 2 #&gt; #&gt; [[3]] #&gt; [1] 3 #&gt; #&gt; [[4]] #&gt; [1] 4 #&gt; #&gt; [[5]] #&gt; [1] 5 #&gt; #&gt; [[6]] #&gt; [1] 6 #&gt; #&gt; [[7]] #&gt; [1] 7 #&gt; #&gt; [[8]] #&gt; [1] 8 #&gt; #&gt; [[9]] #&gt; [1] 9 #&gt; #&gt; [[10]] #&gt; [1] 10 Nessa estrutura, temos: map(.x, .f) .x: um vetor, lista ou data frame .f: uma função Num outro exemplo, aplicaremos a função sum() para somar os valores de vários elementos de uma lista. ## Função map x &lt;- list(1:5, c(4, 5, 7), c(1, 1, 1), c(2, 2, 2, 2, 2)) purrr::map(x, sum) #&gt; [[1]] #&gt; [1] 15 #&gt; #&gt; [[2]] #&gt; [1] 16 #&gt; #&gt; [[3]] #&gt; [1] 3 #&gt; #&gt; [[4]] #&gt; [1] 10 Há diferente tipos de retornos da família purrr::map(). map(): retorna uma lista map_chr(): retorna um vetor de strings map_dbl(): retorna um vetor numérico (double) map_int(): retorna um vetor numérico (integer) map_lgl(): retorna um vetor lógico map_dfr(): retorna um data frame (por linhas) map_dfc(): retorna um data frame (por colunas) ## Variações da função map purrr::map_dbl(x, sum) #&gt; [1] 15 16 3 10 purrr::map_chr(x, paste, collapse = &quot; &quot;) #&gt; [1] &quot;1 2 3 4 5&quot; &quot;4 5 7&quot; &quot;1 1 1&quot; &quot;2 2 2 2 2&quot; Essas funcionalidades já eram conhecidas no Base R pelas funções da família apply: apply(), lapply(), sapply(), vapply(), mapply(), rapply() e tapply(). Essas funções formam a base de combinações mais complexas e ajudam a realizar operações com poucas linhas de código, para diferentes retornos. Temos ainda duas variantes da função map(): purrr::map2() e purrr::pmap(), para duas ou mais listas, respectivamente. Como vimos para a primeira função, existem várias variações do sufixo para modificar o retorno da função. ## Listas x &lt;- list(3, 5, 0, 1) y &lt;- list(3, 5, 0, 1) z &lt;- list(3, 5, 0, 1) ## Função map2 purrr::map2_dbl(x, y, prod) #&gt; [1] 9 25 0 1 ## Função pmap purrr::pmap_dbl(list(x, y, z), prod) #&gt; [1] 27 125 0 1 Essas funções podem ser usadas em conjunto para implementar rotinas de manipulação e análise de dados com poucas linhas de código, mas que não exploraremos em sua completude aqui. Listamos dois exemplos simples. ## Resumo dos dados penguins %&gt;% dplyr::select(where(is.numeric)) %&gt;% tidyr::drop_na() %&gt;% purrr::map_dbl(mean) #&gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g year #&gt; 43.92193 17.15117 200.91520 4201.75439 2008.02924 ## Análise dos dados penguins %&gt;% dplyr::group_split(island, species) %&gt;% purrr::map(~ lm(bill_depth_mm ~ bill_length_mm, data = .x)) %&gt;% purrr::map(summary) %&gt;% purrr::map(&quot;r.squared&quot;) #&gt; [[1]] #&gt; [1] 0.2192052 #&gt; #&gt; [[2]] #&gt; [1] 0.4139429 #&gt; #&gt; [[3]] #&gt; [1] 0.2579242 #&gt; #&gt; [[4]] #&gt; [1] 0.4271096 #&gt; #&gt; [[5]] #&gt; [1] 0.06198376 Para se aprofundar no tema, recomendamos a leitura do Capítulo 21 Iteration de Wickham and Grolemund (2017). 5.13 Exercícios Reescreva as operações abaixo utilizando pipes %&gt;%. log10(cumsum(1:100)) sum(sqrt(abs(rnorm(100)))) sum(sort(sample(1:10, 10000, rep = TRUE))) Use a função download.file() e unzip() para baixar e extrair o arquivo do data paper de médios e grandes mamíferos: ATLANTIC MAMMALS. Em seguinda, importe para o R, usando a função readxl::read_excel(). Use a função tibble::glimpse() para ter uma noção geral dos dados importados no item anterior. Compare os dados de penguins (palmerpenguins::penguins_raw e palmerpenguins::penguins). Monte uma série de funções dos pacotes tidyr e dplyr para fazer limpar os dados e fazer com que o primeiro dado seja igual ao segundo. Usando os dados de penguins (palmerpenguins::penguins), calcule a correlação de Pearson entre comprimento e profundidade do bico para cada espécie e para todas as espécies. Compare os índices de correlação para exemplificar o Paradoxo de Simpsom. Oficialmente a pandemia de COVID-19 começou no Brasil com o primeiro caso no dia 26 de fevereiro de 2020. Calcule quantos anos, meses, dias, semanas, horas, minutos e segundos se passou desde então. Calcule também quanto tempo se passou até você ser vacinado. 5.14 Para se aprofundar Listamos a seguir livros que recomendamos para seguir com sua aprendizagem em R e tidyverse. Português Damiani A, Milz B, Lente C, Falbel D, Correa F, Trecenti J, Luduvice N, Amorim W. 2021. Ciência de Dados em R. [https://livro.curso-r.com/] Faria PD, Parga JPFA. 2020. Introdução à Linguagem R: seus fundamentos e sua prática. [https://www.researchgate.net/publication/345985082_Introducao_a_Linguagem_R_seus_fundamentos_e_sua_pratica] Oliveira PF, Guerra S, Mcdonnell, R. 2018. Ciência de dados com R  Introdução. IBPAD. [https://cdr.ibpad.com.br/] Inglês Grolemund G. 2017. The Essentials of Data Science: Knowledge Discovery Using R. Chapman and Hall/CRC. Holmes S, Huber W. 2019. Modern Statistics for Modern Biology. Cambridge University Press. [https://www.huber.embl.de/msmb/] Irizarry RA. 2019. Introduction to Data Science: Data Analysis and Prediction Algorithms with R. Chapman and Hall/CRC. [https://rafalab.github.io/dsbook/] Ismay C., Kim AY. 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman and Hall/CRC. [https://moderndive.com/] Peng DP. 2020. R Programming for Data Science. [https://bookdown.org/rdpeng/rprogdatascience/] Wickham H, Grolemund G. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. OReilly Media. [https://r4ds.had.co.nz/] Wright C, Ellis S, Hicks S &amp; Peng R D. 2021. Tidyverse Skills for Data Science in R. [https://jhudatascience.org/tidyversecourse/] Zumel N, Mount J. 2014. Practical Data Science with R Paperback. Manning. Referências "],["cap6.html", "Capítulo 6 Guia de bolso de gráficos no R 6.1 1. Introdução 6.2 2. Pacotes necessários 6.3 Principais pacotes 6.4 3. Grámatica dos gráficos 6.5 4. Tipos de gráficos 6.6 4.7. Gráfico de dispersão (scatter plot) 6.7 4.8. Visualização de múltiplos gráficos pareados 6.8 5. Erros comuns dos usuários do ggplot2 e como evitá-los 6.9 6. Finalização de gráficos para publicação 6.10 Check-list para garantir bons gráficos 6.11 Para se aprofundar", " Capítulo 6 Guia de bolso de gráficos no R 6.1 1. Introdução A visualização de dados através de gráficos geralmente é a melhor forma de apresentar e interpretar as informações contidas em seus estudos, fazendo uma uma síntese para melhor entendimento de padrões. Geralmente, os gráficos são necessários em quase todas as análises estatísticas, além de enriquecer a argumentação e discussão de hipóteses levatandas para publicações, trabalhos de consultoria, TCC, dissertação, tese, entre outros. Existem vários tipos de gráficos para representar os padrões em seus dados para diferentes tipos de finalidades. Esses diferentes tipos de gráficos podem até mesmo ser usados para representar o mesmo tipo de dado. Nesta seção, focaremos nos gráficos mais simples, para representar uma ou duas variáveis (i.e., gráficos bidimensionais). Os gráficos mais indicados para representar um conunto de dados mudam dependendo do tipo de variável (categórica ou contínua - veja os tipos no Capítulo 3). De forma simplificada, os gráficos são representações dos nossos dados tabulares. Os eixos de um gráfico representam as colunas (variáveis) e as atributos estéticos (asesthetics: pontos, linhas, barras, caixas, etc.) representam as linhas da tabela. Geralmente, os gráficos vão ser a representação de uma ou duas colunas, quando muito três, em gráficos de três dimensões. Para mais colunas, partimos para dados agregados que são vistos nos capítulo de análise multivariada. Além disso, a utilização de mais de duas colunas pode estar relacionado com outros atributos estéticos (aes()) do gráfico como cor, forma e tamanho de pontos e linhas. Dessa forma, dedicamos esse capítulo inteiramente a apresentar os principais conceitos, como a gramática de gráficos, e uma apresentação geral que pode funcionar como um guia de bolso de gráficos, uma vez que apresentamos os principais tipos de gráficos para análises ecológicas. Além disso, no último tópico deste capítulo focamos na finalização (ajustes finos) de gráficos para publicação. Este capítulo fornece as bases conceitual e prática necessária para enteder a visualização gráfica apresentada nos capítulos 6 a 14. Existe uma ampla gama de pacotes para fazer gráficos no R, sendo esse um ponto muito forte dessa linguagem. Além disso, a ampla disponibilidade de pacotes e funções permitem a visualização dos mais diferentes tipos de dados, o que torna a linguagem R com alta praticidade, uma vez que a maior parte dos pacotes possui uma sintaxe relativamente simples para a apresentação de gráficos excelentes e de ótima qualidade. Mais adiante no Capítulo 14, ampliamos a discussão da visualização gráfica com ferramentas para construção de mapas no R. Este capítulo foi organizdo em quatro partes: (i) principais pacotes, (ii) gramáticas dos gráficos, (iii) um guia de bolso para visualização de vários gráficos no R, e (iv) edição de gráfico com qualidade para publicação. Portanto, apesar de apresentarmos diferentes pacotes com grande potencial para visualização gráfica, nest capítulo iremos focar no pacote ggplot2. Usaremos os dados de medidas de pinguins chamados palmerpenguins para exemplicar as funções do ggplot2 que geram diferentes tipos de gráficos . Esses dados estão disponíveis no pacote palmerpenguins, que foram coletados e disponibilizados pela Dra. Kristen Gorman e Palmer Station, Antarctica LTER, ambas do Long Term Ecological Research Network. O pacote palmerpenguins contém dois conjuntos de dados. Um é chamado de penguins e é uma versão simplificada dos dados brutos. O segundo conjunto de dados é penguins_raw e contém todas as variáveis e nomes originais baixados. Ambos os conjuntos de dados contêm dados para 344 pinguins, de três espécies diferentes, coletados em três ilhas no arquipélago de Palmer, na Antártica. 6.2 2. Pacotes necessários library(ggplot2) library(tidyverse) library(palmerpenguins) library(datasauRus) library(Rmisc) library(gridExtra) Apesar do foco no ggplot2, abaixo detalhamos os principais pacotes e suas funções para visualização gráfica: 6.3 Principais pacotes A seguir, apresentamos uma listagem dos principais pacotes para fazer gráficos no R e, além disso, incluimos as principais funções desses pacotes: graphics: é o pacote default do R para produzir gráfios simples, porém útil para visualizações rápidas de quase todos as classes de objetos. Possui funções como: plot(), hist(), barplot(), boxplot(), abline(), points(), lines() e polygon(). ggplot2: pacote integrado ao tidyverse (Capítulo ??), possui uma sintaxe própria baseada na grática de gráficos por camadas (layers), necessitando de funções específicas para objetos de classes diferentes, demandando geralmente mais tempo para realização. Possui funções como ggplot(), aes(), geom_*(), facet_*(), stats_*(), coord_*() e theme_*(), que são conectadas pelo operador +. ggplot2 extentions: conjunto de pacotes que adicionam diversas expansões ao pacote ggplot2. Exemplos: gganimate, GGally e esquisse. visdat: Crie visualizações preliminares de dados exploratórios de um conjunto de dados inteiro para identificar problemas ou recursos inesperados usando ggplot2. Possui diversas funções específicas: vis_dat() - visão geral dos dados, vis_miss() - visão de dados faltantes (NA), vis_compare() - visualiza a diferença entre dados. ggpubr: pacote que fornece funções simplificadas para criar e personalizar gráficos para publicação baseados no ggplot2. Possui funções específicas: gghistogram(), ggdensity(), ggboxplot(), ggviolin(), ggbarplot() e ggscatter(). pacthwork: pacote que permite combinar vários gráficos em um só de forma extremamente simples e com alta qualidade. plotly: pacote para criar gráficos interativos da web por meio da biblioteca gráfica de JavaScript de código aberto plotly.js. Também possui funções específicas: plot_ly(), add_histogram(), add_bars(), add_boxplot(), add_markers(), add_paths(), add_lines() e add_polygons(). 6.4 3. Grámatica dos gráficos No livro A Gramática do Gráfico, Leland Wilkinson (2005) utiliza uma analogia da linguística para criar esta gramática para a visualizaçã gráfica. Segundo ele, a língua se torna expressiva pelo fato da gramática criar um sistema de regras que tornam as declarações com significado conhecido. De maneira semelhante, a ideia da gramática dos gráficos cria regras para representação gráfica dos dados a partir de atributos estéticos (do inglês aesthetic) como cor, forma e tamanho que definem a geometria dos objetos, como pontos, linhas e barras (Wickham 2009). Além disso, esta gramática reconhece que tais elementos podem ser organizados em camadas, tal como construímos um mapa com diferentes camadas como elevação, hidrografia, rodovias, limites políticos, etc. Inspirado pela Grámatica do Gráfico proposta por Wilkinson, Hadley Wickham crious o pacote ggplot2, onde gg representa a contração de Grammar of Graphics (Wickham 2009). As camadas nesta gramática são organizadas da seguinte forma: Camada 1 - dados brutos: as colunas da matriz são usadas para guiar os dados usados nas diferentes camadas, em especial aes(), stat(), facet() e scale() Camada 2 - mapeamento: atributos estéticos, aes(), define quais colunas serão associadas com qual eixo e determina o tamanho, forma, cor, preenchimento e transparência dos atributos estéticos Camada 3 - transformações estatísticas, stat(), modificam, quando necessário, os dados que serão incluídos no gráfico (ex. calculando a média por grupo) Camada 4 - definição da geometria, geom(): define o tipo de geometria que é plotada no gráfico, como pontos, boxplots, violino, linhas, polígonos, entre outros Camada 5 - sistema de coordenadas (coordinate function): define o sistema de ccordenadas do gráfico e como o eixo Y e X se relacionam (padrão é o sistema cartesiano). Camada 6 - facetas: especifica como a visualização dos elementos aes() são divididos em diferentes janelas gráficas Camada 7 - escala: permite o controle das características visuais (cor, forma e tamanho) dos elementos declarados em aes() Camada 8 - temas: controla a aparência visual dos elementos do gráfico Figura 6.1: Esquema gráfico ilustrando as camadas que definem a strutura de organização aditiva da gramática dos gráficos (ggplot2). No exemplo, a partir de uma banco de dados, o mapeamento de quais colunas representam o eixo Y e X e de um atributo gráfico (pontos) é possível construir um gráfico de dispersão que ilustra a relação quantitativa entre a variável Y e X. Em resumo, o mapeamento gráfico do ggplot2 segue a seguinte estrutura: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;( mapping = aes(&lt;MAPPINGS&gt;), stat = &lt;STAT&gt;, position = &lt;POSITION&gt; ) + &lt;COORDINATE_FUNCTION&gt; + &lt;FACET_FUNCTION&gt; + &lt;SCALE_FUNCTION&gt; + &lt;THEME_FUNCTION&gt; 6.5 4. Tipos de gráficos Nesta seção, listamos os principais gráficos, e uma descrição de quantas colunas e o tipo de variável que eles representam. Histograma (do inglês histogram): distribuição de frequência de uma coluna para dados contínuos (cores diferentes podem representar espécies, populações ou grupos distintos) Gráfico de densidade (density plot): distribuição da densidade de uma coluna para dados contínuos (assim como no histograma, cores diferentes podem ser utilizadas para representar espécies, populações ou grupos distintos) Gráfico de dispersão (scatter plot) e gráfico de linha: relação entre valores de duas colunas para dados contínuos (X e Y) Diagrama de pontos (dot plot): distribuição da quantidade de valores agrupados de uma coluna para dados contínuos Gráfico de setores (pie chart e donut chart): representação da quantidade de valores de uma coluna para dados categóricos, geralmente em proporção ou porcentagem Gráfico de barras (bar plot): representação da quantidade de valores de uma ou mais colunas para dados categóricos Gráfico de caixa (box plot e violin plot): distribuição de valores contínuos de uma coluna (Y) para dois ou mais fatores categóricos de outra coluna (X) no formato de caixas e também no formato de violinos (considerando a variação) Gráfico pareado (pairs plot): relação entre valores de duas colunas para dados contínuos (X e Y), para colunas par a par Para facilitar a compreensão das regras da gramática dos dados, cada tipo de gráfico segue a mesma estrutura de organização, que respeita as camadas de informação descritas anteriormente. O leitor vai perceber, portanto, que algumas camadas não são necessárias dependendo do tipo de gráfico e do conjunto de dados que pretende analisar. Nos exemplos, a versão padrão se refere à representação determinada no default da função. Deste modo, somente informamos as variáveis que serão utilizadas dentro de cada camada e a forma geométrica (i.e., tipo de gráfico) desejada. Porém, para cada tipo gráfico apresentamos funções e argumentos para ajustes finos e personalizados. 6.5.1 4.1. Histograma (histogram) O histograma é um gráfico extremamente popular e bastante útil para visualizar a distribuição de variáveis contínuas. É bem provável que você já tenha visto um histograma quando aprendeu pela primeira vez a famosa distribuição normal. # histograma de uma variavel continua dist_normal &lt;- data.frame(x = rnorm(50000, mean = 100, sd = 5)) ggplot(data = dist_normal, aes(x = x)) + geom_histogram() Neste histograma é possível entender que a maioria dos valores da variável x no data.frame dist_normal estão próximos ao valor da média, i.e., 100. Em ecologia, os histogramas são utilizados para visualizar, por exemplo, a variação morfológica entre espécies (subespécies, gênero, famílias, etc.), variação de parâmetros populacionais entre diferentes espécies ou dentro da mesma espécies em diferentes localidades. 6.5.1.1 4.1.1. Versão padrão Vamos utilizar o conjunto de dados palmerpenguins para construir um histograma da distribuição da variável flipper_length_mm com a função geom_hitogram(). Esta função utiliza uma variável contínua no eixo x e a frequência de cada categoria no eixo y. O gráfico a seguir representa a frequência de uma variável (neste caso, a medida de todos os pinguins, independente da espécie). ggplot(data = penguins, aes(x = flipper_length_mm)) + geom_histogram() 6.5.1.2 4.1.2. Definindo o número de classes Vamos utilizar o argumento bins para definir em quantas classes a variável x deve ser dividida. # histograma com 10 classes ggplot(data = penguins, aes(x = flipper_length_mm)) + geom_histogram(bins = 10) + labs(title = &quot;10 classes&quot;) # histograma com 30 classes ggplot(data = penguins, aes(x = flipper_length_mm)) + geom_histogram(bins = 30) + labs(title = &quot;30 classes&quot;) 6.5.1.3 4.1.3. Comparando múltiplas categorias Se quisermos comparar a distribuição de uma variável contínua entre diferentes categorias, podemos utilizar o argumento fill para colorir o gráfico. No exemplo abaixo, utilizamos cores diferentes para ilustrar a distribuição da variável x entre espécies diferentes (fill = species). # histograma com cores para diferentes categorias com sobreposicao ggplot(data = penguins, aes(x = flipper_length_mm, fill = species)) + geom_histogram(alpha = .5) + ggtitle(&quot;Com sobreposiçao&quot;) # Histograma com cores para diferentes categorias sem sobreposição ggplot(data = penguins, aes(x = flipper_length_mm, fill = species)) + geom_histogram(position = &quot;dodge&quot;) + ggtitle(&quot;Sem sobreposiçao&quot;) 6.5.1.4 4.1.4. Ajustes finos (versão personalizada) # Histogram example: flipper length by species penguins %&gt;% ggplot(aes(x = flipper_length_mm, fill = species)) + geom_histogram(alpha = .5, position = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(x = &quot;Comprimento da nadadeira (mm)&quot;, y = &quot;Frequência (%)&quot;, fill = &quot;Espécies&quot;) 6.5.1.5 4.1.5. Principais camadas utilizadas na função geom_histogram() aes(): Eixo X: variável contínua (flipper_length_mm) Preenchimento (fill): variável categórica (species) que define as cores tendo como base o número de níveis dentro desta categoria geom(): geom_histogram() Transparência dos pontos (alpha): 0,5 (varia de 0, trasparência máxima, a 1, sem trasparência) Posição das barras: o argumento position define se as barras devem ser inseridas de maneira sobreposta (position = \"identity\") ou não (position = \"dodge\") scale():scale_fill_manual() para definir manualmente as cores de preferência do usuário theme(): theme_bw()para selecionar o tema com fundo branco e labs() para personalizar o títulos dos eixos X e Y. 6.5.2 4.2 Gráfico de densidade (density plot) Nesta seção iremos aprender a criar um gráfico de densidade no R utilizando o ggplot2. Assim como o histograma, o gráfico de densidade é utilizado para visualizar a distribuição de uma variável contínua em intervalos. Esse gráfico é uma variação do Histograma (ver seção ??) que utiliza Kernel Smoother e, além de ser muito útil para visualizar distribuições, pode ser usado para testar várias hipóteses ecológicas, como descrito no Capítulo 14 (Diversidade Funcional). 6.5.2.1 4.2.1.Versão padrão Vamos utilizar o conjunto de dados palmerpenguins, para plotar a distribuição da variável flipper_length_mm em um Gráfico de densidade. Utilizaremos a função geom_density() para plotar uma variável no eixo x. ggplot(data = penguins, aes(x = flipper_length_mm)) + geom_density() Além da versão de densidade em linha, é possível utilizar o argumento fill para definir a cor de preenchimento do gráfico e o argumento alpha para definir a transparência do preenchimento. Utilizamos ainda o argumento color para definir a cor da linha. # Argumento fill ggplot(data = penguins, aes(x = flipper_length_mm)) + geom_density(fill = &quot;tomato&quot;) # Argumento fill, color e alpha ggplot(data = penguins, aes(x = flipper_length_mm)) + geom_density(fill = &quot;steelblue&quot;, color = &quot;black&quot;, alpha = .5) 6.5.2.2 4.2.2. Comparando múltiplas categorias Em algumas situações, queremos comparar a distribuição de uma variável contínua entre diferentes categorias. Dessa forma, podemos utilizar o argumento fill para colorir o gráfico. No exemplo abaixo, utilizamos cores diferentes para ilustrar a distribuição da variável x entre espécies diferentes (fill = species). # O argumento fill preenche cada nível da coluna &quot;species&quot; (sem transparência: alpha = 1) ggplot(data = penguins, aes(x = flipper_length_mm, fill = species)) + geom_density() + labs(title = &quot;Sem transparência&quot;) # Gráfico de densidade com cores para diferentes categorias com sobreposicao ggplot(data = penguins, aes(x = flipper_length_mm, fill = species)) + geom_density(alpha = .5) + labs(title = &quot;Com transparência&quot;) 6.5.2.3 4.2.3. Ajustes finos (versão personalizada) ggplot(data = penguins, aes(x = flipper_length_mm, fill = species)) + geom_density(alpha = .5) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) + scale_x_continuous(breaks = seq(from = 160, to = 240, by = 10), limits = c(160, 240)) + scale_y_continuous(breaks = seq(from = 0, to = .07, by = .01)) + theme_bw(base_size = 16) + labs(x = &quot;Comprimento da nadadeira (mm)&quot;, y = &quot;Frequência&quot;, fill = &quot;Espécie&quot;) 6.5.2.4 4.2.4. Principais camadas utilizadas na função geom_density() aes(): Eixo X: variável contínua (flipper_length_mm) Preenchimento (fill): variável categórica (species) que define as cores tendo como base o número de níveis dentro desta categoria geom(): geom_density() Transparência dos pontos (alpha): 0,5 (varia de 0, trasparência máxima, a 1, sem trasparência) Posição das barras: o argumento position() define se as barras devem ser inseridas de maneira sobreposta (position = \"identity\") ou não (position = \"dodge\") scale(): scale_fill_manual() para definir manualmente as cores de preferência do usuário scale_x_continuous() e scale_y_continuous() determinam os limites (valor mínimo e máximo) para os dois eixos e, além disso, os intervalos entre os valores (breaks) theme(): theme_bw()para selecionar o tema com fundo branco e labs() para personalizar o títulos dos eixos X e Y, e da legenda. 6.5.3 4.3. Diagrama de pontos (dot plot) Uma alternativa ao gráfico de densidade e histograma é o diagrama de pontos (Dot plot), apesar de ser relativamente menos usado em ecologia. 6.5.3.1 4.3.1. Versão padrão Vamos utilizar o conjunto de dados palmerpenguins para visualizar a distribuição da variável flipper_length_mm com o diagrama de pontos com a função geom_dotplot(). ggplot(data = penguins, aes(x = flipper_length_mm)) + geom_dotplot() 6.5.3.2 4.3.2. Comparando múltiplas categorias Assim como nas funções geom_histogram() e geom_density(), é possível comparar categorias na função geom_dotplot() utilizando o argumento fill, bem como os argumentos color, alpha e dotsize. ggplot(data = penguins, aes(x = flipper_length_mm, fill = species)) + geom_dotplot(dotsize=1) ggplot(data = penguins, aes(x = flipper_length_mm, fill = species)) + geom_dotplot(dotsize=0.7, color = &quot;black&quot;, alpha = 0.5) 6.5.3.3 4.3.3. Ajustes finos (versão personalizada) ggplot(data = penguins, aes(x = flipper_length_mm, fill = species)) + geom_dotplot(color = &quot;black&quot;, alpha = .7) + theme_bw(base_size = 16) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) + scale_x_continuous(breaks = seq(from = 170, to = 240, by = 10), limits = c(170, 240)) + scale_y_continuous(breaks = seq(from = 0, to = 1.4, by = .2), limits = c(0, 1.4)) + labs(x = &quot;Comprimento da nadadeira (mm)&quot;, y = &quot;Frequência&quot;, fill = &quot;Espécies&quot;) Uma das limitações do dotplot é que a sobreposição dos pontos não permite a visualização apropriada desses valores sobrepostos entre diferentes grupos comparados. 6.5.3.4 4.3.4. Principais camadas utilizadas na função geom_dotplot() aes(): Eixo X: variável contínua (flipper_length_mm) Preenchimento (fill): variável categórica (species) que define as cores tendo como base o número de níveis dentro desta categoria geom(): geom_dotplot() Transparência dos pontos (alpha): 0,5 (varia de 0, trasparência máxima, a 1, sem trasparência) Cor da linha do ponto (color): valor padrão (se não for especificado) é black Tamanho dos pontos (dotsize): valor padrão (se não for especificado) é 1 Posição dos pontos: o argumento position define se as barras devem ser inseridas de maneira sobreposta (position = \"identity\") ou não (position = \"dodge\") scale(): scale_fill_manual() para definir manualmente as cores de preferência do usuário scale_x_continuous() e scale_y_continuous() determinam os limites (valor mínimo e máximo) para os dois eixos e, além disso, os intervalos entre os valores (breaks) theme(): theme_bw()para selecionar o tema com fundo branco e labs() para personalizar o títulos dos eixos X e Y, e da legenda. 6.5.4 4.4. Gráfico de barras (bar plot) O gráfico de barras é um dos mais usados em artigos e livros da ecologia, uma vez que permite comparar valores absolutos ou médios (combinados com alguma medida de variação como desvio padrão) de uma variável continua entre diferentes níveis de uma variável categórica. 6.5.4.1 4.4.1. Versão padrão O gráfico de barras utiliza retângulos para representar uma variável contínua ou a contagem de uma variável categórica, sendo que o comprimeno dos retângulos é proporcional ao valor que ele representa. Por exemplo, é possível comparar qual a quantidade de indivíduos medidos para cada espécie de pinguim. # Número de indivíduos coletados penguins_count &lt;- penguins %&gt;% dplyr::count(species) # grafico de barras ggplot(data = penguins_count, aes(x = species, y = n)) + geom_bar(stat = &quot;identity&quot;) Além disso, é possível alterar as cores (color) e preenchimento (fill) das barras, bem como sua transparência (alpha) e largura (width), como demonstrado nos próximos quatro gráficos.1 # modificando preenchimento ggplot(data = penguins_count, aes(x = species, y = n)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) # Modificando cor e preenchimento ggplot(data = penguins_count, aes(x = species, y = n)) + geom_bar(stat = &quot;identity&quot;, color = &quot;steelblue&quot;, fill = &quot;white&quot;) # Modificando a largura da barra = 0.75 ggplot(data = penguins_count, aes(x = species, y = n)) + geom_bar(stat = &quot;identity&quot;, width = 0.75) + labs(title = &quot;largura = 0.75&quot;) # Modificando a largura da barra = 0.25 ggplot(data = penguins_count, aes(x = species, y = n)) + geom_bar(stat = &quot;identity&quot;, width = 0.25) + labs(title = &quot;largura = 0.25&quot;) Outra possibilidade para representação do gráfico de barras é inverter a direção das barras com a função coord_flip(). # Barras vertical ggplot(data = penguins_count, aes(x = species, y = n)) + geom_bar(stat = &quot;identity&quot;, width = 0.6) # Barras horizontal ggplot(data = penguins_count, aes(x = species, y = n)) + geom_bar(stat = &quot;identity&quot;, width = 0.6) + coord_flip() É possível utilizar variáveis categóricas para definir cores e preenchimento e ilustrar, por exemplo, tratamentos ou espécies diferentes com os argumentos fill e color. # grafico de barras com preenchimento colorido ggplot(data = penguins_count, aes(x = species, y = n, fill = species)) + geom_bar(stat = &quot;identity&quot;) 6.5.4.2 4.4.2. Adicionando medidas de variação Em algumas comparações, utilizar somente os valores absolutos pode não ser a visualização mais apropriadas como, por exemplo, em desenho de ANOVA (Capítulo 7). Desse modo, ao invés do valor máximo da barra representar o valor absoluto (e.g., número de indivíduos de uma espécies), ele vai representar o valor médio. Além disso, linhas adicionais (chamadas barras de erro) vão representar alguma medida de variação como desvio padrão, erro padrão, intervalo de confiança, entre outros. A função Rmisc::summarySE() permite realizar esses cálculos de maneira simples, como demonstrado no exemplo abaixo. # Calculando média e desvio padrão por grupo penguins2 &lt;- penguins %&gt;% drop_na(flipper_length_mm) # remover valores ausentes na variável (NAs) penguins_mean &lt;- summarySE(penguins2, measurevar = &quot;flipper_length_mm&quot;, groupvars = &quot;species&quot;) head(penguins_mean) #&gt; species N flipper_length_mm sd se ci #&gt; 1 Adelie 151 189.9536 6.539457 0.5321735 1.051524 #&gt; 2 Chinstrap 68 195.8235 7.131894 0.8648692 1.726286 #&gt; 3 Gentoo 123 217.1870 6.484976 0.5847306 1.157533 # Gráfico de barras com desvio padrão ggplot(data = penguins_mean, aes(x = species, y = flipper_length_mm, fill = species)) + geom_bar(stat = &quot;identity&quot;, alpha = 0.4) + geom_errorbar(aes(ymin = flipper_length_mm - sd, ymax = flipper_length_mm + sd), width = 0.1) + geom_point() + labs(title = &quot;Barra de erro com desvio padrão&quot;) # Gráfico de barras com intervalo de confiânça ggplot(data = penguins_mean, aes(x = species, y = flipper_length_mm, fill = species)) + geom_bar(stat = &quot;identity&quot;, alpha = 0.4) + geom_errorbar(aes(ymin = flipper_length_mm - se, ymax = flipper_length_mm + se), width = 0.1) + geom_point() + labs(title = &quot;Barra de erro com erro padrão&quot;) 6.5.4.3 4.4.3. Ajustes finos (versão personalizada) ggplot(data = penguins_count, aes(x = species, y = n, fill = species)) + geom_bar(stat = &quot;identity&quot;) + geom_label(aes(label = n), fill = &quot;white&quot;) + theme_bw(base_size = 16) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + labs(x = &quot;Espécie&quot;, y = &quot;Número de indivíduos&quot;, fill = &quot;Espécie&quot;) 6.5.5 4.5. Gráfico de setores (pie chart e donut chart) Além do gráfico de barras, o gráfico de setores representa uma alternativa para comparar a proporção entre categorias. Tais gráficos podem ser representados como pie charts ou donut charts, como demonstrado abaixo. No exemplo abaixo, utilizamos a mesma comparação realizada no item 4.3.3 acima. Porém, os valores de contagem (número de indivíduos por espécie) devem ser transformados previamente em proporção. 6.5.5.1 4.5.1. Gráfico de setores (pie chart) # Cálculo da proporção penguins_prop &lt;- penguins %&gt;% dplyr::count(species) %&gt;% dplyr::mutate(prop = round(n/sum(n), 4)*100) # Pie chart ggplot(data = penguins_prop, aes(x = &quot;&quot;, y = prop, fill = species)) + geom_bar(stat = &quot;identity&quot;, color = &quot;white&quot;) + coord_polar(&quot;y&quot;, start = 0) + geom_text(aes(label = paste0(prop, &quot;%&quot;)), color = &quot;white&quot;, position = position_stack(vjust = 0.5), size = 8) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_void() + labs(fill = &quot;Espécie&quot;) 6.5.5.2 4.5.2. Gráfico de setores (donut chart) ggplot(data = penguins_prop, aes(x = 2, y = prop, fill = species)) + geom_bar(stat = &quot;identity&quot;) + coord_polar(theta = &quot;y&quot;, start = 0) + geom_text(aes(label = paste0(prop, &quot;%&quot;)), color = &quot;white&quot;, position = position_stack(vjust = .5), size = 5) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + xlim(0, 2.5) + theme_void() + theme(legend.position = c(.5, .5), legend.title = element_text(size = 20), legend.text = element_text(size = 15)) + labs(fill = &quot;Espécie&quot;) 6.5.5.3 4.5.3. Comparando gráficos de setores com gráfico de barras O mesmo conjunto de dados pode ser visualizado de diferentes formas. Não diferente, a comparação da proporção de ocorrências de diferentes categorias pode ser feita de várias maneiras. Abaixo, fizemos a comparação da proporção de indivíduos por cada uma das três espécies dos dados penguins. ggplot(data = penguins_prop, aes(x = &quot;&quot;, y = prop, fill = species)) + geom_bar(stat = &quot;identity&quot;, color = &quot;white&quot;) + coord_polar(&quot;y&quot;, start = 0) + geom_text(aes(label = paste0(prop, &quot;%&quot;)), color = &quot;white&quot;, position = position_stack(vjust = 0.5), size = 3) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_void() + labs(title = &quot;Pie chart&quot;, fill = &quot;Espécies&quot;) -&gt; g_pie ggplot(data = penguins_prop, aes(x = 2, y = prop, fill = species)) + geom_bar(stat = &quot;identity&quot;) + coord_polar(theta = &quot;y&quot;, start = 0) + geom_text(aes(label = paste0(prop, &quot;%&quot;)), color = &quot;white&quot;, position = position_stack(vjust = .5), size =3) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + xlim(0, 2.5) + theme_void() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Donut chart&quot;, fill = &quot;Espécies&quot;) -&gt; g_donut ggplot(data = penguins_prop, aes(x = species, y = prop, fill = species)) + geom_bar(stat = &quot;identity&quot;) + geom_label(aes(label = prop), fill = &quot;white&quot;) + theme_bw() + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + labs(title = &quot;Gráfico de Barras (Horizonal)&quot;, x = &quot;Espécies&quot;, y = &quot;Número de indivíduos&quot;, fill = &quot;Espécies&quot;)+ theme(legend.position = &quot;none&quot;)-&gt; g_bar_h ggplot(data = penguins_prop, aes(x = species, y = prop, fill = species)) + geom_bar(stat = &quot;identity&quot;) + geom_label(aes(label = prop), fill = &quot;white&quot;) + theme_bw() + coord_flip()+ scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + labs(title = &quot;Gráfico de Barras (Vertical)&quot;, x = &quot;Espécies&quot;, y = &quot;Número de indivíduos&quot;, fill = &quot;Espécies&quot;) + theme(legend.position = &quot;none&quot;)-&gt; g_bar_v grid.arrange(g_pie, g_donut, g_bar_h, g_bar_v, nrow=2) 6.5.5.4 4.5.4. Principais camadas utilizadas no gráfico de barras e de setores: geom_bar() aes(): Eixo X: variável categórica (species) Eixo Y: variável contínua (flipper_length_mm) Preenchimento (fill): a variável categórica (species) define a cor do preenchimento e os níveis dentro desta categoria determinam o número de cores que devem ser indicadas no scale_fill_manual(). geom(): geom_bar() Transparência das barras (alpha): 0,4 (varia de 0, trasparência máxima, a 1, sem trasparência) stat: é necessário usar o argumento identity quando os valores do eixo Y são adicionados pelo usuário geom_label() forma geométrica que adiciona rótulo dos valores absolutos das barras por categoria (species) geom_errorbar() ymine ymaxdelimitam os valores mínimos e máximos, respectivamente, das barras de erro. Tais valores são representados pelo valor da média menos (no caso do ymin) ou mais (no caso do ymax) o valor do intervalo de confiança, desvio ou erro padrão. coord_polar(): sistema de coordenadas para gerar barras circulares sobrepostas (stacked) que são usadas nos gráficos de setores (pie chart e donut chart) o argumento start = 0 indica o local de início do gráfico que, neste caso, começa na hora 0 em um relógio de 12 horas. scale(): scale_fill_manual() para definir manualmente as cores de preferência do usuário theme(): theme_bw()para selecionar o tema com fundo branco e labs() para personalizar o títulos dos eixos X e Y, e da legenda. 6.5.6 4.6. Gráfico de caixa (boxplot) O boxplot, conhecido amplamente nos artigos e livros de ecologia, é uma visualização gráfica que sintetiza informações importantes de dados contínuos como mediana e variação (quartil 1-3, ver Figura 2). Figura 6.2: Estrutura e elementos do boxplot 6.5.6.1 4.6.1. Versão padrão Vamos plotar uma variável contínua (flipper_length_mm) no eixo y em função de uma variável categórica no eixo x (species). A definição de qual coluna do banco de dados é a x e qual é a y é feita dentro do comendo aes(). ggplot(penguins, aes(y = flipper_length_mm, x = species)) + geom_boxplot() É possível destacar os pontos referentes aos outliers (se houver) com o argumento outlier.color. Caso tenha interesse, é possível também remover os outliers do gráfico. ggplot(penguins, aes(y = flipper_length_mm, x = species)) + geom_boxplot(outlier.color = &quot;red&quot;)+ labs(title = &quot;outliers vermelhos&quot;) ggplot(penguins, aes(y = flipper_length_mm, x = species)) + geom_boxplot(outlier.shape = NA)+ labs(title = &quot;outliers removidos&quot;) Outra alternativa para os gráficos do tipo boxplot é utilizar o argumento notch = TRUE para produzir diagramas de caixa entalhados (notched). Estes diagramas são úteis para inferir de forma aproximada se exite diferença significativa entre as medias dos grupos. ggplot(penguins, aes(y = flipper_length_mm, x = species)) + geom_boxplot(notch = TRUE) 6.5.6.2 4.6.2. Comparando múltiplas categorias No exemplo abaixo, utilizamos cores diferentes para ilustrar espécies diferentes através do argumento fill = species. ggplot(penguins, aes(y = flipper_length_mm, x = species, fill = species)) + geom_boxplot() 6.5.6.3 4.6.3. Combinando boxplot com pontos (jitter) Podemos ainda acrescentar pontos para mostrar a distribuição dos dados. # boxplot com jitters ggplot(penguins, aes(y = flipper_length_mm, x = species, fill = species)) + geom_boxplot() + geom_jitter(size = .6, width = .2) 6.5.6.4 4.6.4. Gráfico de violino (violin plot) como alternativa ao boxplot Além das caixas, podemos utilizar o formato de violino para representar a variação de dados contínuos entre categorias. A informação adicional ao boxplot que o gráfico de violino permite visualizar é a densidade dos pontos, assim como apresentamos acima no gráfico de densidades geom_density(). A diferença é que a densidade é espelhada e, desse modo, podemos visualizar os intervalores dos dados com maior ou menor concentração de valores. # violino com jitters ggplot(penguins, aes(y = flipper_length_mm, x = species, fill = species)) + geom_violin() + geom_jitter(size = .6, width = .2) É possível também combinar boxplot e gráfico de violino em um único gráfico. # violino com boxplot ggplot(penguins, aes(y = flipper_length_mm, x = species, fill = species)) + geom_violin() + geom_boxplot(width = 0.1, fill = &quot;grey&quot;) 6.5.6.5 4.6.5. Ajustes finos (versão personalizada) ### geom_boxplot() ggplot(data = penguins, aes(x = species, y = flipper_length_mm, fill = species)) + geom_boxplot(width = .3, show.legend = FALSE) + geom_jitter(alpha = .5, show.legend = FALSE, position = position_jitter(width = .1, seed = 0)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(x = &quot;Species&quot;, y = &quot;Flipper length (mm)&quot;) # geom_violin() ggplot(data = penguins, aes(x = species, y = flipper_length_mm, fill = species)) + geom_violin(width = .3, show.legend = FALSE) + geom_point(alpha = .5, show.legend = FALSE) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(title = &quot;Pontos sem jitter&quot;, x = &quot;Species&quot;, y = &quot;Flipper length (mm)&quot;) # geom_violin() ggplot(data = penguins, aes(x = species, y = flipper_length_mm, fill = species)) + geom_violin(width = .3, show.legend = FALSE) + geom_jitter(alpha = .5, show.legend = FALSE, position = position_jitter(width = .1, seed = 0)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(title = &quot;Pontos com jitter&quot;,, x = &quot;Species&quot;, y = &quot;Flipper length (mm)&quot;) 6.5.6.6 4.6.6. Principais camadas utilizadas no geom_boxplot()e geom_violin() aes(): Eixo X: variável categórica (species) Eixo Y: variável contínua (flipper_length_mm) Preenchimento (fill): a variável categórica (species) define a cor do preenchimento e os níveis dentro desta categoria determinam o número de cores que devem ser indicadas no scale_fill_manual(). geom(): geom_boxplot() width: largura das barras (valor padrão: width = 1) fill: pode definir uma cor padrão (caso não tenha utilizado o fill dentro do argumento aes()) como fill = \"grey\" notch: a escolha padrão da função geom_boxplot() é notch = FALSE; para utilizar a caixa entalhada o argumento deve ser notch = TRUE geom_violin() assim como nas outras formas geométricas, é possível controlar largura, cor, preenchimento e transparências dos violinos geom_jitter() esta função basicamente agita aleatóriamente os pontos para evitar a sobreposição de valores idênticos. Esta função produz a mesma representação se usar a função geom_point(position = \"jitter\") scale(): scale_fill_manual() para definir manualmente as cores de preferência do usuário theme(): theme_bw()para selecionar o tema com fundo branco e labs() para personalizar o títulos dos eixos X e Y, e da legenda. 6.6 4.7. Gráfico de dispersão (scatter plot) O gráfico de dispersão (em ingl~es, scatterplot) é famoso na ecologia por ser a visualização preferida para prepresentar a relação entre área e riqueza de espécies. Neste gráfico, os eixos X e Y são representados por variáveis contínuas. Em especial, os gráficos de dispersão são usados para representar os resultados testados por análises estatísticas como regressão linear, ancova, mantel, PCA, PCoA, entre outros (Capítulos 7-14, ). 6.6.1 4.7.1. Versão padrão ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() 6.6.2 4.7.2. Definindo a cor, tamanho, forma e preenchimento dos pontos # Cor e tamanho dos pontos ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point(color = &quot;royalblue&quot;, size = 3)+ labs(title = &quot;Sem transparência&quot;) # Cor e tamanho dos pontos ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point(color = &quot;red&quot;, size = 4, alpha = 0.5)+ labs(title = &quot;Com transparência&quot;) A forma dos pontos permite dois controles importantes: a forma em si (símbolos como círculo, quadrado, etc.) e a possibilidade de preenchimento da forma. A figura a seguir discrimina esses símbolos e o valor que deve ser utilizado para desenhar a forma preferida. É importante notar que os símbolos 21 a 25 possuem dois argumentos: (i) cor (que, na verdade, é a cor da linha do símbolo) e (ii) fill (cor que define o preenchimento do símbolo). O tipo de símbolo é definido pelo argumento shape. Figura 6.3: Figura 3. Tipos de símbolos disponíveis. Assim, é possível controlar cores, formas e preenchimento combinado os argumentos shape, fille colorcom a função scale_manual(). É importante notar que para os símbolos entre 15 e 20 só podemos controlar o argumento cor, enquanto os símbolos entre 21 e 25 podemos controlar a cor e o preenchimento. # shape = 1 e size = 2 ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point(shape = 1, size = 2) # shape = 19 (símbolo padrão da função) e size = 3 ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + geom_point(shape = 19, size = 3) # shape = 21 e size = 4 ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, fill = species)) + geom_point(shape = 21, size = 4, color = &quot;black&quot;) 6.6.3 4.7.3. Definindo linhas de ajuste Quando usamos modelos estatísticos como, por exemplo, lm(), glm(), gam(), entre outros, podemos utilizar os valores preditos para demonstrar a relação entre as variáveis X e Y. No ggplot2 a função geom_smooth() faz esse ajuste com certa simplicidade. Além disso, incluir a cor da espécie dentro do aes() essa informação é herdada para as próximas camadas. Neste caso, uma regressão linear é plotada para o subconjunto de dados que representa cada espécie. # shape = 21 e size = 4 ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + geom_point(size = 4, alpha = .5)+ geom_smooth(method= lm) 6.6.4 4.7.4. Ajustes finos (versão personalizada) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;) Além disso, podemos relacionar dados não tão usuais. Recomendamos a leitura do artigo de Matejka &amp; Fitzmaurice (2017) que apresenta as armadilhas típicas que dados podem gerar quando evitamos de visualizá-los previmamente. # data + plot datasaurus_dozen %&gt;% dplyr::filter(dataset == &quot;dino&quot;) %&gt;% ggplot() + aes(x = x, y = y) + geom_point(colour = &quot;black&quot;, fill = &quot;black&quot;, size = 5, alpha = .75, pch = 21) + theme_bw() + theme(axis.title = element_text(size = 24), axis.text.x = element_text(size = 20), axis.text.y = element_text(size = 20)) 6.7 4.8. Visualização de múltiplos gráficos pareados Muitas vezes precisamos compreender a correlação entre múltiplas variáveis, sendo comuum que essas variáveis sejam de mais de um tipo (contínua, categórica, etc). A solução mais indicada para termos uma visão geral do conjunto de dados e de suas interrelações é o gráfico generalizado pareado (Emerson et al. 2013). 6.7.1 4.8.1. Gráfico pareado com variáveis contínuas A função ggpairs()do pacote GGally permite criar múltiplos gráficos pareados comparando as variáveis contínuas no seu conjunto de dados. Além de plotar gráficos de dispersão de cada par de variáveis, ela apresenta gráficos de densidade de cada variável individualmente e, além disso, os valores de correlação entre os pares analisados com ou sem uma potencial variável categórica (neste caso, species) penguins %&gt;% dplyr::select(body_mass_g, ends_with(&quot;_mm&quot;)) %&gt;% GGally::ggpairs(aes(color = penguins$species)) + scale_colour_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw() 6.7.2 4.8.2. Gráfico pareado com vários tipos de variáveis Como alternativa, a função ggpairs() permite também incluir variáveis categóricas nas comparações. Neste caso, ela reconhece o tipo de gráfico (boxplot, dispersão, etc) a partir da classe das variáveis. penguins %&gt;% dplyr::select(species, sex, body_mass_g, ends_with(&quot;_mm&quot;)) %&gt;% GGally::ggpairs(aes(color = species)) + scale_colour_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw() 6.8 5. Erros comuns dos usuários do ggplot2 e como evitá-los Abaixo, apresentamos uma lista não exaustiva dos erros mais comuns que cometemos (e vimos muitos usuários cometerem) ao fazer gráficos no ggplot2: Utilizar ajuste manual nas funções scale_shape_manual(), scale_color_manual() ou scale_fill_manual() sem indicar no argumento aes() as variáveis que devem definir cada um desses elementos gráficos. Definir a cor ou preenchimento de um geom dentro do aes() global (ggplot(aes(color = \"black¨)) quando no fundo essa definição deveria ser dento do geom (geom_point(color = \"black\"). Utilizar ajuste manual na função scale_size_manual() indicando uma variável categórica ao invés de numérica. Número de cores indicadas como valores no scale_fill_manual() ou scale_color_manual(): ao definir as cores de maneira personalizada (ou seja, não usando o padrão da função) é muito comum utilizarmos o número de cores usados por algum tutorial ou livro. Com frequência, o exemplo seguido e seus dados não possuem o mesmo número de cores. Deste modo, você pode usar comando no R para ajudar a quantificar o número de cores necessárias. Por exemplo, para os dados penguins, o comando a seguir indica o número de cores necessárias: length(levels(penguins$species)). Assim, será necessário indicar três cores diferentes dentro da função scale_(). Função geom_smooth(): como falado acima, a função geom_smooth() é muito útil (e simples) para gerar as linhas de ajuste (best fit) típicas de modelos lineares e não lineares. Porém, fique alerta que ao usar, por exemplo, geom_smooth(method = lm), o modelo linear utilizado para testar sua predição foi o lm(). Se tiver utilizado glm()ou gam() o ajuste deve ser produzido a partir desses modelos. Uso incorreto da classe das variáveis: neste caso, o usuário utilizar uma variável numérica (por exemplo, 1, 2 e 3) como variável categórica. Neste caso, é preciso transformar a variável numérica em variável categóricas (antes de fazer o ggplot2 ou dentro do aes()). Veja exemplos abaixo: penguins %&gt;% ggplot(aes(x = year, y = bill_length_mm))+ geom_boxplot() + theme_bw()+ labs(title = &quot;Figura incorreta&quot;) penguins %&gt;% ggplot(aes(x = factor(year), y = bill_length_mm))+ geom_boxplot() + theme_bw()+ labs(title = &quot;Figura correta com transformação interna&quot;) penguins %&gt;% mutate(year_f = as.factor(year)) %&gt;% ggplot(aes(x = year_f, y = bill_length_mm))+ geom_boxplot() + theme_bw()+ labs(title = &quot;Figura correta com transformação prévia&quot;) 6.9 6. Finalização de gráficos para publicação 6.9.1 6.1. Posição, cores e fonte da legenda É possível controlar a posição, cores e fonte da legenda em diversos locais com alguns argumentos dentro da função theme(): legend.positioncontrola a posição na área do gráfico: top, right, bottom, left ou none. Além disso, é possível inserir a legenda internamente no gráfico indicando as posições nos eixos X e Y legend.boxdetermina as caracteríscas do retângulo onde a legenda é inserida: legend.box.background (combinado com element_rect()) e legend.box.margin (combinado com margin()) legend.text controla a cor e tamanho da legenda (as duas informações devem ser inseridas dentro da função element_text()) legend.title personaliza a cor e tamanho da legenda também dentro da função element_text() ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(title = &quot;Legenda acima do gráfico&quot;, x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;) + theme(legend.position = &quot;top&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(title = &quot;Legenda abaixo do gráfico&quot;, x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;)+ theme(legend.position = &quot;bottom&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(title = &quot;Sem legenda&quot;, x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;)+ theme(legend.position = &quot;none&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(title = &quot;Legenda personalizada&quot;, x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;)+ theme(legend.position = &quot;right&quot;, legend.text = element_text(size = 14, colour = &quot;red&quot;), legend.title = element_text(face = &quot;bold&quot;), legend.box.background = element_rect(color=&quot;red&quot;, size=2), legend.margin = margin(6, 6, 6, 6)) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(title = &quot;Legenda interna&quot;, x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;)+ theme(legend.position = c(0.1, 0.1), legend.title = element_blank(), legend.key = element_blank(), legend.background = element_blank(), legend.text = element_text(size = 12, face = &quot;bold&quot;)) 6.9.2 6.2. Elementos gráficos: eixo, fonte, grid O gráfico padronizado (sem edição extra) geralmente não traz elementos mínimos para publicação em revistas, livros e periódicos. Além do controle da posição, cor e tamanho da legenda, é fundamental personalizar os seguintes elementos: eixo, fonte e grid. Eixos Variação: define limites mínimos e máximos para os eixos X (xlim()) e Y (ylim()) Intervalo: define o valor intervalo entre os números dos eixos X e Y Escala ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 4, alpha = 0.5) + ylim(0, 22) + xlim(0, 60) + labs(x = &quot;Eixo X&quot;, y = &quot;Eixo Y&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 4, alpha = 0.5) + scale_x_continuous(limits = c(20, 60), breaks = seq(20, 60, 2))+ labs(x = &quot;Eixo X&quot;, y = &quot;Eixo Y&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 4, alpha = 0.5) + scale_x_continuous(limits = c(20, 60), breaks = seq(20, 60, 10))+ labs(x = &quot;Eixo X&quot;, y = &quot;Eixo Y&quot;) Fonte dos eixos X e Y Tipo Tamanho Cor Face (itálico, negrito, etc.) Ângulo ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 4, alpha = 0.5) + theme(axis.title.x = element_text(face = &quot;bold&quot;, size = 20, colour = &quot;royalblue&quot;), axis.text.x = element_text(size = 14), axis.title.y = element_text(face = &quot;bold&quot;, size = 20, colour = &quot;royalblue&quot;), axis.text.y = element_text(size = 14))+ labs(x = &quot;Eixo X&quot;, y = &quot;Eixo Y&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 4, alpha = 0.5) + scale_x_continuous(limits = c(20, 60), breaks = seq(20, 60, 2))+ theme(axis.title.x = element_text(face = &quot;bold&quot;, size = 20, colour = &quot;royalblue&quot;), axis.text.x = element_text(size = 14, angle = 45), axis.title.y = element_text(face = &quot;bold&quot;, size = 20, colour = &quot;royalblue&quot;), axis.text.y = element_text(size = 14))+ labs(x = &quot;Eixo X&quot;, y = &quot;Eixo Y&quot;) Grid Linhas de grade principais (panel.grid.major) Linhas de grade secundárias (panel.grid.minor) Borda do gráfico (panel.border) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 4, alpha = 0.5) + scale_x_continuous(limits = c(30, 60), breaks = seq(30, 60, 5))+ theme(axis.title.x = element_text(face = &quot;bold&quot;, size = 16), axis.text.x = element_text(size = 12), axis.title.y = element_text(face = &quot;bold&quot;, size = 16), axis.text.y = element_text(size = 12), panel.grid.minor = element_blank())+ labs(title=&quot;Linhas de grade principais&quot;, x = &quot;Eixo X&quot;, y = &quot;Eixo Y&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 4, alpha = 0.5) + scale_x_continuous(limits = c(30, 60), breaks = seq(30, 60, 5))+ theme(axis.title.x = element_text(face = &quot;bold&quot;, size = 16), axis.text.x = element_text(size = 12), axis.title.y = element_text(face = &quot;bold&quot;, size = 16), axis.text.y = element_text(size = 12), panel.grid.minor = element_blank(), panel.grid.major = element_blank())+ labs(x = &quot;Eixo X&quot;, y = &quot;Eixo Y&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 4, alpha = 0.5) + scale_x_continuous(limits = c(30, 60), breaks = seq(30, 60, 5))+ theme(axis.title.x = element_text(face = &quot;bold&quot;, size = 16), axis.text.x = element_text(size = 12), axis.title.y = element_text(face = &quot;bold&quot;, size = 16), axis.text.y = element_text(size = 12), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), panel.border = element_rect(size = 2, colour = &quot;black&quot;, fill = NA))+ labs(x = &quot;Eixo X&quot;, y = &quot;Eixo Y&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 4, alpha = 0.5) + scale_x_continuous(limits = c(30, 60), breaks = seq(30, 60, 5))+ theme(axis.title.x = element_text(face = &quot;bold&quot;, size = 16), axis.text.x = element_text(size = 12), axis.title.y = element_text(face = &quot;bold&quot;, size = 16), axis.text.y = element_text(size = 12), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.line = element_line(size = 1))+ labs(x = &quot;Eixo X&quot;, y = &quot;Eixo Y&quot;) 6.9.3 6.3. Temas personalizados ggtheme() Existem vários temas criados dentro do universo ggtheme() que podem facilitar Existem vários temas criados dentro do universo ggtheme() que podem facilitar a escolha de um modelo com ótima qualidade para publicação. Abaixo, demonstramos os modelos mais utilizados. ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_gray(base_size = 16) + labs(title = &quot;theme_gray()&quot;, x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_bw(base_size = 16) + labs(title = &quot;theme_bw()&quot;, x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + theme_classic(base_size = 16) + labs(title = &quot;theme_classic()&quot;, x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;) 6.9.4 6.4. Criando seu próprio theme_custom() Por fim, é possível criar um tema personalizado como uma função dentro do R. Assim, o usuário pode controlar todos os elementos gráficos em um único comando. O maior benefício de personalizar uma função é que não será necessários fazer os ajustes finos em todos os gráficos que tiver construindo, o que pode representar grande economia de tempo. Esse tipo de padronização é fundamental para que todos os gráficos de um artigo tenham consitência e harmonia estética. theme_book &lt;- function(){ # escolha uma fonte font &lt;- &quot;Times&quot; # digite names(pdfFonts()) no console do R para ver a lista theme( # Defina elementos do grid panel.grid.major = element_line(colour = &quot;#d3d3d3&quot;), panel.grid.minor = element_blank(), axis.ticks = element_blank(), panel.border = element_rect(colour = &quot;black&quot;, fill = NA, size = .5), # Defina elementos textuais plot.title = element_text( # título family = font, #set font family size = 20, #set font size face = &#39;bold&#39;, #bold typeface hjust = 0, #left align vjust = 2), #raise slightly plot.subtitle = element_text( #subtitle family = font, #font family size = 14), #font size plot.caption = element_text( #caption family = font, #font family size = 14, #font size hjust = 1), #right align axis.title = element_text( #axis titles family = font, #font family size = 14), #font size axis.text = element_text( #axis text family = font, #axis famuly size = 14), #font size axis.text.x = element_text( #margin for axis text margin=margin(5, b = 10)) #since the legend often requires manual tweaking #based on plot content, don&#39;t define it here ) } ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + labs(title = &quot;Tema personalizado&quot;, x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;) + theme_book() 6.9.5 6.5. Exportando dados com alta qualidade com a função ggsave() O último passo para construir gráficos com qualidade de publicação é exportar em um formato específico, como png, pdf ou svg (entre outros). A função ggsave() não só permite que você tenha o controle sobre o formato, mas também sobre a qualidade e tamanho desejados com os seguintes argumentos: width = largura do gráfico height = altura do gráfico units = a unidade (cm, mm) que do gráfico para definir largura e tamanho dpi = qualidade da imagem (padrão = 300) g1 &lt;- ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = species)) + geom_point(size = 3, alpha = 0.8) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_shape_manual(values = c(19, 15, 17))+ scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) + labs(x = &quot;Comprimento do bico (mm)&quot;, y = &quot;Profundidade do bico (mm)&quot;, color = &quot;Espécies&quot;, shape = &quot;Espécies&quot;) + theme(legend.position = c(0.1, 0.1), legend.title = element_blank(), legend.key = element_blank(), legend.background = element_blank())+ theme_book() g1 ggsave(&quot;g1.pdf&quot;, g1, width = 15, height = 15, dpi =300, units = &quot;cm&quot;) ggsave(&quot;g1.png&quot;, g1, width = 15, height = 15, dpi =300, units = &quot;cm&quot;) ggsave(&quot;g1.svg&quot;, g1, width = 15, height = 15, dpi =300, units = &quot;cm&quot;) 6.10 Check-list para garantir bons gráficos Os eixos (Y e X) estão nomeados correatmente? As unidades de ambos os eixos (Y e X) estão indicadas corretamente? O tamanho da fonte dos eixos está adequado? A escala e os intervalos dos eixos estão corretos? O gráfico está proporcional (sem distorções: achatado em algum dos eixo)? As cores utilizadas possuem uma lógica clara e agregam valor na compreensão e interpretação do gráfico? Pare por um minuto e avalie se a mensagem principal do gráfico está clara. 6.11 Para se aprofundar 6.11.1 Livros Chang W. 2018. R Graphics Cookbook. [http://www.cookbook-r.com/Graphs/] Healy K. 2019. Data Visualization: a practical introduction. Princeton University Press. [https://socviz.co/´] Kabacoff R. 2020. Data Visualization with R. [https://rkabacoff.github.io/datavis/] Rahlf T. 2019. Data Visualisation with R: 111 Examples. 2ed. Springer. [http://www.datavisualisation-r.com/] Sievert C. 2019. Interactive web-based data visualization with R, plotly, and shiny. Chapman &amp; Hall/CRC. [https://plotly-r.com/] Wickham H. 2016. ggplot2: elegant graphics for data analysis. Springer. [https://ggplot2-book.org/] Wilke C O. 2019. Fundamentals of Data Visualization. OReilly Media. [https://clauswilke.com/dataviz/] Wilkinson L, Wills D, Rope D, Norton A, Dubbs R. 2005. The Grammar of Graphics. Springer. 6.11.2 Links The R Graph Gallery From Data to Viz Data Viz Project Color Brewer2 "],["cap7.html", "Capítulo 7 Modelos lineares 7.1 Teste T (de Student) para duas amostras independentes 7.2 Teste T para amostras pareadas 7.3 Correlação de Pearson 7.4 Regressão Linear Simples 7.5 Regressão Linear Múltipla 7.6 Análises de Variância (ANOVA) 7.7 ANOVA de um fator 7.8 ANOVA com dois fatores ou ANOVA fatorial 7.9 ANOVA em blocos aleatorizados 7.10 Análise de covariância (ANCOVA)", " Capítulo 7 Modelos lineares Pré-requisitos do capítulo ## Pacotes library(ecodados) library(car) library(ggplot2) library(&quot;ggpubr&quot;) library(ggforce) library(lsmeans) library(lmtest) library(sjPlot) ## Dados necessários CRC_PN_macho &lt;- ecodados::teste_t_var_igual CRC_LP_femea &lt;- ecodados::teste_t_var_diferente Pareado &lt;- ecodados::teste_t_pareado correlacao_arbustos &lt;- ecodados::correlacao dados_regressao &lt;- ecodados::regressoes dados_regressao_mul &lt;- ecodados::regressoes dados_anova_simples &lt;- ecodados::anova_simples dados_dois_fatores &lt;- ecodados::anova_dois_fatores dados_dois_fatores_interacao &lt;- ecodados::anova_dois_fatores dados_dois_fatores_interacao2 &lt;- ecodados::anova_dois_fatores_interacao2 dados_bloco &lt;- ecodados::anova_bloco dados_ancova &lt;- ecodados::ancova  Importante Estatísticas frequentistas como as que serão abordadas neste capítulo são baseadas em testes estatísticos (e.g. F, t, 2, etc), que são resultados númericos do teste, e um valor de probabilidade (valor de P) que é associado com o teste estatístico (Nicholas J. Gotelli and Ellison 2012). O valor de P mede a probabilidade que os valores observados ou mais extremos seriam encontrados caso a hipótese nula seja verdadeira (veja @{cap3}). Ao longo do livro usaremos o critério convencional de rejeitar a hipótese nula quando P &lt; 0.05. Contudo, sugerimos a leitura destes artigos (White et al. 2013; Barber and Ogle 2014; Burnham and Anderson 2014b; Murtaugh 2014; Halsey 2019) que discutemas limitações e problemas associados ao valor de P . 7.1 Teste T (de Student) para duas amostras independentes Uma das perguntas mais comuns em estatística é saber se há diferença entre as médias de dois grupos ou tratamentos. Para responder a esta pergunta, William Sealy Gosset, químico da cervejaria Guinness em 1908, desenvolveu o Teste T que é uma estátistica que segue uma distribuição t de Student para rejeitar ou não uma hipótese nula de médias iguais entre os grupos. \\[ t = \\frac{(\\bar{X}_1 - \\bar{X}_2)}{\\sqrt{\\frac{2S^2_p}{n}}}\\] Onde: \\(\\bar{X}\\)1 - \\(\\bar{X}\\)2 = diferença entre as médias de duas amostras, S2p = desvio padrão das amostras, n = tamanho das amostras. Premissas do Teste t : As amostras devem ser independentes; As unidades amostrais são selecionadas aleatoriamente; Distribuição normal (gaussiana) dos resíduos. Observação: Zar (2010) indica que o Test T é robusto mesmo com moderada violação da normalidade, principalmente se o tamanho amostral for alto. Homogeneidade da variância. Observação: Caso as variâncias não sejam homogêneas, isso deve ser informado na linha de comando, pois o denominador da fórmula acima será corrigido. Avaliação das premissas: Uma das maneiras de avaliarmos as premissas de normalidade e homogeneidade da variância relacionadas às análises do teste T, ANOVA e regressões lineares simples e múltiplas é o uso da inspeção gráfica da distribuição dos resíduos (Figura 1) (Zuur, Ieno, and Elphick 2009a). A homegeneidade da variância utiliza um gráfico dos resíduos pelos valores preditos (Figura 1A). A distribuição dos resíduos será homogênea se não observarmos nenhum padrão na distribuição dos pontos (i.e. forma em V, U ou funil). A normalidade dos resíduos utiliza um gráfico de quantis-quantis (QQ-plots). A distribuição dos resíduos será normal se os pontos estiverem próximos à reta (Figure 1B). Inspeção gráfica da homogeneidade da variância (A) e normalidade (B) dos resíduos. Os símbolos verdes indicam que os gráficos em que os resíduos apresentam distribuição homogênea e normal, enquanto os símbolos vermelhos indicam os gráficos em que os resíduos violam as premissas do teste. 7.1.0.1 Exemplo prático 1 - Teste T para duas amostras com variâncias iguais Explicação dos dados Neste exemplo avaliaremos o comprimento rostro-cloacal (CRC em milímetros) de machos de Physalaemus nattereri (Anura:Leptodactylidae) amostrados em diferentes estações do ano com armadilhas de interceptação e queda na Região Noroeste do estado de São Paulo (da Silva and Rossa-Feres 2010). Pergunta: O CRC dos machos de P. nattereri é maior na estação chuvosa do que na estação seca? Predições O CRC dos machos será maior na estação chuvosa porque há uma vantangem seletiva para os indivíduos maiores durante a atividade reprodutiva. Variáveis Variáveis resposta e preditoras Dataframe com os indivíduos (unidade amostral) nas linhas e CRC (mm - variável resposta contínua) e estação (variável preditora categórica) como colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e variáveis preditoras e respostas nas colunas. Análise Vamos olhar os dados usando a função head head(CRC_PN_macho) #&gt; CRC Estacao #&gt; 1 3.82 Chuvosa #&gt; 2 3.57 Chuvosa #&gt; 3 3.67 Chuvosa #&gt; 4 3.72 Chuvosa #&gt; 5 3.75 Chuvosa #&gt; 6 3.83 Chuvosa Vamos verificar a normalidade dos resíduos usando o QQ-plot. ## Teste de normalidade residuos &lt;- lm(CRC ~ Estacao, data = CRC_PN_macho) qqPlot(residuos) #&gt; [1] 22 26 Os pontos estão próximos a reta indicando que a distribuição dos resíduos é normal (Figura 1). Outra possibilidade é usar os testes de Shapiro-Wilk e Levene para verificar a normalidade e a homogeneidade da variância respectivamente.  Importante Hipótese nula destes testes é que a distribuição é normal ou homogênea: Valor de p &lt; 0.05 significa que os dados não apresentam distribuição normal ou homogênea; valor de p &gt; 0.05 significa que os dados apresentam distribuição normal ou homogênea. # Teste de Shapiro shapiro.test (CRC_PN_macho$CRC) #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: CRC_PN_macho$CRC #&gt; W = 0.95559, p-value = 0.05417 Teste de Levene para homogeneidade de variância. ## Teste de homogeneidade de variância leveneTest(CRC ~ Estacao, data = CRC_PN_macho) #&gt; Levene&#39;s Test for Homogeneity of Variance (center = median) #&gt; Df F value Pr(&gt;F) #&gt; group 1 1.1677 0.2852 #&gt; 49 Percebam que a distribuição dos resíduos foi normal e homogênea na inspeção gráfica, assim como nos testes de Shapiro e Levene, respectivamente. Agora podemos realizar a análise sabendo que os dados seguem as premissas requeridas pelo test T. Vamos para os comandos da análise do Teste T amostrans indenpendentes e variâncias iguais. ## Análise Teste T t.test(CRC ~ Estacao, data = CRC_PN_macho, var.equal = TRUE) #&gt; #&gt; Two Sample t-test #&gt; #&gt; data: CRC by Estacao #&gt; t = 4.1524, df = 49, p-value = 0.000131 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.2242132 0.6447619 #&gt; sample estimates: #&gt; mean in group Chuvosa mean in group Seca #&gt; 3.695357 3.260870 Quatro valores devem ser apresentados ao leitores: i ) estatística do teste - representada por t = 4,15; ii) valor de significancia - representado por p-value = 0,0001; iii) graus de liberdade - representado por df = 49; e iv) diferença entre as médias. Veja abaixo como descrever os resultados no seu trabalho. Visualizar os resultados em gráfico. ## Gráfico ggplot(data = CRC_PN_macho, aes(x = Estacao, y = CRC, color = Estacao)) + labs(x = &quot;Estações&quot;, y = &quot;CRC (mm) - P. nattereri&quot;) + geom_boxplot(fill = c(&quot;darkorange&quot;, &quot;cyan4&quot;), color = &quot;black&quot;, outlier.shape = NA) + geom_jitter(shape = 16, position = position_jitter(0.1), cex = 5, alpha = 0.7) + scale_color_manual(values = c(&quot;black&quot;, &quot;black&quot;)) + tema_livro() + theme(legend.position = &quot;none&quot;) Interpretação dos resultados Neste exemplo, rejeitamos a hipótese nula de que as médias do CRC dos machos entre as estações seca e chuvosa são iguais. Os resultados mostram que os machos de P. nattereri coletados na estação chuvosa foram em média 0,43 mm maiores do que os coletados na estação seca (t49 = 4,15, P &lt; 0,001). 7.1.0.2 Exemplo prático 2 - Teste T para duas amostras independentes com variâncias diferentes Explicação dos dados Neste exemplo, avaliaremos o comprimento rostro-cloacal (CRC - milímetros) de fêmeas de Leptodactylus podicipinus amostradas em diferentes estações do ano com armadilhas de interceptação e queda na Região Noroeste do estado de São Paulo (da Silva and Rossa-Feres 2010). Observação: Os dados foram alterados em relação a publicação original para se enquadrarem no exemplo de amostras com variâncias diferentes. Pergunta: O CRC das fêmeas de L. podicipinus é maior na estação chuvosa do que na estação seca? Predições O CRC das fêmeas será maior na estação chuvosa porque há uma vantangem seletiva para os indivíduos maiores durante a atividade reprodutiva. Variáveis Variáveis resposta e preditoras Dataframe com os indivíduos (unidade amostral) nas linhas e CRC (mm - variável resposta contínua) e estação (variável preditora categórica) como colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e variáveis preditoras e respostas nas colunas. Análise Olhar os dados usando a funçãohead head(CRC_LP_femea) #&gt; CRC Estacao #&gt; 1 2.72 Chuvosa #&gt; 2 2.10 Chuvosa #&gt; 3 3.42 Chuvosa #&gt; 4 1.50 Chuvosa #&gt; 5 3.90 Chuvosa #&gt; 6 4.00 Chuvosa Vamos avaliar as premissas do teste. Començando com o teste de normalidade. ## Teste de normalidade usando QQ-plot residuos_LP &lt;- lm(CRC ~ Estacao, data = CRC_LP_femea) qqPlot(residuos_LP) #&gt; [1] 4 6 Os resíduos apresentam distribuição normal. Agora vamos avaliar a homogeneidade da variância. # Teste de homogeneidade da variância leveneTest(CRC ~ Estacao, data = CRC_LP_femea) #&gt; Levene&#39;s Test for Homogeneity of Variance (center = median) #&gt; Df F value Pr(&gt;F) #&gt; group 1 9.8527 0.01053 * #&gt; 10 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Os resíduos não apresentam distribuição homogênea. Portanto, vamos realizazr o teste T com variâncias diferentes. Para isso, use o argumento var.equal = FALSE t.test(CRC ~ Estacao, data = CRC_LP_femea, var.equal = FALSE) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: CRC by Estacao #&gt; t = -1.7633, df = 6.4998, p-value = 0.1245 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.5489301 0.2375016 #&gt; sample estimates: #&gt; mean in group Chuvosa mean in group Seca #&gt; 2.834286 3.490000 Neste exemplo, não rejeitamos a hipótese nula e consideramos que as médias do CRC das fêmeas entre as estações seca e chuvosa são iguais (t6,49 = 1,76, P = 0,12). Visualizar os resultados em gráfico. ## Gráfico ggplot(data = CRC_LP_femea, aes(x = Estacao, y = CRC, color = Estacao)) + labs(x = &quot;Estações&quot;, y = &quot;CRC (mm) - L. podicipinus&quot;, size = 15) + geom_boxplot(fill=c(&quot;darkorange&quot;, &quot;cyan4&quot;), color=&quot;black&quot;, outlier.shape = NA) + geom_jitter(shape = 16, position=position_jitter(0.2), cex = 5, alpha = 0.7) + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;cyan4&quot;)) + tema_livro() + theme(legend.position = &quot;none&quot;) Interpretação dos resultados Os resultados mostram que as fêmeas de L. podicipinus coletadas na estação chuvosa não são maiores do que as fêmeas coletadas na estação seca, apesar de possuirem maior variância, o que pode ser biologicamente interessante. 7.2 Teste T para amostras pareadas O Teste T Pareado é uma estatística que usa dados medidos duas vezes na mesma unidade amostral, resultando em pares de observações para cada amostra (amostras pareadas). Ele determina se a diferença da média entre duas observações é zero. \\[ t = \\frac{\\bar{d}}{S_{\\bar{d}}}\\] Onde: \\(\\bar{d}\\) = média da diferença das medidas pareadas. Observe que o teste não usa as medidas originais, e sim, a diferença para cada par, S\\(\\bar{d}\\) = erro padrão da diferença das medidas pareadas. Premissas do Teste t para amostras pareadas: As unidades amostrais são selecionadas aleatoriamente; As observações não são independentes; Distribuição normal (gaussiana) dos valores da diferença para cada par. 7.2.0.1 Exemplo prático 1 - Teste T para amostras pareadas Explicação dos dados Neste exemplo avaliaremos a diferença na riqueza de espécies de artrópodes registradas em 27 localidades. Todas as localidades foram amostradas duas vezes. A primeira amostragem foi realizada na localidade antes da pertubação e a segunda amostragem foi realizada após a localidade ter sofrido uma queimada. Portanto, existe uma dependência temporal uma vez que amostramos a mesma localidade antes e depois da queimada. Pergunta: A riqueza de espécies de artrópodes é prejudicada pelas queimadas? Predições A riqueza de espécies de artrópodes será maior antes da queimada devido a extinção local das espécies. Variáveis Variáveis resposta e preditoras Dataframe com as localidades nas linhas e riqueza de espécies (variável resposta contínua) e estado (Pre-queimada ou Pós-queimada - variável preditora categórica) da localidade nas colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e variáveis preditoras e respostas nas colunas. Análise Olhando os dados com a função head head(Pareado) #&gt; Areas Riqueza Estado #&gt; 1 1 92 Pre-Queimada #&gt; 2 2 74 Pre-Queimada #&gt; 3 3 96 Pre-Queimada #&gt; 4 4 89 Pre-Queimada #&gt; 5 5 76 Pre-Queimada #&gt; 6 6 80 Pre-Queimada Cálculo do Teste T com amostras pareadas. ## Análise Teste T Pareado # O uso do [] é para selecionar dentro do vetor/coluna *Riqueza* os 27 # primeiros números [1:27] que representam as localidades antes da # queimada e os últimos 27 números [28:54] que representam as mesmas # localidades pós-queimada. t.test(Pareado$Riqueza[1:27], Pareado$Riqueza[28:54], paired = TRUE) #&gt; #&gt; Paired t-test #&gt; #&gt; data: Pareado$Riqueza[1:27] and Pareado$Riqueza[28:54] #&gt; t = 7.5788, df = 26, p-value = 4.803e-08 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 32.47117 56.63994 #&gt; sample estimates: #&gt; mean of the differences #&gt; 44.55556 Neste exemplo, rejeitamos a hipótese nula de que a riqueza de espécies de artrópodes é igual antes e depois da queimada (t26 = 7,57, P &lt; 0,001). Visualizar os resultados em gráfico. ## Gráfico ggpaired(Pareado, x = &quot;Estado&quot;, y = &quot;Riqueza&quot;, color = &quot;Estado&quot;, line.color = &quot;gray&quot;, line.size = 0.8, palette = c(&quot;darkorange&quot;, &quot;cyan4&quot;), width = 0.8, point.size = 4, xlab = &quot;Estado das localidades&quot;, ylab = &quot;Riqueza de Espécies&quot;) + expand_limits(y=c(0,150)) + tema_livro() Interpretação dos resultados Os resultados mostram que as localidades após as queimadas apresentam em média 44,5 espécies de artrópodes a menos do que antes das queimadas. 7.3 Correlação de Pearson É um teste que mede a força relativa da relação linear entre duas variáveis contínuas (X e Y). Importante ressaltar que a análise de correlação não assume que a variável X influencie a variável Y ou que exista uma relação de causa e efeito entre elas (Zar 2010). A análise é definida em termos da variância de X, a variância de Y, e a covariância de X e Y (i.e. como elas variam juntas). \\[ r = \\frac{\\sum{XY} - \\frac{\\sum{X} \\sum{Y}}{n}}{\\sqrt{\\left(\\sum{X^2} - \\frac{\\sum{X}^2}{n}\\right)\\left(\\sum{Y^2} - \\frac{\\sum{Y}^2}{n}\\right)}} \\] Onde: r = coeficiente de correlação que indica a força da relação linear entre as duas variáveis. Seu limite de valores está entre -1 \\(\\leq\\) r \\(\\le\\) 1. A correlação positiva indica que o aumento no valor de uma das variáveis é acompanhado pelo aumento no valor da outra variável. A correlação negativa indica que o aumento no valor de uma das variáveis é acompanhado pela diminuição no valor da outra variável. Se r é igual a zero, não existe correlação entre as variáveis (Figura 2). Premissas da Correlação de Person: As amostras devem ser independentes e pareadas (i.e. as duas variáveis devem ser medidas na mesma unidade amostral); As unidades amostrais são selecionadas aleatoriamente; A relação entre as variáveis tem que ser linear. Exemplo de correlações negativa (A), positiva (B) e nula (C) e variáveis que não apresentam relações lineares entre si (D-E). 7.3.0.1 Exemplo prático 1 - Correlação de Pearson Explicação dos dados Neste exemplo, avaliaremos a correlação entre a altura do tronco e o tamanho da raiz medidos em 35 indivíduos de uma espécie vegetal arbustiva. Pergunta: Existe correlação entre a altura do tronco e o tamanho da raiz dos arbustos? Predições A altura do tronco é positivamente correlacionada com o tamanho da raiz. Variáveis Variáveis Dataframe com os indivíduos (unidade amostral) nas linhas e altura do tronco e tamanho da raiz (duas variáveis tem que ser contínuas) como colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e variáveis preditoras e respostas nas colunas. Análise Vamos plhar os dados com a função head. head(correlacao_arbustos) #&gt; Tamanho_raiz Tamanho_tronco #&gt; 1 10.177049 19.54383 #&gt; 2 6.622634 17.13558 #&gt; 3 7.773629 19.50681 #&gt; 4 11.055257 21.57085 #&gt; 5 4.487274 13.22763 #&gt; 6 11.190216 21.62902 Cálculo do Teste de Correlação de Pearson. Para outros testes de correlação como Kendall ou Spearman é só alterar na # linha de comando a opção *method* e inserir o teste desejado. ## Correção de Person cor.test(correlacao_arbustos$Tamanho_raiz, correlacao_arbustos$Tamanho_tronco, method = &quot;pearson&quot;) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: correlacao_arbustos$Tamanho_raiz and correlacao_arbustos$Tamanho_tronco #&gt; t = 11.49, df = 33, p-value = 4.474e-13 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.7995083 0.9457816 #&gt; sample estimates: #&gt; cor #&gt; 0.8944449 Neste exemplo, rejeitamos a hipótese nula de que as variáveis não são correlacionadas (r = 0.89, P &lt; 0,001). Visualizar os resultados em gráfico. ## Gráfico ggplot(data = correlacao_arbustos, aes(x = Tamanho_raiz, y = Tamanho_tronco)) + labs(x = &quot;Tamanho da raiz&quot;, y = &quot;Altura do tronco&quot;) + geom_point(size = 4, shape = 21, fill = &quot;darkorange&quot;, alpha = 0.7) + geom_text(x = 14, y = 14, label = &quot;r = 0.89, P &lt; 0.001&quot;, color = &quot;black&quot;, size = 5) + tema_livro() + theme(legend.position = &quot;none&quot;) + geom_smooth(method = lm, se = FALSE, color = &quot;black&quot;, linetype =&quot;dashed&quot;)  Importante: a linha de tendência tracejada no gráfico é apenas para ilustrar a relação positiva entre as variáveis. Ela não é gerada pela análise de correlação. Interpretação dos resultados Os resultados mostram que o aumento na altura dos arbutos é acompanhado pelo aumento no tamanho da raiz. 7.4 Regressão Linear Simples A regressão linear simples é usada para analisar a relação entre uma variável preditora (plotada no eixo-X) e uma variável resposta (plotada no eixo-Y). As duas variáveis devem ser contínuas. Diferente das correlações, a regressão assume uma relação de causa e efeito entre as variáveis. O valor da variável preditora (X) causa, direta ou indiretamente, o valor da variável resposta (Y). Assim, Y é uma função linear de X: \\[ Y = \\beta_0 + \\beta_{1}X_i + \\epsilon_i \\] Onde: \\(\\beta_0\\) = intercepto (intercept) que representa o valor da função quando X = 0, \\(\\beta_{1}\\) = inclinação (slope) que mede a mudança na variável Y para cada mudança de unidade da variável X. \\(\\epsilon_{1}\\) = erro aleatório referente à variável Y que não pode ser explicado pela variável X. Premissas da Regressão Linear Simples: As amostras devem ser independentes; As unidades amostrais são selecionadas aleatoriamente; Distribuição normal (gaussiana) dos resíduos; Homogeneidade da variância. 7.4.0.1 Exemplo prático 1 - Regressão linear simples Explicação dos dados Neste exemplo, avaliaremos a relação entre o gradiente de temperatura média anual (°C) e o tamanho médio do comprimento rostro-cloacal (CRC em mm) de populações de Dendropsophus minutus (Anura:Hylidae) amostradas em 109 localidades no Brasil (Boaratti and da Silva 2015). Pergunta: Há relação de dependência entre o tamanho do CRC das populações e a temperatura das localidades onde os indivíduos ocorrem? Predições O CRC das populações serão menores em localidades mais quentes do que em localidades mais frias de acordo com a Hipótese do balanço de calor (Olalla-Tárraga and Rodríguez 2007). Variáveis Variáveis resposta e preditoras Dataframe com as populações (unidade amostral) nas linhas e CRC (variável resposta) médio (mm) e temperatura média anual (variável preditora) como colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e variáveis preditoras e respostas nas colunas. Análise Olhando os dados com a função head head(dados_regressao) #&gt; Municipio CRC Temperatura Precipitacao #&gt; 1 Acorizal 22.98816 24.13000 1228.2 #&gt; 2 Alpinopolis 22.91788 20.09417 1487.6 #&gt; 3 Alto_Paraiso 21.97629 21.86167 1812.4 #&gt; 4 Americana 23.32453 20.28333 1266.2 #&gt; 5 Apiacas 22.83651 25.47333 2154.0 #&gt; 6 Arianopolis 20.86989 20.12167 1269.2 Vamos calcular a regressão linear simples. ## Regressão simples modelo_regressao &lt;- lm(CRC ~ Temperatura, data = dados_regressao) Antes de vermos os resultados, vamos verificar a normalidade e homogeneidade das variâncias ## Verificar as premissas do teste par(mfrow = c(2, 2), oma = c(0, 0, 2, 0)) plot(modelo_regressao) dev.off() # volta a configuração dos gráficos para o formato padrão #&gt; null device #&gt; 1 Os gráficos Residuals vs Fitted, Scale-Location, e Residual vs Leverage estão relacionados com a homogeneidade da variância. Nestes gráficos, esperamos ver os pontos dispersos no espaço sem padrões com formatos em U ou funil. Podemos observar que tanto a normalidade como a homogeneidade do resíduos estão dentro dos padrões esperados. Vamos ver os resultados da regressão simples usando as funções anova e summary. A função anova retorna uma tabela contendo o grau de liberdade (df), soma dos quadrados, valor de F e o valor de P. ## Resultados usando a função anova anova(modelo_regressao) #&gt; Analysis of Variance Table #&gt; #&gt; Response: CRC #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Temperatura 1 80.931 80.931 38.92 9.011e-09 *** #&gt; Residuals 107 222.500 2.079 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A função summary retorna uma tabela contendo o valor do intercepto, inclinação da reta (slope) e o coeficiente de determinação (R2) que indica a proporção da variação na variável Y que pode ser atribuída à variação na variável X. Percebam que a parte final dos resultados apresentados no summary, são os mesmo apresentados pela função anova. # Resultados usando a função summary summary(modelo_regressao) #&gt; #&gt; Call: #&gt; lm(formula = CRC ~ Temperatura, data = dados_regressao) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.4535 -0.7784 0.0888 0.9168 3.1868 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 16.23467 0.91368 17.768 &lt; 2e-16 *** #&gt; Temperatura 0.26905 0.04313 6.239 9.01e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.442 on 107 degrees of freedom #&gt; Multiple R-squared: 0.2667, Adjusted R-squared: 0.2599 #&gt; F-statistic: 38.92 on 1 and 107 DF, p-value: 9.011e-09 Vamos visualizar os resultados em gráfico. ## Gráfico ggplot(data = dados_regressao, aes(x = Temperatura, y = CRC)) + labs(x = &quot;Temperatura média anual (°C)&quot;, y = &quot;Comprimento rostro-cloacal (mm)&quot;) + geom_point(size = 4, shape = 21, fill = &quot;darkorange&quot;, alpha = 0.7) + tema_livro() + theme(legend.position = &quot;none&quot;) + geom_smooth(method = lm, se = FALSE, color = &quot;black&quot;) Interpretação dos resultados Neste exemplo, rejeitamos a hipótese nula de que não existe relação entre o tamanho do CRC das populações de D. minutus e a temperatura da localidade onde elas ocorrem (F1,107 = 38,92, P &lt; 0,001). Os resultados mostram que o tamanho do CRC das populações tem uma relação positiva com a temperatura das localidades. Assim, populações de D. minutus em localidades mais quentes apresentam maior CRC do que as populações em localidades mais frias. 7.5 Regressão Linear Múltipla A regressão linear múltipla é uma extensão da regressão linear simples. Ela é usada quando queremos determinar o valor da variável resposta (Y) com base nos valores de duas ou mais variáveis preditoras (X1, X2, Xn). \\[ Y = \\beta_0 + \\beta_{1}X_1 + \\beta_{n}X_n + \\epsilon_i \\] Onde: \\(\\beta_0\\) = intercepto (intercept) que representa o valor da função quando X = 0; \\(\\beta_{n}\\) = inclinação (slope) que mede a mudança na variável Y para cada mudança de unidade das variáveis Xn; \\(\\epsilon_{1}\\) = erro aleatório referente a variável Y que não pode ser explicado pelas variáveis preditoras. Premissas da Regressão Linear Múltipla: As amostras devem ser independentes; As unidades amostrais são selecionadas aleatoriamente; Distribuição normal (gaussiana) dos resíduos; Homogeneidade da variância. 7.5.0.1 Exemplo prático 1 - Regressão linear múltipla Explicação dos dados Utilizaremos o mesmo exemplo da regressão linear simples. Contudo, além do gradiente de temperatura média anual (°C), incluiremos o gradiente de precipitação anual (mm) como outra variável preditora do tamanho médio do comprimento rostro-cloacal (CRC em mm) de populações de Dendropsophus minutus (Anura:Hylidae) amostradas em 109 localidades no Brasil (Boaratti and da Silva 2015). Pergunta: O tamanho do CRC das populações de D. minutus é influênciado pela temperatura e precipitação das localidades onde os indivíduos ocorrem? Predições O CRC das populações serão menores em localidades com clima quente e chuvoso do que em localidades com clima frio e seco. Variáveis Variáveis resposta e preditoras Dataframe com as populações (unidade amostral) nas linhas e CRC (variável resposta) médio (mm) e temperatura e precipitação (variáveis preditoras) como colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e variáveis preditoras e respostas nas colunas. Análise Olhando os dados usando a função head head(dados_regressao_mul) #&gt; Municipio CRC Temperatura Precipitacao #&gt; 1 Acorizal 22.98816 24.13000 1228.2 #&gt; 2 Alpinopolis 22.91788 20.09417 1487.6 #&gt; 3 Alto_Paraiso 21.97629 21.86167 1812.4 #&gt; 4 Americana 23.32453 20.28333 1266.2 #&gt; 5 Apiacas 22.83651 25.47333 2154.0 #&gt; 6 Arianopolis 20.86989 20.12167 1269.2 Comandos para o modelo de regressão múltipla. ## Regressão múltipla modelo_regressao_mul &lt;- lm(CRC ~ Temperatura + Precipitacao, data = dados_regressao_mul)  Importante Multicolinearidade ocorre quando as variáveis preditoras são correlacionadas. Essa correlação é um problema porque as variáveis preditoras deveriam ser independentes. O Fator de Inflação da Variância (VIF) é um teste que identifica a correlação entre as variáveis e mostra a força dessa correlação. Alguns autores consideram valores de VIF acima de 10 como fortemente correlacionadas, outros mais conservadores consideram o valor de 3. Vamos analisar se as variáveis apresentam multicolinearidade. # Multicolinearidade vif(modelo_regressao_mul) #&gt; Temperatura Precipitacao #&gt; 1.041265 1.041265 Os valores são menores que 3 indicando que não há multicolinearidade. Agora vamos verificar as premissas de normalidade e homogeneidade das variâncias. ## Normalidade e homogeneidade par(mfrow = c(2, 2), oma = c(0, 0, 2, 0)) plot(modelo_regressao_mul) dev.off() #&gt; null device #&gt; 1 Os resíduos apresentam distribuição normal e variâncias homogêneas. Podemos ver os resultados da análise. ## Regressão múltipla summary(modelo_regressao_mul) #&gt; #&gt; Call: #&gt; lm(formula = CRC ~ Temperatura + Precipitacao, data = dados_regressao_mul) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.4351 -0.8026 0.0140 0.9420 3.4300 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 16.7162571 1.0108674 16.537 &lt; 2e-16 *** #&gt; Temperatura 0.2787445 0.0439601 6.341 5.71e-09 *** #&gt; Precipitacao -0.0004270 0.0003852 -1.108 0.27 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.44 on 106 degrees of freedom #&gt; Multiple R-squared: 0.2751, Adjusted R-squared: 0.2614 #&gt; F-statistic: 20.12 on 2 and 106 DF, p-value: 3.927e-08 Percebam que a temperatura tem uma relação significativa e positiva com o tamanho do CRC das populações (P &lt; 0.001), enquanto que a precipitação não apresenta relação com o CRC (P = 0.27). Neste caso, é interessante saber se um modelo mais simples (e.g. contendo apenas temperatura) explicaria a distribuição tão bem ou melhor do que este modelo mais complexo considerando duas variáveis (temperatura e precipitação). Para isso, podemos utilizar a Likelihood Ratio Test (LRT) para comparar modelos. A LRT compara dois modelos aninhados, testando se os parâmetros do modelo mais complexo diferem significativamente do modelo mais simples. Em outras palavras, ele testa se há necessidade de se incluir uma variável extra no modelo para explicar os dados. ## Criando os modelos aninhados modelo_regressao_mul &lt;- lm(CRC ~ Temperatura + Precipitacao, data = dados_regressao_mul) modelo_regressao &lt;- lm(CRC ~ Temperatura, data = dados_regressao_mul) ## Likelihood Ratio Test (LRT) lrtest(modelo_regressao_mul, modelo_regressao) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: CRC ~ Temperatura + Precipitacao #&gt; Model 2: CRC ~ Temperatura #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 4 -192.93 #&gt; 2 3 -193.55 -1 1.2558 0.2624  Importante Hipótese nula é que o modelo mais simples é o melhor Valor de p &lt; 0.05 rejeita a hipótese nula e o modelo mais complexo é o melhor; valor de p &gt; 0.05 não rejeita a hipótese nula e o modelo mais simples é o melhor. ## Comparando com o modelo somente com o intercepto # Criando um modelo sem variáveis, só o intercepto. modelo_intercepto &lt;- lm(CRC ~ 1, data = dados_regressao_mul) lrtest(modelo_regressao, modelo_intercepto) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: CRC ~ Temperatura #&gt; Model 2: CRC ~ 1 #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 3 -193.55 #&gt; 2 2 -210.46 -1 33.815 6.061e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Interpretação dos resultados Neste exemplo, a precipitação não está associada com a variação no tamanho do CRC das populações de D. minutus. Por outro lado, a temperatura explicou 26% da variação do tamanho do CRC das populações. 7.6 Análises de Variância (ANOVA) ANOVA refere-se a uma variedade de delineamentos experimentais nos quais a variável preditora é categórica e a variável resposta é contínua (Nicholas J. Gotelli and Ellison 2012). Exemplos desses delineamentos experimentais são: ANOVA de um fator, ANOVA de dois fatores, ANOVA em blocos aleatorizados, ANOVA de medidas repetidas e ANOVA split-splot. De forma geral, a ANOVA é um teste estatístico usado para comparar a média entre grupos amostrados independentemente. Para isso, o teste leva em conta, além das médias dos grupos, a variação dos dados dentro e entre os grupos. Neste capítulo, iremos demonstrar as linhas de comandos para alguns dos principais delineamentos experimentais. Premissas da ANOVA: As amostras devem ser independentes. Observação: ANOVA de medidas repetidas e ANOVA split-plot são designs experimentais que apresentam dependência entre as amostras, mas controlam esse depedência nas suas formulações matemátcas; As unidades amostrais são selecionadas aleatoriamente; Distribuição normal (gaussiana) dos resíduos; Homogeneidade da variância. 7.7 ANOVA de um fator Este teste considera delineamentos experimentais com apenas um fator (ou tratamento) que pode ser composto por três ou mais grupos (ou níveis). 7.7.0.1 Exemplo prático 1 - Anova de um fator Explicação dos dados Neste exemplo, avaliaremos se o adubo X-2020 disponibilizado recentemente no mercado melhora o crescimento dos indivíduos de Coffea arabica como divulgado pela empresa responsável pela venda do produto. Para isso, foi realizado um experimento com indivíduos de C. arabica cultivados em três grupos: i) grupo controle onde os indivíduos não receberam adubação, ii) grupo onde os indivíduos receberam a adição do adubo tradicional mais utilizado pelos produtores de C. arabica, e iii) grupo onde os indivíduos receberam a adição do adubo X-2020. Pergunta: O crescimento dos indivíduos de C. arabica é melhorado pela adição do adubo X-2020? Predições O crescimento dos indivíduos de C. arabica será maior no grupo que recebeu o adubo X-2020. Variáveis Variáveis resposta e preditoras Dataframe com as plantas (unidade amostral) nas linhas e o crescimento dos indivíduos de C. arabica (variável resposta) e os tratamentos (variável preditora) nas colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e variável preditora e resposta nas colunas. 7.7.1 Análise Olhando os dados e criando o modelo para Anova de um fator. head(dados_anova_simples) #&gt; Crescimento Tratamento #&gt; 1 7.190 Controle #&gt; 2 6.758 Controle #&gt; 3 6.101 Controle #&gt; 4 4.758 Controle #&gt; 5 6.542 Controle #&gt; 6 7.667 Controle ## Análise ANOVA de um fator Modelo_anova &lt;- aov(Crescimento ~ Tratamento, data = dados_anova_simples) Vamos verificar a normalidade e homogeneidade da variância usando os testes de Shapiro-Wilk e bartett.test respectivamente. ## Normalidade shapiro.test(dados_anova_simples$Crescimento[1:12]) #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: dados_anova_simples$Crescimento[1:12] #&gt; W = 0.96731, p-value = 0.8806 shapiro.test(dados_anova_simples$Crescimento[13:24]) #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: dados_anova_simples$Crescimento[13:24] #&gt; W = 0.87324, p-value = 0.07184 shapiro.test(dados_anova_simples$Crescimento[25:36]) #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: dados_anova_simples$Crescimento[25:36] #&gt; W = 0.9294, p-value = 0.3738 ## Normalidade bartlett.test(Crescimento ~ Tratamento, data = dados_anova_simples) #&gt; #&gt; Bartlett test of homogeneity of variances #&gt; #&gt; data: Crescimento by Tratamento #&gt; Bartlett&#39;s K-squared = 0.61835, df = 2, p-value = 0.7341 Os resíduos apresentam distribuição normal e variância homogêneas. Vamos ver os resultados da análise. ## Resultados anova anova(Modelo_anova) #&gt; Analysis of Variance Table #&gt; #&gt; Response: Crescimento #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Tratamento 2 340.32 170.160 77.989 3.124e-13 *** #&gt; Residuals 33 72.00 2.182 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Percebam que o resultado da ANOVA (Pr(&gt;F) &lt; 0.001) indica que devemos rejeitar a hipótese nula que não há diferença entre as médias dos grupos. Contudo, os resultados não mostram quais são os grupos que apresentam diferenças. Para isso, temos que realizar testes de comparações múltiplas post-hoc para detectar os grupos que apresentam diferenças significativas entre as médias.  Importante Os testes post-hoc só devem ser utilizados quando rejeitamos a hipótese nula (P &lt; 0.05) no teste da ANOVA. ## Diferenças entre os tratamentos # Teste de Tuckey&#39;s honest significant difference TukeyHSD(Modelo_anova) #&gt; Tukey multiple comparisons of means #&gt; 95% family-wise confidence level #&gt; #&gt; Fit: aov(formula = Crescimento ~ Tratamento, data = dados_anova_simples) #&gt; #&gt; $Tratamento #&gt; diff lwr upr p adj #&gt; Adubo_X-2020-Adubo_Tradicional 0.04991667 -1.429784 1.529617 0.9962299 #&gt; Controle-Adubo_Tradicional -6.49716667 -7.976867 -5.017466 0.0000000 #&gt; Controle-Adubo_X-2020 -6.54708333 -8.026784 -5.067383 0.0000000 Visualizar os resultados em gráfico. # Reordenando a ordem que os grupos irão aparecer no gráfico dados_anova_simples$Tratamento &lt;- factor(dados_anova_simples$Tratamento , levels=c(&quot;Controle&quot;, &quot;Adubo_Tradicional&quot;, &quot;Adubo_X-2020&quot;)) # Gráfico ggplot(data = dados_anova_simples, aes(x = Tratamento, y = Crescimento, color = Tratamento)) + labs(x = &quot;Adubação&quot;, y = &quot;Crescimento Coffea arabica (cm)&quot;, size = 20) + geom_boxplot(fill = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;), color = &quot;black&quot;, show.legend = FALSE, alpha = 0.4) + geom_jitter(shape = 16, position = position_jitter(0.1), cex = 4, alpha = 0.7) + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) + scale_y_continuous(limits = c(0, 20), breaks = c(0, 5, 10, 15, 20)) + geom_text(x = 1, y = 12, label = &quot;ab&quot;, color = &quot;black&quot;, size = 5) + geom_text(x = 2, y = 17, label = &quot;a&quot;, color = &quot;black&quot;, size = 5) + geom_text(x = 3, y = 17, label = &quot;b&quot;, color = &quot;black&quot;, size = 5) + scale_x_discrete(labels = c(&quot;Sem adubo&quot;,&quot;Tradicional&quot;,&quot;X-2020&quot;)) + tema_livro() + theme(legend.position = &quot;none&quot;) Interpretação dos resultados Neste exemplo, os indivíduos de C. arabica que receberam adubação (tradicional e X-2020) apresentaram maior crescimento do que os indivíduos que não receberam adubação. Contudo, diferente do que foi divulgado pela empresa, o adubo X-2020 não apresentou melhor desempenho que o adubo tradicional já utilizado pelos produtores. 7.8 ANOVA com dois fatores ou ANOVA fatorial Este teste considera delineamentos amostrais com dois fatores (ou tratamentos) que podem ser compostos por dois ou mais grupos (ou níveis). Esta análise tem uma vantagem, pois permite avaliar o efeito da interação entre os fatores na variável resposta. Quando a interação está presente, o impacto de um fator depende do nível (ou grupo) do outro fator. 7.8.0.1 Exemplo prático 1 - ANOVA com dois fatores Explicação dos dados Neste exemplo, avaliaremos se o tempo que o corpo leva para eliminar uma droga utilizada em exames de ressonância magnética está relacionado com o sistema XY de determinação do sexo e/ou com a idade dos pacientes. Para isso, foi realizado um experimento com 40 pacientes distribuídos da seguinte maneira: i) 10 indivíduos XX - jovens, ii) 10 indivíduos XX - idosas, iii) 10 indivíduos XY - jovens, e iv) 10 indivíduos XY - idosos. Pergunta: O tempo de eliminação da droga é dependente do sistema XY de determinação do sexo e idade dos pacientes? Predições O tempo de eliminação da droga vai ser mais rápido nas pacientes XX e jovens. Variáveis Variáveis resposta e preditoras Dataframe com os pacientes (unidade amostral) nas linhas e o tempo de eliminação da droga (variável resposta) e os tratamentos sexo e idade dos pacientes (variáveis preditoras) nas colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e as variáveis preditoras e respostas nas colunas. 7.8.1 Análise Olhando os dados usando a função head head(dados_dois_fatores) #&gt; Tempo Pessoas Idade #&gt; 1 18.952 XX Jovem #&gt; 2 16.513 XX Jovem #&gt; 3 17.981 XX Jovem #&gt; 4 21.371 XX Jovem #&gt; 5 14.470 XX Jovem #&gt; 6 19.130 XX Jovem Comandos da ANOVA com dois fatores. ## Análise Anova de dois fatores # A interação entre os fatores é representada por * Modelo1 &lt;- aov(Tempo ~ Pessoas * Idade, data = dados_dois_fatores) # Olhando os resultados anova(Modelo1) #&gt; Analysis of Variance Table #&gt; #&gt; Response: Tempo #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Pessoas 1 716.72 716.72 178.8538 1.56e-15 *** #&gt; Idade 1 1663.73 1663.73 415.1724 &lt; 2.2e-16 *** #&gt; Pessoas:Idade 1 4.77 4.77 1.1903 0.2825 #&gt; Residuals 36 144.26 4.01 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Percebam que a interação não apresenta um efeito significativo (P &gt; 0.05). Assim, iremos retirar a interação e verificar, usando Likelihood Ratio Test, se o modelo mais simples é melhor. # Criando modelo sem interação. Modelo2 &lt;- aov(Tempo ~ Pessoas + Idade, data = dados_dois_fatores) ## LRT lrtest(Modelo1, Modelo2) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: Tempo ~ Pessoas * Idade #&gt; Model 2: Tempo ~ Pessoas + Idade #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 5 -82.413 #&gt; 2 4 -83.063 -1 1.3012 0.254 A interação não é importante. Então podemos seguir com o modelo mais simples. Vamos verficiar a normalidade e homogeneidade da variância. # Verificando as premissas do teste. par(mfrow = c(2, 2), oma = c(0, 0, 2, 0)) plot(Modelo2) dev.off() #&gt; null device #&gt; 1 Dois pontos estão fugindo da reta e chamam atenção sobre a normalidade da distribuição dos resíduos. A homogeneidade da variância está adequada. Por enquanto, vamos seguir a análise, mas veja o ?? para entender como lidar como modelos que os resíduos não apresentam distribuição normal. # Resultados do modelo anova(Modelo2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: Tempo #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Pessoas 1 716.72 716.72 177.94 1.041e-15 *** #&gt; Idade 1 1663.73 1663.73 413.05 &lt; 2.2e-16 *** #&gt; Residuals 37 149.03 4.03 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Percebam que o resultado da ANOVA (Pr(&gt;F) &lt; 0.001) indica que devemos rejeitar a hipótese nula de que não há diferença entre as médias dos sistema XY e idade dos pacientes. Neste caso, não precisamos realizar testes de comparações múltiplas post-hoc porque os fatores apresentam apenas dois níveis. Contudo, se no seu delineamento experimental um dos fatores apresentar três ou mais níveis, você deverá utilizar os testes de comparações post-hoc para determinar as diferenças entre os grupos. Visualizar os resultados em gráfico. Interpretação dos resultados Neste exemplo, o sistema XY de determinação do sexo e a idade dos pacientes têm um efeito no tempo de eliminação da droga do organismo. Os pacientes XX e jovens apresentaram eliminação mais rápida da droga do que pacientes XY e idosos. 7.8.1.1 Exemplo prático 2 - ANOVA com dois fatores com efeito da interação Explicação dos dados Neste exemplo, usaremos os mesmos dados do exemplo anterior. Neste caso, alteramos os dados para que a interação seja significativa. head(dados_dois_fatores_interacao) #&gt; Tempo Pessoas Idade #&gt; 1 18.952 XX Jovem #&gt; 2 16.513 XX Jovem #&gt; 3 17.981 XX Jovem #&gt; 4 21.371 XX Jovem #&gt; 5 14.470 XX Jovem #&gt; 6 19.130 XX Jovem ## Análise ANOVA com dois fatores Modelo_interacao1 &lt;- aov(Tempo ~ Pessoas * Idade, data = dados_dois_fatores_interacao) ## Olhando os resultados anova(Modelo_interacao1) #&gt; Analysis of Variance Table #&gt; #&gt; Response: Tempo #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Pessoas 1 716.72 716.72 178.8538 1.56e-15 *** #&gt; Idade 1 1663.73 1663.73 415.1724 &lt; 2.2e-16 *** #&gt; Pessoas:Idade 1 4.77 4.77 1.1903 0.2825 #&gt; Residuals 36 144.26 4.01 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Percebam que a interação é significativa (P &lt; 0.05). Agora nossa interpretação precisa ser baseada na interação entre os fatores. Vamos visualizar os resultados em gráfico. ## Gráfico ggplot(data = dados_dois_fatores_interacao, aes(y = Tempo, x = Pessoas, color = Idade)) + geom_boxplot() + stat_summary(fun = mean, geom =&quot;point&quot;, aes(group = Idade, x = Pessoas), color = &quot;black&quot;, position = position_dodge(0.7), size = 4) + geom_link(aes(x = 0.8, y = 31, xend = 1.8, yend = 40), color = &quot;darkorange&quot;, lwd = 1.3, linetype = 2) + geom_link(aes(x = 1.2, y = 28.5, xend = 2.2, yend = 26.5), color = &quot;cyan4&quot;, lwd = 1.3, linetype = 2) + labs(x = &quot;Sistema XY de determinação do sexo&quot;, y = &quot;Tempo (horas) para eliminar a droga&quot;) + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;cyan4&quot;, &quot;darkorange&quot;, &quot;cyan4&quot;)) + scale_y_continuous(limits = c(10, 50), breaks = c(10, 20, 30, 40, 50)) + tema_livro() Interpretação dos resultados Percebam que para saber a resposta do fator idade (jovem ou idoso) na eliminação da droga, você precisa saber com qual pessoa (XX ou XY) ele está associado. Isso porque a resposta de um fator, depende do outro fator. Jovens eliminam a droga do corpo mais rápido nas pessoas XY, enquanto os idosos eliminam a droga mais rápido nas pessoas XX. 7.8.1.2 Exemplo prático 3 - ANOVA com dois fatores com efeito da interação Explicação dos dados Neste exemplo, usaremos os mesmos dados do exemplo anterior. Entretanto, alteramos os dados para que a interação seja significativa. # Olhando os dados head(dados_dois_fatores_interacao2) #&gt; Tempo Pessoas Idade #&gt; 1 18.952 XX Jovem #&gt; 2 16.513 XX Jovem #&gt; 3 17.981 XX Jovem #&gt; 4 21.371 XX Jovem #&gt; 5 14.470 XX Jovem #&gt; 6 19.130 XX Jovem ## Análise anova de dois fatores Modelo_interacao2 &lt;- aov(Tempo ~ Pessoas * Idade, data = dados_dois_fatores_interacao2) ## Olhando os resultados anova(Modelo_interacao2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: Tempo #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Pessoas 1 716.72 716.72 178.8538 1.56e-15 *** #&gt; Idade 1 4.77 4.77 1.1903 0.2825 #&gt; Pessoas:Idade 1 1663.73 1663.73 415.1724 &lt; 2.2e-16 *** #&gt; Residuals 36 144.26 4.01 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Percebam que a interação é significativa (P &lt; 0.05), mas a idade não é significativa. Nossa interpretação precisa ser baseada na interação entre os fatores. Vamos visualizar os resultados em gráfico. ## Gráfico ggplot(data = dados_dois_fatores_interacao2, aes(y = Tempo, x = Pessoas, color = Idade)) + geom_boxplot() + stat_summary(fun = mean, geom =&quot;point&quot;, aes(group = Idade, x = Pessoas), color = &quot;black&quot;, position = position_dodge(0.7), size = 4) + geom_link(aes(x = 0.8, y = 31, xend = 1.8, yend = 27), color = &quot;darkorange&quot;, lwd = 1.3, linetype = 2) + geom_link(aes(x = 1.2, y = 19, xend = 2.2, yend = 41), color = &quot;cyan4&quot;, lwd = 1.3, linetype = 2) + labs(x = &quot;Sistema XY de determinação do sexo&quot;, y = &quot;Tempo (horas) para eliminar a droga&quot;) + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;cyan4&quot;, &quot;darkorange&quot;, &quot;cyan4&quot;)) + scale_y_continuous(limits = c(10, 50), breaks = c(10, 20, 30, 40, 50)) + tema_livro() Interpretação dos resultados Percebam que as linhas se cruzam. Esse é um exemplo clássico de interação. Novamente, para saber a resposta do fator idade (jovem ou idoso), você precisa saber com qual pessoa (XX ou XY) ele está associado. Jovens são mais rápidos para eliminarem a droga em pessoas XX, enquanto os idosos são mais rápidos para eliminarem a droga nas pessoas XY. 7.9 ANOVA em blocos aleatorizados No delineamento experimental com blocos aleatorizados, cada fator é agrupado em blocos, com réplicas de cada nível do fator representado em cada bloco (Nicholas J. Gotelli and Ellison 2012). O bloco é uma área ou período de tempo dentro do qual as condições ambientais são relativamente homogêneas. O objetivo do uso dos blocos é controlar fontes de variações indesejadas na variável dependente que não são de interesse do pesquisador. Desta maneira, podemos retirar dos resíduos os efeitos das variações indesejadas que não são do nosso interesse, e testar com maior poder estatístico os efeitos dos tratamentos de interesse. Importante, os blocos devem ser arranjados de forma que as condições ambientais sejam mais similares dentro dos blocos do que entre os blocos. 7.9.0.1 Exemplo prático 1 - ANOVA em blocos aleatorizados Explicação dos dados Neste exemplo, avaliaremos a riqueza de espécies de anuros amostradas em poças artificiais instaladas a diferentes distâncias de seis fragmentos florestais no sudeste do Brasil (da Silva et al. 2011). Os fragmentos florestais apresentam diferenças entre si que não são do interesse do pesquisador. Por isso, eles foram incluídos como blocos nas análises. As poças artificiais foram instaladas em todos os fragmentos florestais com base no seguinte delineamento experimental (da Silva et al. 2011): i) quatro poças no interior do fragmento a 100 m de distância da borda do fragmento; ii) quatro poças no interior no fragmento a 50 m de distância da borda do fragmento; iii) quatro poças na borda do fragmento; iv) quatro poças na matriz de pastagem a 50 m de distância da borda do fragmento; e v) quatro poças na matriz de pastagem a 100 m de distância da borda do fragmento. Percebam que todos os tratamentos foram instalados em todos os blocos. Pergunta: A distância da poça artifical ao fragmento florestal influencia a riqueza de espécies anuros? Predições Poças na borda do fragmento florestal apresentarão maior riqueza de espécies do que poças distantes da borda. Variáveis Variáveis resposta e preditoras Dataframe com as poças (unidade amostral) nas linhas e a riqueza de espécies (variável reposta), distância dos fragmentos florestais (variável preditora categórica) e fragmentos florestais (blocos) nas colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e variáveis preditoras e respostas nas colunas. Análise Olhando os dados usando a função head. head(dados_bloco) #&gt; Riqueza Blocos Pocas #&gt; 1 90 A Int-50m #&gt; 2 95 A Int-100m #&gt; 3 107 A Borda #&gt; 4 92 A Mat-50m #&gt; 5 89 A Mat-100m #&gt; 6 92 B Int-50m Há duas formas de incluir os efeitos dos blocos nos modelos. ## Análise Anova em blocos aleatorizados model_bloco1 &lt;- aov(Riqueza ~ Pocas + Blocos, data = dados_bloco) summary(model_bloco1) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Pocas 4 1504 376.1 2.907 0.0478 * #&gt; Blocos 5 1089 217.8 1.683 0.1846 #&gt; Residuals 20 2588 129.4 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 model_bloco2 &lt;- aov(Riqueza ~ Pocas + Error(Blocos), data = dados_bloco) summary(model_bloco2) #&gt; #&gt; Error: Blocos #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Residuals 5 1089 217.8 #&gt; #&gt; Error: Within #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Pocas 4 1504 376.1 2.907 0.0478 * #&gt; Residuals 20 2588 129.4 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Percebam que as duas formas apresentam os mesmos resultados para o efeito # da distância das poças que é o fator de interesse no estudo. Lembre-se que nos delineamentos experimentais em bloco, o pesquisador não está interessado no efeito do bloco, mas sim em controlar a variação associada a ele. O que não pode acontecer é ignorar o efeito do bloco que é incorporado pelos resíduos quando não informado no modelo. Veja abaixo a forma errada de analisar delineamento experimental com blocos. ## Forma errada de análisar Anova em blocos modelo_errado &lt;- aov(Riqueza ~ Pocas, data = dados_bloco) anova(modelo_errado) #&gt; Analysis of Variance Table #&gt; #&gt; Response: Riqueza #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Pocas 4 1504.5 376.12 2.5576 0.06359 . #&gt; Residuals 25 3676.5 147.06 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 O resultado da ANOVA (Pr(&gt;F) &lt; 0.001) indica que devemos rejeitar a hipótese nula que não há diferença entre as médias dos grupos. Contudo, os resultados não mostram quais são os grupos que apresentam diferenças. Para isso, temos que realizar testes de comparações múltiplas post-hoc para detectar os grupos que apresentam diferenças significativas entre as médias. ## Teste de Tuckey&#39;s honest significant difference pairs(lsmeans(model_bloco1, &quot;Pocas&quot;), adjust = &quot;tukey&quot;) #&gt; contrast estimate SE df t.ratio p.value #&gt; Borda - (Int-100m) 16.000 6.57 20 2.436 0.1463 #&gt; Borda - (Int-50m) 19.833 6.57 20 3.020 0.0472 #&gt; Borda - (Mat-100m) 15.833 6.57 20 2.411 0.1531 #&gt; Borda - (Mat-50m) 8.167 6.57 20 1.244 0.7269 #&gt; (Int-100m) - (Int-50m) 3.833 6.57 20 0.584 0.9760 #&gt; (Int-100m) - (Mat-100m) -0.167 6.57 20 -0.025 1.0000 #&gt; (Int-100m) - (Mat-50m) -7.833 6.57 20 -1.193 0.7553 #&gt; (Int-50m) - (Mat-100m) -4.000 6.57 20 -0.609 0.9720 #&gt; (Int-50m) - (Mat-50m) -11.667 6.57 20 -1.777 0.4135 #&gt; (Mat-100m) - (Mat-50m) -7.667 6.57 20 -1.167 0.7692 #&gt; #&gt; Results are averaged over the levels of: Blocos #&gt; P value adjustment: tukey method for comparing a family of 5 estimates Visualizar os resultados em gráfico. # Reordenando a ordem que os grupos irão aparecer no gráfico. dados_bloco$Pocas &lt;- factor(dados_bloco$Pocas, levels = c(&quot;Int-100m&quot;, &quot;Int-50m&quot;, &quot;Borda&quot;, &quot;Mat-50m&quot;, &quot;Mat-100m&quot;)) ## Gráfico ggplot(data = dados_bloco, aes(x = Pocas, y = Riqueza)) + labs(x = &quot;Poças artificiais&quot;, y = &quot;Riqueza de espécies de anuros&quot;) + geom_boxplot(color = &quot;black&quot;, show.legend = FALSE, alpha = 0.4) + geom_jitter(shape = 16, position = position_jitter(0.1), cex = 4, alpha = 0.7) + scale_x_discrete(labels = c(&quot;-100m&quot;,&quot;-50m&quot;,&quot;Borda&quot;, &quot;50m&quot;, &quot;100m&quot;)) + tema_livro() + theme(legend.position = &quot;none&quot;) Interpretação dos resultados Neste exemplo, rejeitamos a hipótese nula de que a distância das poças artificiais até as bordas dos fragmentos florestais não influência a riqueza de espécies de anuros. As poças artificiais instaladas nas bordas dos fragmentos florestais apresentaram maior riqueza de espécies do que as poças distantes. 7.10 Análise de covariância (ANCOVA) A ANCOVA pode ser compreendida como uma extensão da ANOVA com a adição de variável contínua (covariável) medida em todas as unidades amostrais (Nicholas J. Gotelli and Ellison 2012). A ideia é que a covariável também afete os valores da variável resposta. Não incluir a covariável irá fazer com que a variação não explicada pelo modelo concentre-se nos resíduos. Incluindo a covariável, o tamanho do resíduo é menor, e o teste para avaliar as diferenças nos tratamentos, que é o interesse do pesquisador, terá mais poder estatístico. 7.10.0.1 Exemplo prático 1 - ANCOVA Explicação dos dados Neste exemplo, avaliaremos o efeito da herbivoria na biomassa dos frutos de uma espécie de árvore na Mata Atlântica. O delineamento experimental permitiu que alguns indivíduos sofressem herbivoria e outros não. Os pesquisadores também mediram o tamanho da raiz dos indíviduos para inseri-la como uma covariável no modelo. Pergunta: A herbivoria diminiu a biomassa dos frutos? Predições Os indivíduos que sofreram herbivoria irão produzir frutos com menor biomassa do que os indivíduos sem herbivoria. Variáveis Variáveis resposta e preditoras Dataframe com as indivíduos da espécie de planta (unidade amostral) nas linhas e a biomassa dos frutos (variável resposta), herbivoria (variável preditora categórica) e tamanho da raiz (covariável contínua) nas colunas. Checklist Verificar se o seu dataframe está com as unidades amostrais nas linhas e variáveis preditoras e respostas nas colunas. 7.10.1 Análise Olhando os dados usando a função head head(dados_ancova) #&gt; Raiz Biomassa Herbivoria #&gt; 1 6.225 59.77 Sem_herb #&gt; 2 6.487 60.98 Sem_herb #&gt; 3 4.919 14.73 Sem_herb #&gt; 4 5.130 19.28 Sem_herb #&gt; 5 5.417 34.25 Sem_herb #&gt; 6 5.359 35.53 Sem_herb Cálculo da ANCOVA. ## Ancova modelo_ancova &lt;- lm(Biomassa ~ Herbivoria * Raiz, data = dados_ancova) # Verificando as premissas da Anova. plot_grid(plot_model(modelo_ancova, type = &quot;diag&quot;)) As premissas da anova estão adequadas. Vamos olhar os resultados do modelo. ## Resultados do modelo anova(modelo_ancova) #&gt; Analysis of Variance Table #&gt; #&gt; Response: Biomassa #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Herbivoria 1 1941.9 1941.9 35.101 8.764e-07 *** #&gt; Raiz 1 17434.1 17434.1 315.124 &lt; 2.2e-16 *** #&gt; Herbivoria:Raiz 1 136.7 136.7 2.471 0.1247 #&gt; Residuals 36 1991.7 55.3 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Percebam que o resultado da ANCOVA (Pr(&gt;F) &lt; 0.001) indica que tanto a herbivoria como o tamanho da raiz (covariável) têm efeitos significativos na biomassa dos frutos. Contudo, a interação entre as variáveis não foi signigicativa. Vamos usar o Likelihood ratio test (LRT) para ver se podemos seguir com um modelo mais simples (sem interação). ## Criando modelo sem interação modelo_ancova2 &lt;- lm(Biomassa ~ Herbivoria + Raiz, data = dados_ancova) ## Likelihood Rate Test lrtest(modelo_ancova, modelo_ancova2) #&gt; Likelihood ratio test #&gt; #&gt; Model 1: Biomassa ~ Herbivoria * Raiz #&gt; Model 2: Biomassa ~ Herbivoria + Raiz #&gt; #Df LogLik Df Chisq Pr(&gt;Chisq) #&gt; 1 5 -134.91 #&gt; 2 4 -136.24 -1 2.6554 0.1032 A interação não é importante. Seguiremos com o modelo mais simples. Visualizar os resultados em gráfico. ## Gráfico ggplot(data = dados_ancova, aes(x = Raiz, y = Biomassa, fill = Herbivoria)) + labs(x = &quot;Tamanho da raiz (cm)&quot;, y = &quot;Biomassa dos frutos (g)&quot;) + geom_point(size = 4, shape = 21, alpha = 0.7) + tema_livro() + scale_colour_manual(values = c(&quot;darkorange&quot;, &quot;cyan4&quot;)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;cyan4&quot;), labels = c(&quot;Com herbivoria&quot;, &quot;Sem herbivoria&quot;)) + geom_smooth(aes(color = Herbivoria), method = &quot;lm&quot;, show.legend = FALSE) Interpretação dos resultados Neste exemplo, o tamanho da raiz (covariável) tem uma relação positiva com a biomassa dos frutos. Quanto maior o tamanho da raiz, maior a biomassa dos frutos. Usando a ANCOVA e controlando o efeito da covariável, percebemos que a herbivoria também afeta a biomassa dos frutos. Os indivíduos com mesmo tamanho de raiz que não sofreram herbivoria produziram frutos com maior biomassa do que os indivíduos com herbivoria. 7.10.2 Para se aprofundar Recomendamos aos interessados os livros: i) Zar (2010) Biostatiscal analysis; ii) Gotelli &amp; Ellison (2012) A primer of ecological statistics; e iii) Quinn &amp; Keough (2002) Experimental design and data analysis for biologists. Referências "],["cap8.html", "Capítulo 8 Cap. 8 - Análises univariadas (modelos lineares mistos generalizados) 8.1 Introdução 8.2 Como um GLM funciona? 8.3 Como escolher a distribuição correta para seus dados? 8.4 Dados de contagem: a distribuição de Poisson 8.5 Dados de contagem: modelos quasi-likelihood 8.6 Dados de contagem: a distribuição Binomial 8.7 Análise com dados de incidência 8.8 Dados de contagem com excesso de zeros 8.9 Dados ordinais: os modelos cumulative link 8.10 Dados contínuos: distribuição beta 8.11 Leituras recomendadas", " Capítulo 8 Cap. 8 - Análises univariadas (modelos lineares mistos generalizados) Pré-requisitos do capítulo library(ecodados) library(visdat) library(tidyverse) library(lattice) library(RVAideMemoire) library(DHARMa) library(performance) library(MuMIn) library(piecewiseSEM) library(MASS) library(ggExtra) library(sciplot) library(emmeans) library(sjPlot) library(bbmle) library(glmmTMB) library(ordinal) library(car) 8.1 Introdução No capítulo anterior descrevemos sobre os modelos lineares (também chamados de Modelos Lineares Gerais) que podem ser descritos pelo mesmo modelo matemático de uma equação da reta do tipo:: Yi = a + b*xi + erro no qual o que difere uma regressão linear de uma análise de variância é a natureza do elemento xi, variável contínua para regressão, variável categórica no caso da ANOVA (que é codificada numa matriz design para desenhos mais complexos). Nesse sentido, o que todos esses métodos têm em comum é a variável resposta Y que é um vetor numérico contínuo. Outro elemento em comum desses métodos é a distribuição de frequência do erro. Se quiser mais detalhes como sobre modelos lineares podem ser escritos na forma de matrizes, consulte a introdução de (Fox, Negrete-Yankelevich, and Sosa 2015). Todos os modelos lineares assumem que a distribuição do erro seja Gaussiana (ou Normal). Isso de certa forma limita o tipo de dado que pode ser usado como variável resposta por estas análises. Por exemplo, dados de contagem (e.g., riqueza e abundância de espécies), frequência (e.g., frequência de ocorrência, porcentagem de cobertura vegetal), incidência (e.g., presença ou ausência de uma espécie) ou proporção (e.g., números de animais infectados a cada 1000 animais) não são adequados para serem utilizados como variáveis resposta em modelos lineares. Uma prática comum quando nossos dados não são Normais é transformar por log ou raiz quadrada. No entanto, para dados de contagem isso não é recomendado (veja (OHara and Kotze 2010), (Ives 2015), (Warton 2018)). Nestes casos devemos recorrer a um conjunto de modelos chamados Modelos Lineares Generalizados (GLM). Nestes modelos, o usuário especifica a distribuição de frequência que deseja utilizar para modelar a variável resposta. Esta distribuição de frequência deve pertencer à família exponencial, que inclui a distribuição de Poisson, Gaussiana, Binomial, Binomial Negativa, Gamma, Bernoulli e Beta. Ainda é possível utilizar Cumulative Link Models para modelar dados ordinais (fatores cuja ordem dos elementos importa, tais como muito baixo, baixo, alto e muito alto). Abaixo vamos ver um pouco sobre como um GLM funciona e exemplos com cada uma destas distribuições. 8.2 Como um GLM funciona? Diferentemente do modelo linear, um GLM estima os parâmetros por meio de Máxima Verossimilhança (ML) ao invés dos mínimos quadrados comuns (OLS). Portanto, um GLM relaciona a distribuição da variável resposta aos preditores lineares por meio de uma função de ligação. Por exemplo, no caso da distribuição de Poisson usa-se uma ligação logarítmica (também chamada de log link) que garante que o valores ajustados são sempre não negativos. Portanto, um GLM é composto por esses 3 componentes: função de distribuição, preditor linear e função de ligação. A função de distribuição é uma hipótese sobre a distribuição da variável resposta Yi. Isso também define a média e a variância de Yi. Já a função de ligação define a relação entre o valor médio de Yi e da parte sistemática. Esta é também chamada de ligação entre a média e a parte sistemática do modelo. Existem três tipos de função de ligação: Identity link, que é definido por g(µ)= , e modela a média ou valor esperado de Y. Usado em modelos lineares padrão. Log link, que é g()=log(), e modela o log da média. É usado para dados de contagem (que não podem assumir valores negativos) em modelos log-linear Logit link, que é g()=log[ /(1- )], e é usado para dados binários e regressão logística Logo, um modelo linear pode ser visto como um caso particular de um GLM em que utiliza distribuição Gaussiana, com identity link 8.3 Como escolher a distribuição correta para seus dados? 8.3.1 Para dados contínuos Se Y é uma variável contínua, a sua distribuição de probabilidade deve ser normal. Nesses casos as distribuições recomendadas são a Gaussiana (Normal) ou Gamma. Para essas distribuições, o parâmetro de dispersão é estimado separadamente da média e é às vezes, chamado de nuisance parameter. Uma particularidade da distribuição Gamma é que ela só aceita valores contínuos positivos. 8.3.2 Para dados de contagem Se Y é binário (e.g., vivo ou morto), a distribuição de probabilidade deve ser binomial. Se Y é uma contagem (e.g., abundância ou riqueza de espécies), então a distribuição de probabilidade deve ser Poisson ou Binomial Negativa. Existem também correções dessas distribuições quando apresentam sobredispersão, tais como quasi-Poisson ou quasi-Negative binomial. Falaremos delas no momento certo. Para distribuições tais como binomial e Poisson, a variância deve ser igual à media e o parâmetro de dispersão é sempre 1. Na maioria dos dados ecológicos esse pressuposto não é cumprido, veremos estratégias para lidar com isso logo à frente. As funções Ord_plot e goodfit do pacote vcd podem auxiliar na escolha da distribuição para dados de contagem. 8.4 Dados de contagem: a distribuição de Poisson Para casos em que estamos interessados em quantificar uma variável discreta, ou seja, uma variável positiva, representada sempre por números inteiros, contendo um número finito de possibilidades, devemos utilizar a distribuição de Poisson. Esta distribuição é peculiar por ser descrita apenas por um parâmetro livre (\\(\\lambda\\)). Isso quer dizer que tanto a média quanto a variância dos dados são descritos por um único parâmetro, o que implica em dizer que a média e a variância têm de ser iguais. Vamos ver um exemplo com dados reais. 8.4.0.1 Exemplo 1 Explicação dos dados Neste exemplo iremos utilizar dados de riqueza de anfíbios anuros coletados em 40 poças, açudes e brejos ao redor de fragmentos florestais no Noroeste Paulista (Prado and Rossa-Feres 2014). Os autores mediram seis variáveis em escala local e outras três em escala de paisagem. Pergunta A distância linear para o corpo dágua mais próximo influencia a abundância total de espécies de anuros? Predições Corpos dágua mais conectados permitem que indivíduos dispersem entre eles com maior facilidade, suportando melhor dinâmicas de metapopulações. Portanto, espero que poças que estejam mais conectadas entre si tenham maior riqueza total de sapos. Variáveis  Variável reposta: riqueza de sapos em 40 poças.  Variável preditora: distância da poça focal para a mais próxima na escala da paisagem Checklist  Verificar se o seu dataframe está com as unidades amostrais nas linhas (neste caso poças) e variáveis nas colunas. Antes de começar com a análise, vamos primeiro explorar os dados. head(fragmentos) #&gt; locality site Riqueza_obs Riqueza_HB Bsc Dne Dnm Dnn Dns Hal Hra Lfu Lla Lpo Eun Pce Pcu Pfa1 #&gt; 1 MAC MacAc1 3 6 0 0 0 0 0 0 0 0 0 0 1 0 0 0 #&gt; 2 MAC MacAc2 11 13 1 0 1 1 0 0 1 1 0 1 0 0 1 1 #&gt; 3 MAC MacAc3 10 12 1 0 0 0 0 0 1 1 0 1 1 0 1 1 #&gt; 4 MAC MacAc4 10 13 1 1 1 1 0 1 1 0 0 1 0 0 1 1 #&gt; 5 MAC MacAc5 3 6 0 0 0 0 0 0 0 1 0 0 1 0 1 0 #&gt; 6 MAC MacBr1 9 12 0 0 0 1 0 1 1 1 1 0 1 0 1 1 #&gt; Ppa Sfm Sfv Ebi Esp hydrop hydrop2 vegcov nveg fish area area2 #&gt; 1 0 0 1 0 1 -2.553590 -2.23573 -1.461851 -1.965130 -1.508310 -2.418270 -1.884470 #&gt; 2 1 1 0 1 0 0.573255 0.60630 -1.145775 -0.158114 0.646419 0.147353 0.019560 #&gt; 3 0 1 1 0 0 0.573255 0.60630 -0.987737 -1.061622 -1.508310 -0.564022 -0.699829 #&gt; 4 0 1 0 0 0 0.573255 0.60630 0.908718 -0.158114 0.646419 -0.348279 -0.497176 #&gt; 5 0 0 0 0 0 -2.553590 -2.23573 -1.461851 -1.965130 -1.508310 -2.315159 -1.844802 #&gt; 6 0 0 0 0 1 0.573255 0.60630 1.382832 -0.158114 -1.508310 -0.601947 -0.734057 #&gt; depth forcov forcov2 forcov10 dfrag dfrag2 dwater dwater2 X #&gt; 1 -1.232668 -0.604596 -0.672774 -6.045965 0.410084 0.166782 1.198175 1.166645 -49.9376 #&gt; 2 0.821168 -0.020849 -0.152952 -0.208489 -0.097045 -0.381401 0.970207 0.864035 -49.9353 #&gt; 3 -0.704539 -0.013816 -0.146124 -0.138159 -1.242271 -1.059858 -0.121245 -0.299232 -49.9348 #&gt; 4 0.821168 -0.171663 -0.296136 -1.716633 -1.242271 -1.059858 -0.087507 -0.270350 -49.9334 #&gt; 5 -1.306019 0.203364 0.071358 2.033643 -0.471888 -0.688845 0.162610 -0.042156 -49.9270 #&gt; 6 -1.306019 0.203364 0.071358 2.033643 -1.242271 -1.059858 -0.121245 -0.299232 -49.9271 #&gt; Y #&gt; 1 -20.7408 #&gt; 2 -20.7410 #&gt; 3 -20.7419 #&gt; 4 -20.7462 #&gt; 5 -20.7453 #&gt; 6 -20.7451 glimpse(fragmentos) #&gt; Rows: 40 #&gt; Columns: 40 #&gt; $ locality &lt;chr&gt; &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC&quot;, &quot;NOV&quot;, &quot;NOV&quot;, &quot;NOV&quot;, &quot;NOV&quot;, &quot;PIN&quot;, ~ #&gt; $ site &lt;chr&gt; &quot;MacAc1&quot;, &quot;MacAc2&quot;, &quot;MacAc3&quot;, &quot;MacAc4&quot;, &quot;MacAc5&quot;, &quot;MacBr1&quot;, &quot;NovBr1&quot;, &quot;NovBr2~ #&gt; $ Riqueza_obs &lt;int&gt; 3, 11, 10, 10, 3, 9, 2, 8, 9, 8, 6, 4, 8, 8, 6, 17, 15, 13, 8, 10, 12, 14, 14~ #&gt; $ Riqueza_HB &lt;int&gt; 6, 13, 12, 13, 6, 12, 5, 11, 12, 11, 8, 7, 10, 11, 9, 18, 17, 15, 11, 13, 14,~ #&gt; $ Bsc &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,~ #&gt; $ Dne &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,~ #&gt; $ Dnm &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,~ #&gt; $ Dnn &lt;int&gt; 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~ #&gt; $ Dns &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,~ #&gt; $ Hal &lt;int&gt; 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,~ #&gt; $ Hra &lt;int&gt; 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,~ #&gt; $ Lfu &lt;int&gt; 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~ #&gt; $ Lla &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,~ #&gt; $ Lpo &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,~ #&gt; $ Eun &lt;int&gt; 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,~ #&gt; $ Pce &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,~ #&gt; $ Pcu &lt;int&gt; 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~ #&gt; $ Pfa1 &lt;int&gt; 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,~ #&gt; $ Ppa &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,~ #&gt; $ Sfm &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,~ #&gt; $ Sfv &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,~ #&gt; $ Ebi &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,~ #&gt; $ Esp &lt;int&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,~ #&gt; $ hydrop &lt;dbl&gt; -2.553590, 0.573255, 0.573255, 0.573255, -2.553590, 0.573255, 0.573255, 0.573~ #&gt; $ hydrop2 &lt;dbl&gt; -2.235730, 0.606300, 0.606300, 0.606300, -2.235730, 0.606300, 0.606300, 0.606~ #&gt; $ vegcov &lt;dbl&gt; -1.461851, -1.145775, -0.987737, 0.908718, -1.461851, 1.382832, 1.382832, 1.6~ #&gt; $ nveg &lt;dbl&gt; -1.965130, -0.158114, -1.061622, -0.158114, -1.965130, -0.158114, 0.745394, 1~ #&gt; $ fish &lt;dbl&gt; -1.508310, 0.646419, -1.508310, 0.646419, -1.508310, -1.508310, 0.646419, 0.6~ #&gt; $ area &lt;dbl&gt; -2.418270, 0.147353, -0.564022, -0.348279, -2.315159, -0.601947, 1.556190, -0~ #&gt; $ area2 &lt;dbl&gt; -1.884470, 0.019560, -0.699829, -0.497176, -1.844802, -0.734057, 1.877820, -0~ #&gt; $ depth &lt;dbl&gt; -1.232668, 0.821168, -0.704539, 0.821168, -1.306019, -1.306019, -0.645858, -0~ #&gt; $ forcov &lt;dbl&gt; -0.604596, -0.020849, -0.013816, -0.171663, 0.203364, 0.203364, 0.562496, -0.~ #&gt; $ forcov2 &lt;dbl&gt; -0.672774, -0.152952, -0.146124, -0.296136, 0.071358, 0.071358, 0.459151, -0.~ #&gt; $ forcov10 &lt;dbl&gt; -6.045965, -0.208489, -0.138159, -1.716633, 2.033643, 2.033643, 5.624958, -1.~ #&gt; $ dfrag &lt;dbl&gt; 0.410084, -0.097045, -1.242271, -1.242271, -0.471888, -1.242271, 1.307931, 1.~ #&gt; $ dfrag2 &lt;dbl&gt; 0.166782, -0.381401, -1.059858, -1.059858, -0.688845, -1.059858, 1.510271, 1.~ #&gt; $ dwater &lt;dbl&gt; 1.198175, 0.970207, -0.121245, -0.087507, 0.162610, -0.121245, -0.087507, -1.~ #&gt; $ dwater2 &lt;dbl&gt; 1.166645, 0.864035, -0.299232, -0.270350, -0.042156, -0.299232, -0.270350, -1~ #&gt; $ X &lt;dbl&gt; -49.9376, -49.9353, -49.9348, -49.9334, -49.9270, -49.9271, -49.2742, -49.330~ #&gt; $ Y &lt;dbl&gt; -20.7408, -20.7410, -20.7419, -20.7462, -20.7453, -20.7451, -21.5187, -21.528~ Percebam que o data frame contém 40 colunas. Neste conjunto de dados as variáveis preditoras já estão padronizadas com média 0 e desvio padrão 1. As variáveis com 2 indicam variáveis quadráticas (podem ser usadas para se testar relações não lineares). Também temos a riqueza observada e a estimada (Riqueza_HB) e as coordenadas geográficas (X e Y). Vamos agora explorar os dados e ver como é a relação entre riqueza e distância para a poça mais próxima. Sempre é recomendado visualizar os dados antes de efetivamente os modelar para se ter uma idéia da relação entre as variáveis: # ------------------------------------------------------------------------- ggplot(fragmentos, aes(dfrag, Riqueza_obs))+ geom_point(size=4, alpha = 0.7)+ stat_smooth(method = &quot;lm&quot;) Aqui vemos que há de fato uma relação linear positiva entre as duas variáveis. A partir de agora vamos sempre usar uma mesma estrutura para realizar nossos exercícios de modelagem: Primeiro vamos especificar o modelo; Depois realizar a diagnose; Por último realizar inferência a partir do nosso modelo. 8.4.0.1.1 Modelagem O primeiro argumento da função glmé uma fórmula, em que na parte esquerda temos a variável resposta seguida do símbolo ~ (lê-se: modelado em função de) seguido pelas variáveis preditoras. Aqui podemos usar uma ou mais variáveis e testar o seu efeito aditivo (usando o sinal de +) ou a interação entre elas (usando o sinal de *). Um bom resumo sobre como especificar o seu modelo pode ser encontrada aqui neste blog. Aqui optamos por um modelo bem simples modelando a riqueza de anfíbios apenas em função da distância para o fragmento mais próximo. mod_pois &lt;- glm(Riqueza_obs~dfrag, family = poisson(link = &quot;log&quot;), data = fragmentos) Assim como modelos lineares que vimos no Capítulo 6, GLMs com distribuição de Poisson requerem que se teste os pressupostos, incluindo sobredispersão e inflação de zeros. 8.4.0.1.2 Diagnose básica dos resíduos do modelo Iremos realizar três diagnoses básicas dos GLMs, avaliando diferentes aspectos do modelo: Heterogeneidade da variância e normalidade dos resíduos Overdispersion Zero-inflation Vamos começar avaliando as heterogeneidade da variância e normalidade dos resíduos: plotresid(mod_pois, shapiro = TRUE)#SÓ O PLOT DE RESÍDUOS par(mfrow=c(2,2)) plot(mod_pois)#TODOS OS 4 PLOTS par(mfrow=c(1,1)) Aqui vemos quatro gráficos. Na primeira coluna temos dois gráficos dos valores preditos (brutos ou padronizados pela raiz quadrada) contra os resíduos. Eles medem desvio em relação à homogeneidade de variância. Os quatro gráficos não devem ter nenhum padrão aparente, ou seja, os pontos devem cair em cima da linha pontilhada. Neste caso, vemos que as linhas vermelhas (que indicam a tendência dos dados) estão praticamente retas seguindo a linha pontilhada, sugerindo que não exista heterogeneidade de variância dos resíduos. O gráfico superior direito é o plot de quantis que mede desvios da normalidade. No gráfico inferior direito, os valores extremos são todos aqueles que estejam a mais de uma unidade da distância de Cook (linha pontilhada vermelha). Também não temos problemas com esse pressuposto do modelo aqui. Vemos que nos quatro plots alguns dados, 1, 7 e 30 (referem-se às linhas do data.frame) aparecem identificados, pois apresentam ligeiro desvio da normalidade e estão distantes da média. No entanto, não é algo para nos preocuparmos pois não são valores muito extremos. Portanto, a diagnose indicou que o modelo com Poisson parece ser adequado para modelar estes dados, ao menos em termos de homogeneidade de variância. 8.4.0.1.3 Diagnose avançada Alguns pacotes permitem calcular outros aspectos do modelo que facilitam a diagnose, ou seja, se podemos de fato confiar nos parâmetros estimados por eles, incluindo valores de significância. Um pressuposto importante dos modelos de contagem (incluindo Poisson) é a overdispersion (sobredispersão). Vejamos como o pacote DHARMa funciona: simulationOutput &lt;- simulateResiduals(fittedModel = mod_pois, plot = TRUE) O plot claramente indica que há problema com overdispersion, mas não em termos de desvios de normalidade (KS test) ou outlier, já que apenas o primeiro foi significativo (aparece em vermelho). 8.4.0.1.4 Detectando e lidando com overdispersion O que é sobredispersão (ou overdispersion)? Ela ocorre quando a variância observada é muito maior do que aquela predita pelo modelo. Para modelos que utilizam a distribuição de Poisson, isso ocorre quando a variância aumenta com a média. Lembre-se de que esta distribuição tem apenas um único parâmetro para descrever tanto a média quanto a variância (\\(\\lambda\\)). Portanto, a variância tem de ser igual à média. No entanto, se a variância nos dados observados for muito maior do que a média, dizemos que há sobredispersão nos dados. Existem duas formas de diagnosticar overdispersion que estão implementadas na maioria dos pacotes. Aqui vamos demonstrá-las usando as funções check_overdispersion e testDispersion disponíveis nos pacotes performance e DHARMa, respectivamente. A função testDispersion do DHARMa utiliza um método de aleatorização dos resíduos para determinar se há overdispersion nos dados, cuja vantagem é que aborda diretamente a variação nos dados, ao invés de medir o ajuste do modelo em si, com outros testes. par(mfrow=c(1,1)) testDispersion(mod_pois)#modelo tem overdispersion #&gt; #&gt; DHARMa nonparametric dispersion test via sd of residuals fitted vs. simulated #&gt; #&gt; data: simulationOutput #&gt; dispersion = 1.6489, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: two.sided Aqui temos um gráfico e o resultado novamente do teste de overdispersion (que já aparecia no gráfico anterior) mostrando que de fato há overdispersion: perceba que o valor de P é significativo. O gráfico nos motra em preto a distribuição dos resíduos aleatorizados e a linha vermelha o valor observado da estatística. Já que a linha está bem à direita da distribuição, isso indica overdispersion, se estivese à esquerda seria o caso de underdispersion. Agora vamos utilizar a função check_overdisperion que utiliza uma distribuição qui-quadradado e o valor de dispersion ratio para testar a presença de overdispersion no modelo. Esse teste também pode ser feito com a função acima ao se especificar o argumento type=\"PearsonChisq\" check_overdispersion(mod_pois)#modelo tem overdispersion #&gt; # Overdispersion test #&gt; #&gt; dispersion ratio = 1.657 #&gt; Pearson&#39;s Chi-Squared = 62.951 #&gt; p-value = 0.007 Quando este resultado é significativo, como vimos na última linha acima, isso indica overdispersion. summary(mod_pois) #&gt; #&gt; Call: #&gt; glm(formula = Riqueza_obs ~ dfrag, family = poisson(link = &quot;log&quot;), #&gt; data = fragmentos) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.3467 -0.9110 0.0942 0.8336 2.2773 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 2.3051 0.0500 46.101 &lt;2e-16 *** #&gt; dfrag 0.0718 0.0507 1.416 0.157 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 70.868 on 39 degrees of freedom #&gt; Residual deviance: 68.856 on 38 degrees of freedom #&gt; AIC: 235.29 #&gt; #&gt; Number of Fisher Scoring iterations: 4 Na parte de baixo do output da função summary também podemos calcular o dispersion parameter dividindo o residual deviance pelos graus de liberdade dos resíduos. Esta é outra maneira fácil e rápida de detectar overdispersion. Neste exemplo temos que Dispersion parameter = 1.8119903. Quando esse valor é próximo de 1 isso sugere que não há overdispersion. No entanto, se ele for maior que 1.5 isso sugere que o modelo sofre de overdispersion e que devemos usar outra distribuição, tal como a distribuição binomial negativa. Além disso, uma outra forma de diagnosticar o modelo podemos é calcular os resíduos de Pearson (resíduos normalizados), que é basicamente a raiz quadrada da variância da variável resposta. 8.4.0.1.5 Inflação de zeros Qualquer das formas mostradas acima de diagnosticar overdispersion pode ser usada na maioria das vezes, com exceção de dados com muitos zeros (pouca variância). Por isso devemos também testar se o nosso modelo sofre de inflação de zeros. Vejamos como isso funciona usando as funções check_zeroinflation no pacote performanace e testZeroInflation no pacote DHARMa: check_zeroinflation(mod_pois)#para diagnosticar se o modelo sofre de zero inflation #&gt; Model has no observed zeros in the response variable. #&gt; NULL e no DHARMa testZeroInflation(mod_pois) # para testar se existe zero inflation #&gt; #&gt; DHARMa zero-inflation test via comparison to expected zeros with simulation under H0 = #&gt; fitted model #&gt; #&gt; data: simulationOutput #&gt; ratioObsSim = NaN, p-value = 1 #&gt; alternative hypothesis: two.sided Tanto a função do DHARMa quanto do performance conseguiram detectar que o modelo tem problemas com overdispersion, ou sobre dispersão, mas isso não é causado pelo excesso de zeros. Como já dissemos acima, no caso da distribuição Poisson, tanto a média quanto a variância são modeladas pelo mesmo parâmetro (\\(\\lambda\\)). Isso faz com que esta distribuição não seja muito útil para modelar dados de contagem em que haja muita variância em torno da média. Esse infelizmente é o caso da grande maioria dos dados ecológicos. Por estes motivos não podemos fazer inferência com este modelo porque os parâmetros estimados não são confiáveis. Mas vejamos como seria feita essa inferência caso este modelo fosse adequado. 8.4.0.1.6 Inferência Aqui iremos apresentar várias funções para calcular o coeficiente de determinação (R2). No caso de GLM(M)s, não há um consenso sobre como se calcula este coeficiente, havendo várias propostas que utilizam maneiras diferentes de estimar a heterogeneidade de variância e covariância entre observações dos resíduos, veja (Nakagawa, Johnson, and Schielzeth 2017) e (Ives 2015) para maiores detalhes, assim como o help das respectivas funções. ## Coeficientes estimados pelo modelo summary(mod_pois) #&gt; #&gt; Call: #&gt; glm(formula = Riqueza_obs ~ dfrag, family = poisson(link = &quot;log&quot;), #&gt; data = fragmentos) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.3467 -0.9110 0.0942 0.8336 2.2773 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 2.3051 0.0500 46.101 &lt;2e-16 *** #&gt; dfrag 0.0718 0.0507 1.416 0.157 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 70.868 on 39 degrees of freedom #&gt; Residual deviance: 68.856 on 38 degrees of freedom #&gt; AIC: 235.29 #&gt; #&gt; Number of Fisher Scoring iterations: 4 ## Calculando o R2 do modelo r.squaredGLMM(mod_pois) #&gt; R2m R2c #&gt; delta 0.04925800 0.04925800 #&gt; lognormal 0.05154558 0.05154558 #&gt; trigamma 0.04696308 0.04696308 rsquared(mod_pois) #&gt; Response family link method R.squared #&gt; 1 Riqueza_obs poisson log nagelkerke 0.04919844 r2(mod_pois) #&gt; # R2 for Generalized Linear Regression #&gt; Nagelkerke&#39;s R2: 0.059 Podemos ver que os valores de R2 são bem baixos (em torno de 4 - 5%), independente do método que usamos pra calculá-lo. 8.4.0.1.7 Plot do modelo predito a1 &lt;- ggplot(fragmentos, aes(dfrag, Riqueza_obs))+ geom_point(cex = 4,alpha = 0.7)+ geom_smooth(method = &quot;glm&quot;, formula = y~x, method.args = list(family =&quot;poisson&quot;), se=TRUE)+ labs(x=&quot;Distância para o fragmento mais próximo&quot;, y=&quot;Riqueza observada&quot;) ggMarginal(a1, fill=&quot;red&quot;)  Importante Aqui vemos que há uma leve tendência na relação positiva entre distância para o fragmento mais próximo e a riqueza de anfíbios observada. No entanto, há uma grande dispersão nos dados ao redor da reta do modelo, fazendo com que a relação não seja de fato significativa e tenhamos um R2 bem baixo. Caso pudéssemos confiar nos parâmetros deste modelo poderíamos dizer que existe uma leve tendência a um aumento da riqueza observada de anfíbios anuros à medida que aumenta a distância da poça para o fragmento mais próximo. 8.4.1 O que causa a overdispersion? Existem dois conjuntos de causas: aparente ou real. As causas aparentes são geradas pela má especificação do modelo, tais como: 1. não inclusão de covariáveis ou interações no modelo; presença de outliers na variável resposta, efeitos não lineares da covariável (X2, X3); escolha errada da função de ligação (link function). As causas reais incluem: variância maior que a média; muitos zeros; agregação de observações; correlação entre observações (não independência). 8.4.2 O que fazer se seu modelo tiver overdispersion? Depois de tentar corrigir possíveis más especificações, como as listadas acima, existem duas alternativas: usar outra distribuição, tal como Binomial negativa caso o dispersion parameter seja maior que 15 ou 20; ou Usar um modelo com correção de erro da sobredispersão, caso 1.5 &lt; dispersion &gt; 15. Vejamos agora as características da distribuição Binomial negativa. Geralmente, dados de contagem em estudos ecológicos não seguem uma distribuição Poisson, pois há muita dispersão (variância) nos dados. Logo, o pressuposto da distribuição Poisson, i.e., de que a média e variância são descritas por um mesmo parâmetro (\\(\\lambda\\)) é quebrado. Como vimos, overdispersion (ou sobredispersão) é um problema comum ao analisar dados ecológicos e deve necessariamente ser diagnosticado no modelo. Uma maneira de lidar com esse tipo de problema é utilizar uma outra distribuição diferente da Poisson. A binomial negativa pode ser entendida como uma mistura da distibuição Poisson e Gamma, ou seja, ela aceita dados de contagem que sejam positivos, mas sem zero. A grande vantagem desta distribuição é que, diferentemente da Poisson, ela tem um parâmetro para modelar a média (\\(\\lambda\\)) e outro para modelar a variância (k). Logo, ela permite modelar dados em que a média é diferente da variância. Vejamos um exemplo. Aqui vamos continuar com estes dados para ver como o modelo se comporta com essa nova distribuição especificada. Para isso vamos utilizar a função glm.nb do pacote MASS: 8.4.2.1 Modelagem mod_nb &lt;- glm.nb(Riqueza_obs~dfrag, data = fragmentos) 8.4.2.1.1 Diagnose resíduos Assim como fizemos com o modelo com Poisson, vamos agora diagnosticar os resíduos: par(mfrow=c(2,2)) plot(mod_nb) par(mfrow=c(1,1)) (chat &lt;- deviance(mod_nb) / df.residual(mod_nb))#DISPERSION PARAMETER #&gt; [1] 1.126184 Compare estes gráficos com os do modelo anterior com distribuição Poisson. Eles são praticamente idênticos, ou seja, o modelo com Poisson já não tinha heterogeneidade de variância nem problemas com normalidade dos resíduos. Agora vejamos se o problema com overdispersion foi resolvido: simulationOutput &lt;- simulateResiduals(fittedModel = mod_nb, plot = TRUE) Na diagnose do modelo pelo DHARMa vemos que bastou mudar a distribuição de probabilidade que o problema de overdispersion foi resolvido (nenhum teste foi significativo no quadro da esquerda), e como já sabíamos, não há problemas com heterogeneidade de variância (plot da direita mostrando a tendência entre o predito e resíduos pra cada quantil), nem de outliers. O dispersion parameter é mais próximo de 1 do que no modelo com Poisson. Agora sim podemos levar em conta o R2  8.4.2.1.2 Inferência rsquared(mod_nb) #&gt; Response family link method R.squared #&gt; 1 Riqueza_obs Negative Binomial(14.7068) log nagelkerke 0.02935674 que parece um pouco menor do que anteriormente. Perceba que aqui utilizamos somente uma das funções apresentadas anteriormente, já que se trata de um modelo GLM com binomial negativa, calculamos o R2 pelo método de Nagelkerke. 8.4.2.1.3 Interpretação dos resultados summary(mod_nb) #&gt; #&gt; Call: #&gt; glm.nb(formula = Riqueza_obs ~ dfrag, data = fragmentos, init.theta = 14.70679964, #&gt; link = log) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.7569 -0.7068 0.0694 0.6194 1.6546 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 2.30504 0.06481 35.567 &lt;2e-16 *** #&gt; dfrag 0.07248 0.06571 1.103 0.27 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for Negative Binomial(14.7068) family taken to be 1) #&gt; #&gt; Null deviance: 44.002 on 39 degrees of freedom #&gt; Residual deviance: 42.795 on 38 degrees of freedom #&gt; AIC: 231.68 #&gt; #&gt; Number of Fisher Scoring iterations: 1 #&gt; #&gt; #&gt; Theta: 14.71 #&gt; Std. Err.: 8.62 #&gt; #&gt; 2 x log-likelihood: -225.68  Importante Aqui vemos que o resultado em termos de valor de P não mudou, ou seja, a distância par ao fragmento mais próximo não foi significativo. Mas vejam que o coeficiente (slope) mudou um pouco, antes era 0.0718 (SE=0.0507) e com binomial negativa passa a ser 0.07248 (SE=0.06571). 8.4.2.1.4 Plot do modelo predito ggplot(fragmentos, aes(dfrag, Riqueza_obs))+ geom_point(size=4, alpha=0.7)+ geom_smooth(method = &quot;glm.nb&quot;, formula = y~x, se=TRUE)+ labs(x=&quot;Distância para o fragmento mais próximo&quot;, y=&quot;Riqueza observada&quot;) Aqui vemos que a reta predita pelo modelo é muito similar ao que tivemos com o Poisson. No entanto, agora que sabemos que este modelo com binomial negativa foi corretamente especificado e podemos confiar nos parâmetros estimados. 8.5 Dados de contagem: modelos quasi-likelihood Como dissemos acima, uma outra alternativa para ajustar modelos GLM a dados de contagem são os chamados quasi-likelihood, tais como quasi-Poisson e quasi-binomial. Dependendo do valor do dispersion parameter, pode ser útil escolher este tipo de modelo. No entanto, eles vêm com uma desvantagem: não é possível calcular o valor de Akaike Information Criterion (AIC) porque estes modelos não retornam um valor de likelihood (verosimilhança). Este parâmetro é comumente utilizado em abordagens estatísticas de teoria da informação para selecionar o melhor modelo que se ajusta aos dados. Neste caso, precisamos utilizar outras funções disponíveis nos pacotes MuMIn, AICcmodavg, e bbmle para calcular o QAIC. Para mais detalhes sobre esses modelos, veja o vignette sobre o assunto do pacote bbmle. 8.5.0.1 Análise Aqui vamos apenas exemplificar como um modelo com distribuição quasi-poisson pode ser especificado. mod_quasipois &lt;- glm(Riqueza_obs~dfrag, family = quasipoisson(link = &quot;log&quot;), data = fragmentos) 8.5.0.1.1 Diagnose dos resíduos A função resid não leva em conta a sobredispersão e temos de calcular manualmente o parâmetro de dispersão e inclui-lo no plot. Portanto, não podemos realizar a diagnose de modelos quasi-Poisson apenas com a função plot como fazíamos até então. Então, calculamos primeiramente os resíduos de Pearson e depois dividindo-o pela raiz quadrada do parâmetro de dispersão, veja abaixo: EP &lt;- resid(mod_quasipois, type = &quot;pearson&quot;) ED &lt;- resid(mod_quasipois, type = &quot;deviance&quot;) mu &lt;- predict(mod_quasipois, type = &quot;response&quot;) E &lt;- fragmentos$Riqueza_obs - mu EP2 &lt;- E / sqrt(1.65662 * mu)#dispersion parameter da quasipoisson op &lt;- par(mfrow = c(2, 2)) plot(x = mu, y = E, main = &quot;Response residuals&quot;) plot(x = mu, y = EP, main = &quot;Pearson residuals&quot;) plot(x = mu, y = EP2, main = &quot;Pearson residuals scaled&quot;) plot(x = mu, y = ED, main = &quot;Deviance residuals&quot;) par(op) par(mfrow=c(1,1)) Aqui vemos que não existe um padrão claro nos resíduos, muito similar ao que tínhamos anteriormente. Devido às limitações de distribuições quasi e dado que já temos um modelo adequado com binomial negativa, sugerimos interpretar apenas o modelo anterior com binomial negativa. 8.6 Dados de contagem: a distribuição Binomial Quando temos dados de proporção (e.g., número de doentes por 1000 habitantes) ou incidência (i.e., presença ou ausência), a distribuição mais adequada para modelar os dados é a distribuição binomial. No entanto, temos que especificar o modelo de acordo com o tipo dos dados no argumento formula. Vejamos dois exemplos: 8.6.1 Análise com dados de proporção Neste exemplo vamos ver como podemos modelar a proporção de células sanguíneas em função do tipo de tratamento. Explicação dos dados Este conjunto de dados foi coletado por (Franco-Belussi, De Oliveira, and Sköld 2018). Os autores utilizaram um desenho experimental típico de uma 2x5 ANOVA fatorial (ou two-way ANOVA) em que temos dois tratamentos (fatores): pigmentação do girino com dois níveis (Yes e No) e Tempo de exposição com cinco níveis (controle sem UV, 6 h, 12 h, 18 h e 24 h de exposição à UV). Pergunta A melanina proteje girinos contra os efeitos da radiação ultravioleta? Predições Como a melanina participa do sistema imune inato, ela desempenharia um papel na resposta do organismo à radiação UV, auxiliando as células imunes a combater os seus efeitos deletérios. Variáveis  Variável resposta: Contagem diferencial de eosinófilos  Dataframe com 10 girinos em cada tratamento, totalizando 50 girinos glimpse(uv_cells) #&gt; Rows: 50 #&gt; Columns: 8 #&gt; $ UV &lt;chr&gt; &quot;1.CT&quot;, &quot;1.CT&quot;, &quot;1.CT&quot;, &quot;1.CT&quot;, &quot;1.CT&quot;, &quot;2.6h&quot;, &quot;2.6h&quot;, &quot;2.6h&quot;, &quot;2.6h&quot;, &quot;2.6~ #&gt; $ Pigmentation &lt;chr&gt; &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;,~ #&gt; $ Total_Cell &lt;int&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1~ #&gt; $ Lymphocyte &lt;int&gt; 80, 74, 78, 87, 74, 95, 73, 77, 61, 81, 90, 80, 92, 76, 51, 62, 88, 59, 51, ~ #&gt; $ Neutrophil &lt;int&gt; 18, 17, 22, 13, 21, 4, 16, 20, 27, 4, 7, 17, 5, 21, 44, 27, 4, 41, 21, 25, 4~ #&gt; $ Basophil &lt;int&gt; 0, 6, 0, 0, 1, 0, 9, 4, 11, 0, 0, 0, 3, 3, 1, 5, 3, 0, 0, 21, 0, 3, 0, 0, 0,~ #&gt; $ Monocyte &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 5, 3, 0, 14, 1, 5, 0, 0, 1, 0, ~ #&gt; $ Eosinophil &lt;int&gt; 2, 3, 0, 0, 4, 1, 2, 0, 1, 3, 3, 3, 0, 0, 3, 1, 2, 0, 14, 0, 6, 1, 1, 1, 3, ~ Vamos explorar os dados para tentar entender como são as relações: lineplot.CI(UV, Eosinophil, Pigmentation, data=uv_cells) Aqui vemos que a quantidade de eosinófilos é muito maior nos girinos sem pigmentação (albinos). Já que estes animais não têm pigmentação melânica, as células brancas do sangue são a única ferramenta de combate aos efeitos deletérios da UV. 8.6.1.1 Modelagem Aqui vamos usar o cbind no argumento formula para dizer que queremos modelar a contagem de eosinófilos em relação ao número total de células, ou seja, sua proporção. Aqui temos a contagem do número de eusinófilos (um tipo de célula da série branca do sangue) em lâminas histológicas de girinos da rã-touro (Lithobates catesbeianus) num total de 1000 células: mod1&lt;-glm(cbind(Eosinophil, Total_Cell)~UV*Pigmentation, family=binomial, data=uv_cells) 8.6.1.2 Diagnose básica dos resíduos do modelo par(mfrow=c(2,2)) plot(mod1) par(mfrow=c(1,1)) Parece que os resíduos não sofrem de heterogeneidade de variância (linha vermelha está reta), mas parece haver um pequeno desvio da normalidade (veja pontos 19, 29 e 32 destacados no plot de quantis e no de outliers). Vejamos o que o DHARMa nos diz: simulationBion &lt;- simulateResiduals(fittedModel = mod1, plot = TRUE) binned_residuals(mod1) #&gt; Warning: Probably bad model fit. Only about 29% of the residuals are inside the error bounds. Aqui já não resta dúvidas de que os resíduos deste modelo sofrem tanto com heterogeneidade de variância, quanto overdispersion e problemas com outliers. Provavelmente o problema com outliers ocorreu por conta do pequeno tamanho amostral. 8.6.1.3 Inferência Sabemos que o modelo não parece ser adequado para os dados, mas vamos interpretá-lo mesmo assim para que possamos entender o output do summary e os contrastes entre os níveis dos fatores: summary(mod1) #&gt; #&gt; Call: #&gt; glm(formula = cbind(Eosinophil, Total_Cell) ~ UV * Pigmentation, #&gt; family = binomial, data = uv_cells) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -5.4165 -2.5266 -1.0148 0.8068 8.8233 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.84516 0.12107 -15.241 &lt; 2e-16 *** #&gt; UV2.6h -0.17979 0.17835 -1.008 0.3134 #&gt; UV3.12h 0.38414 0.15899 2.416 0.0157 * #&gt; UV4.18h -0.49825 0.19363 -2.573 0.0101 * #&gt; UV5.24h -0.39916 0.18848 -2.118 0.0342 * #&gt; PigmentationYes -2.17222 0.35745 -6.077 1.22e-09 *** #&gt; UV2.6h:PigmentationYes -0.07152 0.53831 -0.133 0.8943 #&gt; UV3.12h:PigmentationYes -0.38414 0.50150 -0.766 0.4437 #&gt; UV4.18h:PigmentationYes 1.13424 0.45981 2.467 0.0136 * #&gt; UV5.24h:PigmentationYes 0.68684 0.48370 1.420 0.1556 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 737.36 on 49 degrees of freedom #&gt; Residual deviance: 460.85 on 40 degrees of freedom #&gt; AIC: 610.35 #&gt; #&gt; Number of Fisher Scoring iterations: 5 anova(mod1) #&gt; Analysis of Deviance Table #&gt; #&gt; Model: binomial, link: logit #&gt; #&gt; Response: cbind(Eosinophil, Total_Cell) #&gt; #&gt; Terms added sequentially (first to last) #&gt; #&gt; #&gt; Df Deviance Resid. Df Resid. Dev #&gt; NULL 49 737.36 #&gt; UV 4 26.034 45 711.32 #&gt; Pigmentation 1 235.682 44 475.64 #&gt; UV:Pigmentation 4 14.789 40 460.85 Aqui temos tanto a tabela com os resultados por níveis dos fatores (summary) quanto a tabela com a Deviance que mostra os fatores e suas interações (anova). Vemos que nenhum fator foi significativo. Caso houvesse algum fator significativo poderíamos testar a significância de cada nível dos fatores usando contrastes, desta forma: pairs(emmeans(mod1, ~ UV|Pigmentation)) #&gt; Pigmentation = No: #&gt; contrast estimate SE df z.ratio p.value #&gt; 1.CT - 2.6h 0.1798 0.178 Inf 1.008 0.8518 #&gt; 1.CT - 3.12h -0.3841 0.159 Inf -2.416 0.1109 #&gt; 1.CT - 4.18h 0.4982 0.194 Inf 2.573 0.0753 #&gt; 1.CT - 5.24h 0.3992 0.188 Inf 2.118 0.2124 #&gt; 2.6h - 3.12h -0.5639 0.167 Inf -3.384 0.0064 #&gt; 2.6h - 4.18h 0.3185 0.200 Inf 1.593 0.5021 #&gt; 2.6h - 5.24h 0.2194 0.195 Inf 1.125 0.7933 #&gt; 3.12h - 4.18h 0.8824 0.183 Inf 4.824 &lt;.0001 #&gt; 3.12h - 5.24h 0.7833 0.177 Inf 4.414 0.0001 #&gt; 4.18h - 5.24h -0.0991 0.209 Inf -0.474 0.9897 #&gt; #&gt; Pigmentation = Yes: #&gt; contrast estimate SE df z.ratio p.value #&gt; 1.CT - 2.6h 0.2513 0.508 Inf 0.495 0.9879 #&gt; 1.CT - 3.12h 0.0000 0.476 Inf 0.000 1.0000 #&gt; 1.CT - 4.18h -0.6360 0.417 Inf -1.525 0.5461 #&gt; 1.CT - 5.24h -0.2877 0.445 Inf -0.646 0.9675 #&gt; 2.6h - 3.12h -0.2513 0.508 Inf -0.495 0.9879 #&gt; 2.6h - 4.18h -0.8873 0.454 Inf -1.957 0.2876 #&gt; 2.6h - 5.24h -0.5390 0.480 Inf -1.123 0.7942 #&gt; 3.12h - 4.18h -0.6360 0.417 Inf -1.525 0.5461 #&gt; 3.12h - 5.24h -0.2877 0.445 Inf -0.646 0.9675 #&gt; 4.18h - 5.24h 0.3483 0.382 Inf 0.911 0.8928 #&gt; #&gt; Results are given on the log odds ratio (not the response) scale. #&gt; P value adjustment: tukey method for comparing a family of 5 estimates Aqui temos o valor de cada combinação de níveis dos fatores, com seu respectivo valor de contraste e o valor de P. Vemos que para girinos sem pigmentação apenas 3 contrastes foram significativos. 8.6.1.3.1 Plot do modelo predito ggplot(uv_cells, aes(UV, Eosinophil)) + geom_violin(aes(color=Pigmentation))+ geom_jitter(shape = 16, position = position_jitter(0.1), cex = 4, alpha = 0.7) Usando o geom_violin podemos perceber que existe uma dispersão maior nos tratamentos que utilizaram girinos sem pigmentação do que nos tratamentos com girinos pigmentados. 8.7 Análise com dados de incidência Uma outra aplicação da distribuição binomial é quando temos dados de incidência, ou seja, presença ou ausência, de alguma variável. Por exemplo, presença ou ausência de uma espécie ou indivíduo num local. Neste caso a formula é diferente e o modelo é similar a uma regressão logística, vejamos. Aqui vamos utilizar os dados do trabalho de (Oliveira et al. 2020). Pergunta A probabilidade de lagartos da espécie Coleodactylus meridionalis perderem (autotomizarem) a cauda aumenta com o tamanho do corpo e de acordo com o sexo dos lagarto? Predições Quanto maior o lagarto, maior a probabilidade de autotomia da cauda e que esta resposta poderia também diferir entre sexos devido ao dimorfismo sexual. Variáveis  Variável resposta: Presença ou ausência de cauda autotomizada em lagartos encontrados por busca ativa. Exploração dos dados Este conjunto de dados possui muitas entradas faltantes (codificadas como NA). Primeiro vamos visualizar o conjunto de dados, e depois precisamos remover as linhas que contêm dados faltantes. Aqui podemos usar a função interna do ggplot2::remove_missing para remover linhas cujas variáveis informadas no argumento estejam faltando, vejamos: head(lagartos) #&gt; Numero Sex SVL Intact_tail_length Autotomized_tail_length Tail_state #&gt; 1 2 Male 20.70 NA 12.88 0 #&gt; 2 3 Male 21.10 NA 13.07 0 #&gt; 3 6 Female 23.72 NA 17.56 0 #&gt; 4 9 Male 18.84 17.38 NA 1 #&gt; 5 21 Male 22.20 NA 16.50 0 #&gt; 6 22 &lt;NA&gt; 20.59 NA 12.46 0 vis_dat(lagartos) vis_miss(lagartos,cluster = TRUE)#22.9% dos dados estão faltando dados_semNA&lt;-remove_missing(lagartos, vars = &quot;Sex&quot;)#excluindo linhas com dados faltantes para a variável Sex vis_miss(dados_semNA) dim(dados_semNA)#verificar as dimensões da tabela depois que os dados tiverem sido excluídos #&gt; [1] 139 6 Agora, seguindo o que já estamos acostumados a fazer, vamos vizualisar os dados com a nossa hipótese: ggplot(dados_semNA, aes(SVL, Tail_state))+ geom_point(aes(shape=Sex, color=Sex), size = 4, alpha = 0.4)+ geom_smooth(method = &quot;glm&quot;, method.args=list(family=&quot;binomial&quot;))+ labs(y=&quot;Estado da Cauda&quot;, x=&quot;Comprimento Rostro-Cloacal (mm)&quot;) 8.7.0.1 Modelagem Aqui vamos construir dois modelos com a mesma distribuição binomial, mas com dois link function: logit e probit. A função logit possui caudas um pouco mais achatadas, isto é, a curva probit se aproxima dos eixos mais rapidamente que a logit. Geralmente não há muita diferença entre elas. Como não temos nenhuma expectativa de qual dos dois link function é o melhor, podemos fazer uma seleção de modelos: mod_log&lt;-glm(Tail_state~SVL*Sex, data=dados_semNA, family = binomial(link=&quot;logit&quot;)) mod_pro&lt;-glm(Tail_state~SVL*Sex, data=dados_semNA, family = binomial(link=&quot;probit&quot;)) AICctab(mod_log, mod_pro, nobs=139) #&gt; dAICc df #&gt; mod_pro 0.0 4 #&gt; mod_log 0.1 4 Existe pouca diferença entre o modelo probit e logit. Como o modelo logit é mais simples vamos interpretá-lo apenas. 8.7.0.2 Diagnose dos resíduos do modelo simulationBion &lt;- simulateResiduals(fittedModel = mod_log, plot = T) binned_residuals(mod_log) #&gt; Warning: About 92% of the residuals are inside the error bounds (~95% or higher would be good). 8.7.0.3 Inferência anova(mod_log, test=&quot;Chisq&quot; ) #&gt; Analysis of Deviance Table #&gt; #&gt; Model: binomial, link: logit #&gt; #&gt; Response: Tail_state #&gt; #&gt; Terms added sequentially (first to last) #&gt; #&gt; #&gt; Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) #&gt; NULL 138 191.07 #&gt; SVL 1 9.2563 137 181.82 0.002347 ** #&gt; Sex 1 0.3920 136 181.43 0.531262 #&gt; SVL:Sex 1 0.0454 135 181.38 0.831292 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Para modelos com parâmetro de dispersão conhecida (e.g., binomial e Poisson), o chi-quadrado é a estatística mais apropriada. 8.7.0.4 Interpretação dos resultados  Importante A interpretação dos resultados é que o tamanho de corpo (SVL) afeta negativamente a probabilidade da cauda estar intacta, i.e., com o aumento do tamanho, a probabilidade da cauda permanecer intacta diminui. A interação não foi significativa, então o efeito é independente do sexo dos lagartos. 8.8 Dados de contagem com excesso de zeros Quando se analisa abundância ou riqueza de espécies é comum que tenhamos dados com muitos zeros. Esse fenômeno pode ser causado por vários processos ecológicos, tais como locais fora do nicho da espécie, falha na detecção, amostras feitas fora do hábitat ou em locais onde não se espera encontrar a espécie ((BlascoMoreno et al. 2019)). Esse tipo de dado é problemático porque rompe com os pressupostos da distribuição Poisson e binomial negativa, podendo inclusive ser uma das causas da overdispersion. Nesses casos, temos de ajustar modelos que levam em conta esse excesso de zeros nos dados. Esses modelos são chamados de zero-inflated e hurdle models (também chamados de zero-altered models), dependendo de como o processo que causou os zeros é modelado. Hurdle models (ou zero-altered models) modelam os dados dividindo-os em dois subconjuntos: um no qual reduzimos os dados à presença-ausência, ou seja, todos os dados maiores que 1 são transformados em 1 e usamos por exemplo uma distribuição binomial; e uma outra parte que só considera os valores positivos sem zero, utilizando uma Poisson ou binomial negativa truncadas. Ao fazer isso, a distribuição truncada assume que os zeros são gerados tanto por processos ecológicos quanto erros de amostragem (ou seja, é impossível distinguir entre essas duas fontes). Portanto, esses zeros são excluídos da distribuição com dados de contagem. Por exemplo, se uma distribuição binomial negativa for usada para modelar a parte quantitativa, chamamos o modelo de Zero-altered Negative Binomial. A interpretação dos modelos deve ser feita de forma conjunta. Modelos com zero inflados funcionam de maneira similar, mas permitem que a distribuição Poisson contenha zeros, ou seja, não é utilizada uma distribuição truncada. Ao fazer isso, esta distribuição de Poisson pressupõe que os zeros foram gerados por um processo ecológico real, tal como, ausência de hábitat adequado. Para ilustrar como podemos lidar com conjuntos de dados complexos vamos utilizar os dados coletados por (Lima et al. 2018). Pergunta Quais atributos de história de vida dos lagartos são relacionados com o volume (load) de infecção, tais como tamanho e sexo? Predições Quanto maior o lagarto, maior o número de parasitas encontrados, esta resposta poderia também diferir entre sexos devido ao dimorfismo sexual. Variáveis  Variável resposta: Número do parasita Raillietiella mottae, que é um crustáceo parasita, infectando o aparelho respiratório e intestinal de lagartos.  Os autores registraram essa espécie infectando duas espécies de lagartos que ocorrem no nordeste Brasileiro. Ao todo, 63 indivíduos de Hemidactylus agrius e 132 de Phyllopezus pollicaris foram amostrados. head(parasitas) #&gt; Especie Sexo CRC Raillietiella_mottae #&gt; W124 Phyllopezus_pollicaris F 61 3 #&gt; W125 Phyllopezus_pollicaris F 56 0 #&gt; W127 Phyllopezus_pollicaris M 61 0 #&gt; W128 Phyllopezus_pollicaris M 48 0 #&gt; W129 Phyllopezus_pollicaris F 40 0 #&gt; W130 Phyllopezus_pollicaris M 62 0 Explorando os dados ggplot(parasitas, aes(Raillietiella_mottae))+ geom_density(aes(fill=&quot;red&quot;))+ facet_grid(Especie~Sexo)+ theme(legend.position = &quot;none&quot;) ggplot(parasitas, aes(CRC, Raillietiella_mottae)) + geom_point(size = 4 , alpha = 0.4) + facet_grid(Sexo~ Especie) Os gráfico acima mostra a contagem do parasita Raillietiella mottae nos dois sexos (F e M para fêmea e macho) nas duas espécies de lagartos, tanto na forma de uma distribuição de densidade quanto de gráfico de dispersão. Aqui podemos ver que de fato existe um excesso de zeros principalmente em P. pollicaris. Quando nos deparamos com dados complexos assim, a estratégia é sempre começar com um modelo simples e depois adicionar mais parâmetros. Portanto, vamos iniciar com um modelo Poisson, mesmo sabendo que ele muito provavelmente não será adequado para modelar estes dados: 8.8.0.1 Modelagem pois_plain&lt;-glm(Raillietiella_mottae~CRC+Sexo*Especie, data=parasitas, family=&quot;poisson&quot;) 8.8.0.2 Diagnose Aqui vamos utilizar as funções do pacote performance novamente: check_zeroinflation(pois_plain)#para diagnosticar se o modelo sofre de zero inflation #&gt; # Check for zero-inflation #&gt; #&gt; Observed zeros: 156 #&gt; Predicted zeros: 140 #&gt; Ratio: 0.90 check_overdispersion(pois_plain) #&gt; # Overdispersion test #&gt; #&gt; dispersion ratio = 1.932 #&gt; Pearson&#39;s Chi-Squared = 367.133 #&gt; p-value = &lt; 0.001 A diagnose não só nos disse que o modelo possui overdispersion, como também de zero-inflation, como já esperávamos. Vejamos então como melhorar o nosso modelo para lidar com esses dois problemas. Especificamente, vamos utilizar um modelo Hurdle com binomial negativa truncada (ou seja, desconsiderando os zeros), e um outro modelo zero-inflated usando uma distribuição binomial negativa. Aqui vamos utilizar o pacote glmmTMB : hur_NB &lt;- glmmTMB(Raillietiella_mottae~CRC+Sexo*Especie, zi=~., data=parasitas, family=truncated_nbinom2)#Hurdle model ziNB_mod2 &lt;- glmmTMB(Raillietiella_mottae~CRC+Sexo*Especie, zi=~., data=parasitas, family=nbinom2)#zero-inflated Poisson ziP_mod2 &lt;- glmmTMB(Raillietiella_mottae~CRC+Sexo*Especie, zi=~., data=parasitas, family=poisson)#zero-inflated Negative Binomial 8.8.0.3 Diagnose check_zeroinflation(hur_NB)#prediz melhor os zeros #&gt; # Check for zero-inflation #&gt; #&gt; Observed zeros: 156 #&gt; Predicted zeros: 156 #&gt; Ratio: 1.00 check_zeroinflation(ziP_mod2) #&gt; # Check for zero-inflation #&gt; #&gt; Observed zeros: 156 #&gt; Predicted zeros: 140 #&gt; Ratio: 0.90 check_zeroinflation(ziNB_mod2) #&gt; # Check for zero-inflation #&gt; #&gt; Observed zeros: 156 #&gt; Predicted zeros: 140 #&gt; Ratio: 0.90 Aqui vemos que o modelo zero-altered (Hurdle Model) conseguiu predizer exatamente a quantidade de zeros observada, fazendo com que o modelo seja suficiente para usarmos com esses dados. ICtab(pois_plain, hur_NB,ziP_mod2,ziNB_mod2, type=c(&quot;AICc&quot;), weights = TRUE) #&gt; dAICc df weight #&gt; ziP_mod2 0.0 10 0.62 #&gt; ziNB_mod2 1.6 11 0.28 #&gt; hur_NB 3.6 11 0.10 #&gt; pois_plain 44.6 5 &lt;0.001 Mas quando comparamos o AICc entre modelos, os modelos zero-inflated (tanto Poisson, quanto binomial negativa) que tem menos parâmetros, são ranqueados ligeiramente melhor do que o modelo binomial negativa zero-altered (ou hurdle). Não podemos distinguir entre os dois modelos com zero-inflated porque o dAIC &lt; 2, ou seja, o ajuste deles aos dados são praticamente iguais. Vejam que a diferença de Akaike Weights entre os dois primeiros modelos e o hurdle é bastante substancial (0.52). Além disso, vemos que os modelos que levam em conta o excesso de zeros se ajustam bem melhor aos dados do que o modelo simples com distribuição Poisson. Vamos ver como os modelos se saem em relação aos outros pressupostos: simulationOutput &lt;- simulateResiduals(fittedModel = hur_NB, plot = T) simulationOutput &lt;- simulateResiduals(fittedModel = ziNB_mod2, plot = T)#tem um outlier nos resíduos (asterisco vermelho) Os gráficos de diagnose do DHARMa são outra evidência de que tanto o modelo hurdle quanto o zero-inflated Poisson são adequados para os dados, em termos de heterogeneidade de variância, outliers e overdispersion. 8.8.0.4 Interpretação dos resultados Apesar de não ter um ajuste tão bom aos dados, o modelo hurdle prediz melhor a quantidade de zeros. Portanto, vamos interpretar os coeficientes apenas deste modelo: summary(hur_NB) #&gt; Family: truncated_nbinom2 ( log ) #&gt; Formula: Raillietiella_mottae ~ CRC + Sexo * Especie #&gt; Zero inflation: ~. #&gt; Data: parasitas #&gt; #&gt; AIC BIC logLik deviance df.resid #&gt; 277.8 313.8 -127.9 255.8 184 #&gt; #&gt; #&gt; Dispersion parameter for truncated_nbinom2 family (): 4.64 #&gt; #&gt; Conditional model: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 3.03428 2.36511 1.283 0.1995 #&gt; CRC -0.05041 0.04861 -1.037 0.2997 #&gt; SexoM -1.49505 0.71440 -2.093 0.0364 * #&gt; EspeciePhyllopezus_pollicaris 0.68945 1.09380 0.630 0.5285 #&gt; SexoM:EspeciePhyllopezus_pollicaris 1.75281 0.94217 1.860 0.0628 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Zero-inflation model: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 7.6283 1.8529 4.117 3.84e-05 *** #&gt; CRC -0.1291 0.0369 -3.499 0.000468 *** #&gt; SexoM -1.0893 0.5867 -1.856 0.063386 . #&gt; EspeciePhyllopezus_pollicaris 2.2701 0.9140 2.484 0.013003 * #&gt; SexoM:EspeciePhyllopezus_pollicaris 2.2002 0.8192 2.686 0.007239 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Para maiores detalhes na interpretação deste tipo de modelo, sugerimos fortemente consultar p. 382-3 de Brooks et al. (2017). Para fatores com mais de um nível, o summary mostra os resultados usando contraste, para isto toma como referência um dos níveis do fator (o primeiro em ordem alfabética) e o compara com os outros. Note que na parte com excesso de zeros o contraste é positivo para Espécie. Ou seja, o P. pollicaris tem maior chance de ter ausência de parasitas que H. agrius. O contraste para espécie continua sendo positivo na parte condicional do modelo, mas o valor do parâmetro não é tão alto. Isso quer dizer que P. pollicaris tem abundância de parasitas em média ligeiramente maior que H. agrius. Vemos que a interação é significativa entre sexo e espécie na parte do modelo com excesso de zeros, mas apenas marginalmente significativa na parte condicional. Portanto, a influência do sexo na incidência, mas não na abundância, do parasita depende conjuntamente da espécie. No entanto, o CRC só passa a ser significativo na parte de excesso de zeros, ou seja, quando modelamos apenas a incidência (presença-ausência) do parasita. Portanto, o CRC determina se o lagarto vai ou não ser infectado, mas não o quanto vai receber de parasitas. Já tanto o sexo quanto a espécie foram significativas em ambas as partes do modelo, ou seja, esses fatores não influenciam diferentemente a infecção e a quantidade de parasitas. Agora vejamos como podemos plotar as predições deste modelo: parasitas$phat &lt;- predict(hur_NB, type=&quot;response&quot;) parasitas &lt;- parasitas[with(parasitas, order(Sexo, Especie)), ] ggplot(parasitas, aes(x = CRC, y = phat, colour = Especie,shape = Sexo, linetype = Sexo)) + geom_point(aes(y = Raillietiella_mottae), size=4, alpha=.7, position=position_jitter(h=.2)) + geom_line(size = 1) + labs(x = &quot;Comprimento Rostro-Cloacal&quot;, y = expression(paste(&quot;Abundância de &quot;, italic(&quot;Raillietiella mottae&quot;)))) 8.9 Dados ordinais: os modelos cumulative link Uma outra maneira de codificarmos os dados é utilizando categorias ordenadas, tais como ranques. Exemplos incluem a escala de Likert, scores, intervalos (e.g., de idade). Para este exemplo, iremos utilizar um outro conjunto de dados do artigo de (Franco-Belussi, De Oliveira, and Sköld 2018) que manipulou in vitro a concentração do hormônio noradrenalina (NA) nos olhos de peixes esgana-gato (Gasterosteus aculeatus) e avaliaram a expressão de várias cores conferidas por tipos de células (cromatóforos). Aqui vamos usar os dados do efeito do NA na cor vermelha em machos. Pergunta A NA causa uma diminuição da coloração vermelha, via agregação dos pigmentos? Predições A presença de NA causa a agregação dos pigmentos, permitindo que os hormônios reprodutivos atuem. Variáveis  Variável resposta: Escala de intensidade de cor. Para mais detalhes veja o artigo original. cores &lt;- read.csv2(&quot;https://ndownloader.figshare.com/files/10250700&quot;, h=TRUE) head(cores) #&gt; Animal Treatment Time Sex Black Red #&gt; 1 1 CT 0h M 5 5 #&gt; 2 1 CT 1h M 5 5 #&gt; 3 1 CT 2h M 5 5 #&gt; 4 1 CT 3h M 5 5 #&gt; 5 2 CT 0h M 5 4 #&gt; 6 2 CT 1h M 5 4 ## Filtrando dados - Red Male redmale&lt;- filter(cores, Sex==&quot;M&quot;) head(redmale) #&gt; Animal Treatment Time Sex Black Red #&gt; 1 1 CT 0h M 5 5 #&gt; 2 1 CT 1h M 5 5 #&gt; 3 1 CT 2h M 5 5 #&gt; 4 1 CT 3h M 5 5 #&gt; 5 2 CT 0h M 5 4 #&gt; 6 2 CT 1h M 5 4 Esses dados no entanto tem de ser codificados como um fator ordenado antes de entrarmos com eles no modelo. redmale$Animal&lt;-factor(redmale$Animal) redmale$Red&lt;-factor(redmale$Red, levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;), ordered = TRUE) str(redmale) #&gt; &#39;data.frame&#39;: 40 obs. of 6 variables: #&gt; $ Animal : Factor w/ 5 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... #&gt; $ Treatment: chr &quot;CT&quot; &quot;CT&quot; &quot;CT&quot; &quot;CT&quot; ... #&gt; $ Time : chr &quot;0h&quot; &quot;1h&quot; &quot;2h&quot; &quot;3h&quot; ... #&gt; $ Sex : chr &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ... #&gt; $ Black : int 5 5 5 5 5 5 5 5 4 4 ... #&gt; $ Red : Ord.factor w/ 5 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 5 5 5 5 4 4 4 4 4 4 ... Repare que a classe do objeto muda e temos agora que Red é um Ordered factor. 8.9.0.1 Modelagem mod3&lt;-clmm(Red~Treatment+Time+(1|Animal), data=redmale, threshold = &quot;equidistant&quot;) 8.9.0.2 Diagnose Infelizmente, o pacote ordinal não fornece métodos para lidar com modelos mistos, como o nosso. Então, montamos um modelo fixo apenas para entrar nas duas funções de diagnose. Essas duas funções scale_test e nominal_test testam a qualidade do ajuste (goodness-of-fit) do modelo, similar aos likelihood ratio tests só que para dados ordinais. assumption3 &lt;- clm(Red~Treatment+Time, data=redmale, threshold = &quot;equidistant&quot;) scale_test(assumption3) #&gt; Tests of scale effects #&gt; #&gt; formula: Red ~ Treatment + Time #&gt; Df logLik AIC LRT Pr(&gt;Chi) #&gt; &lt;none&gt; -24.301 60.602 #&gt; Treatment 1 -24.293 62.586 0.015248 0.9017 #&gt; Time nominal_test(assumption3) #&gt; Tests of nominal effects #&gt; #&gt; formula: Red ~ Treatment + Time #&gt; Df logLik AIC LRT Pr(&gt;Chi) #&gt; &lt;none&gt; -24.301 60.602 #&gt; Treatment 1 -19.749 53.499 9.1031 0.002552 ** #&gt; Time 3 -22.803 63.606 2.9953 0.392356 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Parece que não há problemas com o efeito de escala do dado ordinal, mas a diagnose sugere que possa haver evidência de rompimento do pressuposto de probabilidades proporcionais em relação ao tratamento. Esse é um pressuposto importante de modelos ordinais, os quais assumem que os efeitos de qualquer uma das variáveis explicativas são consistentes (proporcionais) ao longo de diferentes thresholds (que são as quebras entre cada par de categorias da variável resposta ordinal). Isto provavelmente se deve ao baixo tamanho amostral. Por questão de brevidade vamos apenas ignorar este aspecto e interpretar o resultado do modelo mesmo assim. Mas se o seu modelo apresentar este problema, a solução deve ser realizar regressões logísticas separadamente. 8.9.0.3 Inferência summary(mod3) #&gt; Cumulative Link Mixed Model fitted with the Laplace approximation #&gt; #&gt; formula: Red ~ Treatment + Time + (1 | Animal) #&gt; data: redmale #&gt; #&gt; link threshold nobs logLik AIC niter max.grad cond.H #&gt; logit equidistant 40 -22.89 59.77 226(681) 1.04e-05 4.1e+01 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. #&gt; Animal (Intercept) 1.438 1.199 #&gt; Number of groups: Animal 5 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; TreatmentNA10uM -4.602 1.228 -3.748 0.000178 *** #&gt; Time1h -3.602 1.377 -2.616 0.008894 ** #&gt; Time2h -3.602 1.377 -2.616 0.008894 ** #&gt; Time3h -3.602 1.377 -2.616 0.008894 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Threshold coefficients: #&gt; Estimate Std. Error z value #&gt; threshold.1 -6.198 1.722 -3.60 #&gt; spacing 4.978 1.254 3.97 anova(assumption3) #&gt; Type I Analysis of Deviance Table with Wald chi-square tests #&gt; #&gt; Df Chisq Pr(&gt;Chisq) #&gt; Treatment 1 15.3616 8.877e-05 *** #&gt; Time 3 9.1992 0.02676 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 pairs(emmeans(mod3, ~ Treatment|Time, adjust= &quot;tukey&quot;)) #&gt; Time = 0h: #&gt; contrast estimate SE df z.ratio p.value #&gt; CT - NA10uM 4.6 1.23 Inf 3.748 0.0002 #&gt; #&gt; Time = 1h: #&gt; contrast estimate SE df z.ratio p.value #&gt; CT - NA10uM 4.6 1.23 Inf 3.748 0.0002 #&gt; #&gt; Time = 2h: #&gt; contrast estimate SE df z.ratio p.value #&gt; CT - NA10uM 4.6 1.23 Inf 3.748 0.0002 #&gt; #&gt; Time = 3h: #&gt; contrast estimate SE df z.ratio p.value #&gt; CT - NA10uM 4.6 1.23 Inf 3.748 0.0002 Aqui vemos que tanto o tratamento quanto o tempo de exposição foram significativos. 8.9.0.4 Interpretação dos resultados lineplot.CI(Time, as.numeric(Red), Treatment, data=redmale, cex = 1, xlab = &quot;Experimental time (hours)&quot;, ylab = &quot;Erythrophore Index (EI)&quot;, cex.lab = 1.5, x.leg = 1, y.leg = 1.2, cex.leg = 1.3, cex.axis = 1.5, col = c(&quot;#EE6363&quot;,&quot;#79CDCD&quot;), pch = c(12,12), lwd = 1.5, ylim= c(0,5)) 8.10 Dados contínuos: distribuição beta Aqui vamos utilizar como exemplo os dados do artigo de (Franco-Belussi, De Oliveira, and Sköld 2018). Os pesquisadores fizeram um experimento in vivo com peixes esgana-gato (Gasterosteus aculeatus) para testar como a coloração dos animais respondem ao fármaco ioimbina (YOH), que bloqueia a coloração típica que os machos exibem na época de acasalamento, e o tempo de exposição ao mesmo (além de um controle), num desenho de ANOVA fatorial. Como as medidas foram feitas repetidamente no mesmo animal, iremos incluir o Animal como um fator aleatório no modelo. Pergunta A YOH aumenta a coloração escura no olho e mandíbula dos peixes via dispersão dos pigmentos? Predições A YOH promoverá um escurecimento do corpo do animal, já que ela inibe a ação NorAdrenalia (NA). Variáveis  Variável resposta: A intensidade de coloração escura em peixes machos. Esses dados são expressos em termos de porcentagem e variam continuamente de 0 a 100%. Para facilitar a modelagem e nos adequarmos à maneira com que a função requer os dados, vamos simplesmente dividir por 100 para que os dados variem entre 0 e 1. Para modelar os dados vamos utilizar a função glmmTMB ##Filtrando dados fish$Animal&lt;-factor(fish$Animal) fish$Sex&lt;-factor(fish$Sex) darknessmale&lt;- dplyr::filter(fish, Sex==&quot;M&quot;) ggplot(darknessmale, aes(Darkness/100)) + geom_density(colour=&quot;red&quot;, fill=&quot;red&quot;) + theme(legend.position=&quot;none&quot;) No histograma podemos ver que os dados de fato variam continuamente no intervalo entre 0 e 1, tendo uma distribuição notadamente bimodal. 8.10.0.1 Modelagem mod2&lt;-glmmTMB(Darkness/100~Treatment*Time+(1|Animal), family= beta_family, data=darknessmale) 8.10.0.2 Diagnose Aqui utilizaremos o mesmo pacote DHARMa para realizar a diagnose do modelo: simulationOutput &lt;- simulateResiduals(fittedModel = mod2, plot = TRUE) Podemos ver que o modelo não sofre de heterogeneidade de dispersão, overdispersion, nem problemas com outlier. 8.10.0.3 Interpretação dos resultados Agora que podemos interpretar o output com confiança, vamos obter a tabela de anova em que teremos os testes de cada fator do modelo: Anova(mod2) #&gt; Analysis of Deviance Table (Type II Wald chisquare tests) #&gt; #&gt; Response: Darkness/100 #&gt; Chisq Df Pr(&gt;Chisq) #&gt; Treatment 105.546 1 &lt; 2.2e-16 *** #&gt; Time 40.719 3 7.499e-09 *** #&gt; Treatment:Time 49.262 3 1.147e-10 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1  Importante aqui vemos que a interação é significativa. Portanto, temos de interpretar os níveis do fator da combinação, fazemos isso no pacote emmeans colocando a barra | : pairs(emmeans(mod2, ~ Treatment|Time)) #&gt; Time = 0h: #&gt; contrast estimate SE df t.ratio p.value #&gt; CT - YOH 0.0283 0.160 30 0.177 0.8609 #&gt; #&gt; Time = 1h: #&gt; contrast estimate SE df t.ratio p.value #&gt; CT - YOH -1.3068 0.181 30 -7.210 &lt;.0001 #&gt; #&gt; Time = 2h: #&gt; contrast estimate SE df t.ratio p.value #&gt; CT - YOH -1.2286 0.182 30 -6.763 &lt;.0001 #&gt; #&gt; Time = 3h: #&gt; contrast estimate SE df t.ratio p.value #&gt; CT - YOH -1.4025 0.185 30 -7.582 &lt;.0001 #&gt; #&gt; Results are given on the log odds ratio (not the response) scale.  Importante e então podemos perceber que a diferença entre o controle e o tratado só passa a ser significativa depois de 1 h de exposição. Isso fica mais evidente quando plotamos os dados lineplot.CI(Time, Darkness, Treatment, data=darknessmale, cex = 1, xlab = &quot;Tempo experimental (horas)&quot;, ylab = &quot;Escuridão do corpo de machos (%)&quot;, cex.lab = 1, x.leg = 1, col = c(&quot;#EE6363&quot;,&quot;#79CDCD&quot;), pch = c(12,12), lwd = 1.5) 8.11 Leituras recomendadas Neste capítulo apenas fizemos uma breve introdução aos modelos lineares generalizados. Para conhecer um pouco mais a fundo todos os detalhes recomendamos a consulta dos livros (Zuur, Ieno, and Elphick 2009b) e (Pinheiro and Bates 2000a) que são as referências clássicas sobre GLM com aplicações em ecologia. Para dados ordinais, sugerimos os livros do Alan Agresti, tais como (Agresti 2010) e Categorical Data Analysis, 3rd Edition, do mesmo autor. Referências "],["cap9.html", "Capítulo 9 Análises Multidimensionais Pré-requisitos do capítulo 9.1 Aspectos teóricos 9.2 Análise de agrupamento hierárquico 9.3 K-means e agrupamentos não-hierárquicos 9.4 Espécies indicadoras 9.5 Análises de Ordenação 9.6 PCR - Regressão de Componentes Principais 9.7 Ordenação restrita 9.8 PERMANOVA", " Capítulo 9 Análises Multidimensionais Pré-requisitos do capítulo ## Pacotes library(ade4) library(ecodados) library(tidyverse) library(vegan) library(pvclust) library(BiodiversityR) library(labdsv) library(ggplot2) library(gridExtra) library(ape) library(FactoMineR) library(factoextra) library(FD) library(palmerpenguins) library(GGally) library(ade4) library(ggord) library(adespatial) library(spdep) ## Dados necessários sp_compos &lt;- ecodados::bocaina species &lt;- ecodados::com_birds env &lt;- ecodados::env_birds xy &lt;- ecodados::birds.xy data(mite) data(doubs) data(mite.env) data(package = &#39;palmerpenguins&#39;) 9.1 Aspectos teóricos Em geral, análises multivariadas têm três principais utilidades: reduzir a dimensionalidade dos dados e encontrar a principal direção de variação dos dados, testar relações entre matrizes, ou ainda encontrar diferenças entre grupos. Apesar dessas análises também serem utilizadas como análises exploratórias e para descrever padrões em estudos ecológicos, a necessidade de se ter hipóteses, ou ao menos expectativas a priori, não pode ser ignorada. Antes de entrar de cabeça nas análises multivariadas, também sugerimos fortemente o estudo de métodos de amostragem e como fazer boas perguntas. Análises multivariadas podem ser divididas, grosseiramente, em dois tipos: agrupamento e ordenação. Análises de agrupamento em geral tentam agrupar objetos (observações) ou descritores em grupos de maneira que objetos do mesmo grupo sejam mais semelhantes entre si do que objetos de outros grupos (P. Legendre and Legendre 2012a). Por exemplo, os objetos podem ser localidades como parcelas, riachos ou florestas, enquanto os descritores são as difentes variáveis coletadas nesses objetos (e.g., espécies, variáveis ambientais). A análise de ordenação, por sua vez, é uma operação pela qual os objetos (ou descritores) são posicionados num espaço que contém menos dimensões que o conjunto de dados original; a posição dos objetos ou descritores em relação aos outros também podem ser usadas para agrupá-los. Vamos começar com análises de agrupamento. Aqui vamos exemplificar dois métodos: uma técnica de agrupamento hierarquica (dendrograma) e outra não-hierarquica (k-means). 9.1.1 Coeficientes de associação Assim chamados genericamente, os coeficientes de associação medem o quão parecidos objetos ou descritores são entre si. Quando analisamos a relação entre objetos fazemos uma análise no modo Q, ao passo que o modo R é quando analisamos a relação entre descritores. Coeficientes de associação do modo Q são medidas de (dis)similaridade ou distância, enquanto para o modo R utilizamos covariância ou correlação. Como já tratamos neste livro sobre covariância e correlação, neste tópico vamos falar sobre índices de distância e similaridade. Mas qual a definição destas duas quantitades? Similaridade são máximas (S=1) quando dois objetos são idênticos Distâncias são o contrário da similaridade (D=1-S) e não têm limites superiores (dependem da unidade de medida) Existem ao menos 26 índices de similaridade que podem ser agrupados de acordo com o tipo de dado (qualitativos ou quantitativos), a maneira com que lidam com duplos zeros (simétricos ou assimétricos). Do seu lado, as distâncias só se aplicam a dados quantitativos e têm como características serem métricas, semi-métricas ou não-métricas. Vejamos agora os principais índices de similaridade e distância de cada tipo. 9.1.2 Métricas de distância O principal coeficiente de distância usado em ecologia é a distância euclidiana. Além disso temos ainda Canberra, Mahalanobis (calcula a distância entre dois pontos num espaço não ortogonal, levando em consideração a covariância entre descritores), Manhattan, Chord (elimina diferenças entre abundância total de espécies), 2 (dá peso maior para espécies raras), Hellinger (não dá peso para espécies raras). Essas distâncias são recomendada nos casos em que as variáveis de estudo forem contínuas, como por exemplo variáveis morfométricas ou descritores ambientais. Uma característica comum de conjuntos de dados ecológicos são os vários zeros encontrados em matrizes de composição. Eles surgem porque não encontramos nenhum indivíduo de uma determinada espécie num local, seja porque aquele local não tem as condições ambientais adequadas a ela, falha na detectabilidade, ou dinâmicas demográficas estocásticas de colonização-extinção. Logo, quando dois locais compartilham ausência de espécies, não é possível atribuir uma única razão da dupla ausência. Como essas medidas de distância apresentadas acima assumem que os dados são quantitativos e não de contagem, elas não são adequadas para lidar com dados de abundância ou incidência de espécies, porque atribuem um grau de parecênça a pares de locais que compartilham zeros (P. Legendre and Legendre 2012a). Por esse motivo precisamos de coeficientes que desconsiderem os duplos zeros. Eles são chamados de assimétricos. 9.1.2.1 Coeficientes assimétricos binários para objetos Esses coeficientes (ou índices) são apropriados para dados de incidência de espécies (presença-ausência) e desconsideram as duplas ausências. Os índices deste tipo mais comuns utilizados em ecologia são Sørensen, Jaccard, e Ochiai. \\[ \\beta j= a/a+b+c \\] , onde a = número de espécies compartilhadas, b = número de espécies exclusivas da comunidade 1, c = número de espécies exclusivas da comunidade 2. A diferença entre Jaccard e Sørensen é o Sørensen dá peso dobrado para duplas presenças. Por conta dessas características estes índices são adequados para quantificar diversidade beta (Marti J. Anderson et al. 2010; Pierre Legendre and De Cáceres 2013). Esses índices variam entre 0 (nenhuma espécie é compartilhada entre o par de locais) a 1 (todas as espécies são compartilhadas entre o par de locais). 9.1.2.2 Coeficientes binários para descritores (R mode) Se o objetivo for calcular a similaridade entre descritores binários (e.g., presença ou ausência de características ambientais) de pares de locais, geralmente o coeficiente recomendado é o de Sokal &amp; Michener. Este índice está implementado em ade4::dist.binary. 9.1.2.3 Coeficientes quantitativos para objetos Estes são os coeficientes utilizados para dados de contagem (e.g., abundância), quantitativos (e.g., frequência, biomassa, porcentagem cobertura). Diferentemente das distâncias, estes coeficientes são assimétricos, ou seja, não consideram duplas ausências, e portanto são adequados para analisar dados de composição de espécies. Além disso, uma outra característica deles é serem semi-métricos. Os índices mais comuns deste tipo são Bray-Curtis (conhecido como percentage difference em inglês), Chord, log-Chord, Hellinger, chi-quadrado, e Morisita-Horn. Todos os índices discutidos até aqui estão implementados nas funções ade4::dist.ktab, adespatial::dist.ldc, e vegan::vegdist. 9.1.2.4 Coeficientes para descritores (R mode) que incluem mistura de tipos de dados É comum em análises de diversidade funcional que tenhamos um conjunto de atributos (traits) de espécies que são formados por vários tipos de dados: quantitativos (e.g., tamanho de corpo), binários (presença ausência de uma dada característica), fuzzy (um atributo multiestado descrito codificado em várias colunas com porcentagem), ordinais, e circulares (e.g., distribuição de uma fenofase ao longo de um ano). O índice que lida com todos esses dados é o Gower. A versão extendida do índice de Gower pode ser encontrada na função ade4::dist.ktab. O capítulo 7 de (P. Legendre and Legendre 2012a) fornece uma chave dicotômica para escolha do índice mais adequado. 9.1.2.5 Padronizações e transformações É comum coletarmos múltiplas variáveis ambientais cujas unidades sejam diferentes. Por exemplo, temperatura (ºC), distância da margem (m), área (m2). Para diminuir a taxa de Erro Tipo I das análises é recomendado que padronizemos os dados utilizando distribuição Z, assim todas as variáveis passam a ter média 0 e desvio padrão 1. Essa padronização pode ser implementada na função vegan::decostand. Um outro problema comum de matrizes de dados de composição de espécies é o alto número de zeros, enquanto outras espécies podem ter altas abundâncias. Isso gera problemas em ordenações. Para diminuir esta discrepância podemos transformar os dados, por exemplo, utilizando a distância de Hellinger ou Chord. Isso pode ser feito na função vegan::decostand. 9.2 Análise de agrupamento hierárquico O objetivo da análise de agrupamento é agrupar objetos admitindo que haja um grau de similaridade entre eles. Esta análise pode ser utilizada ainda para classificar uma população em grupos homogêneos de acordo com uma característica de interesse. A grosso modo, uma análise de agrupamento tenta resumir uma grande quantidade de dados e apresentá-la de maneira fácil de visualizar e entender (em geral, na forma de um dendrograma). No entanto, os resultados da análise podem não refletir necessariamente toda a informação originalmente contida na matriz de dados. Para avaliar o quão bem uma análise de agrupamento representa os dados originais existe uma métrica  o coeficiente de correlação cofenético  o qual discutiremos em detalhes mais adiante. Antes de considerar algum método de agrupamento, pense porque você esperaria que houvesse uma descontinuidade nos dados; ou ainda, considere se existe algum ganho prático em dividir uma nuvem de objetos contínuos em grupos. O padrão apresentado pelo dendograma depende do protocolo utilizado (método de agrupamento e índice de dissimilaridade); os grupos formados dependem do nível de corte escolhido. A matriz deve conter os objetos a serem agrupados (e.g., espécies) nas linhas e as variáveis (e.g., locais de coleta ou medidas morfológicas) nas colunas. A escolha do método de agrupamento é crítico para a escolha de um coeficiente de associação. É importante compreender as propriedades dos métodos de agrupamento para interpretar corretamente a estrutura ecológica que eles evidenciam (P. Legendre and Legendre 2012a). De acordo com a classificação de Sneath &amp; Sokal (1973) existem cinco tipos de métodos: 1) sequenciais ou simultâneos; 2) aglomerativo ou divisivo; 3) monotéticos ou politéticos; 4) hierárquico ou não hierárquicos e 5) probabilístico. Sugerimos a leitura do artigo citado para aprofundar seus conhecimentos sobre os diferentes métodos. Métodos hierárquicos podem ser divididos naqueles que consideram o centróide ou a média aritmética entre os grupos. O principal método hierárquico que utiliza a média aritmética é o UPGMA (Agrupamento pelas médias aritméticas não ponderadas), e o principal método que utiliza centróides é a Distância mínima de Ward. O UPGMA funciona da seguinte forma: a maior similaridade (ou menor distância) identifica os próximos agrupamentos a serem formados. Após esse evento, o método calcula a média aritmética das similaridades ou distâncias entre um objeto e cada um dos membros do grupo ou, no caso de um grupo previamente formado, entre todos os membros dos dois grupos. Todos os objetos recebem pesos iguais no cálculo. O método de Ward é baseado no critério de quadrados mínimos (OLS), o mesmo utilizado para ajustar um modelo linear. O objetivo é definir os grupos de maneira que a soma de quadrados (i.e. similar ao erro quadrado da ANOVA) dentro dos grupos seja minimizada (Borcard, Gillet, and Legendre 2018). No entanto, para interpretar os resultados precisamos antes definir um nível de corte, que vai nos dizer quantos grupos existem. Há vários métodos para definir grupos, desde os heurísticos aos que utilizam bootstrap. Se quisermos interpretar este dendrograma, podemos por exemplo estabelecer um nível de corte de 50% de distância (ou seja, grupos cujos objetos tenham ao menos 50% de similaridade entre si). Checklist Verifique se não há espaço nos nomes das colunas e linhas Se os dados forem de abundância, recomenda-se realizar a transformação de Hellinger [@legendre2001]. Esta transformação é necessária porque a matriz de comunidades (em especial, com a presença de muitas espécies raras) pode causar distorções nos métodos de ordenação baseados em distância Euclidiana [@legendre2001]. Se a matriz original contiver muitos valores discrepantes (e.g., uma espécie muito mais ou muito menos abundante que outras) é necessário transformar os dados usando log1p. Se as variáveis forem medidas tomadas em diferentes escalas (metros, graus celcius etc), é necessário padronizar cada variável para ter a média 0 e desvio padrão 1. Isso pode ser feito utulizando a função decostand do pacote vegan. 9.2.0.1 Exemplo 1 Neste exemplo vamos utilizar um conjunto de dados que contém larvas de espécies de anfíbios anuros coletados em 14 poças com diferentes coberturas de dossel. Pergunta Existem grupos de espécies de anfíbios anuros com padrões de ocorrência similar ao longo das poças? Predições Iremos encontrar ao menos dois grupos de espécies: aquelas que ocorrem em poças dentro de floresta (i.e., maior cobertura de dossel) vs. aquelas que ocorrem em poças de áreas abertas (menor cobertura de dossel). Variáveis Variáveis preditoras: a matriz de dados contém a abundância das espécies nas linhas e locais (poças) nas colunas. 9.2.1 Análise no R Para começar, vamos primeiro importar os dados e depois calcular a matriz de distância que seja adequada para o tipo de dado que temos (abundância de espécies - dados de contagem) ## Composição de espécies (seis primeiras localidades) head(sp_compos) #&gt; BP4 PP4 PP3 AP1 AP2 PP1 PP2 BP9 PT1 PT2 PT3 BP2 PT5 #&gt; Aper 0 3 0 0 2 0 0 0 0 0 0 181 0 #&gt; Bahe 859 14 14 0 87 312 624 641 0 0 0 14 0 #&gt; Rict 1772 1517 207 573 796 0 0 0 0 0 0 0 0 #&gt; Cleuco 0 0 0 0 0 0 0 0 0 29 369 0 84 #&gt; Dmic 0 0 6 60 4 0 0 0 2758 319 25 0 329 #&gt; Dmin 0 84 344 1045 90 0 0 0 8 0 0 0 0 ## Matriz de similaridade com o coeficiente de Morisita-Horn distBocaina &lt;- vegdist(sp_compos, method=&quot;horn&quot;) # Agrupamento com a função hclust e o método UPGMA dendro &lt;- hclust(distBocaina, method=&quot;average&quot;) # Visualizar os resultados plot(dendro) 9.2.2 Assessando a qualidade do dendrograma Precisamos verificar que o agrupamento reduziu a dimensionalidade da matiz de forma eficiente, de maneira a não distorcer a informação. Fazemos isso calculando o Coeficiente de correlação cofenética (CCC) cofresult &lt;- cophenetic(dendro) cor(cofresult, distBocaina) #&gt; [1] 0.9455221 Um CCC &gt; .7 indica uma boa representação. Portanto, o nosso resultado de 0.9455221 é alto, garantindo que o dendrograma é adequado. plot(dendro) k &lt;- 4 n &lt;- ncol(sp_compos) MidPoint &lt;- (dendro$height[n-k] + dendro$height[n-k+1]) / 2 abline(h = MidPoint, lty=2) Nesse caso teremos a formação de cinco grupos, representados pelos nós que estão abaixo da linha de corte. Portanto, o resultado não suporta a nossa hipótese a priori que predizia a formação de apenas dois grupos de espécies. 9.2.2.1 Exemplo 2 No exemplo anterior vimos que é difícil interpretar os grupos baseado num nível de corte. A seguir, vamos utilizar o pacote pvclust que calcula automaticamente o nível de corte de similaridade baseado no Bootstrap de cada nó. Uma desvantagem deste método é que ele somente aceita índices de similaridade da função dist que possui apenas a distância Euclidiana, Manhattan e Canberra. Uma maneira de contornarmos essa limitação é utilizar transformações dos dados disponíveis na função disttransform no pacote BiodiversityR ou o decostand do pacote vegan. Também é possível utilizar a transformação de Box-Cox para dados multivariados, disponível no material suplementar de (Pierre Legendre and Borcard 2018). aqui. Esta transformação é geralmente utilizada para tornar a distribuição dos dados mais simétrica (menos enviesada para valores extremos: reduzir o skewness dos dados). 9.2.3 Análise no R Vamos utilizar o mesmo conjunto de dados acima pra responder à mesma pergunta. Aqui vamos utilizar a distância de Chord (que é indicada para dados de composição de espécies) para calcular a matriz de distância. Se transformarmos uma matriz usando a transformação Chord e depois calcularmos a distância Euclidiana, isso equivale à calcular diretamente a distância de Chord: # Dados head(sp_compos) #&gt; BP4 PP4 PP3 AP1 AP2 PP1 PP2 BP9 PT1 PT2 PT3 BP2 PT5 #&gt; Aper 0 3 0 0 2 0 0 0 0 0 0 181 0 #&gt; Bahe 859 14 14 0 87 312 624 641 0 0 0 14 0 #&gt; Rict 1772 1517 207 573 796 0 0 0 0 0 0 0 0 #&gt; Cleuco 0 0 0 0 0 0 0 0 0 29 369 0 84 #&gt; Dmic 0 0 6 60 4 0 0 0 2758 319 25 0 329 #&gt; Dmin 0 84 344 1045 90 0 0 0 8 0 0 0 0 # Passo 1: transformar para distância de Chord bocaina_transf &lt;- disttransform(sp_compos, &quot;chord&quot;) # Passo 2: realizar pvclust com método average e distância euclidiana analise &lt;- pvclust(bocaina_transf, method.hclust=&quot;average&quot;, method.dist=&quot;euclidean&quot;) #&gt; Bootstrap (r = 0.5)... Done. #&gt; Bootstrap (r = 0.56)... Done. #&gt; Bootstrap (r = 0.69)... Done. #&gt; Bootstrap (r = 0.75)... Done. #&gt; Bootstrap (r = 0.88)... Done. #&gt; Bootstrap (r = 1.0)... Done. #&gt; Bootstrap (r = 1.06)... Done. #&gt; Bootstrap (r = 1.19)... Done. #&gt; Bootstrap (r = 1.25)... Done. #&gt; Bootstrap (r = 1.38)... Done. # Passo 3: dendrograma plot(analise, hang=-1) pvrect(analise) É possível notar que existe um único grupo com BS &gt; 95%. Agora vamos tentar usar a distância de Hellinger, que é recomendada (junto com a distância de Chord) para transformar dados de composição de espécies e, desse modo, reduzem distorções nas ordenações como PCA e CA (Pierre Legendre and Gallagher 2001). # Passo 1: transformar dados com Hellinger bocaina_transf2 &lt;- disttransform(bocaina, &quot;hellinger&quot;) # Passo 2: realizar pvclust com método average e distância euclidiana analise2 &lt;- pvclust(bocaina_transf2, method.hclust=&quot;average&quot;, method.dist=&quot;euclidean&quot;) #&gt; Bootstrap (r = 0.5)... Done. #&gt; Bootstrap (r = 0.56)... Done. #&gt; Bootstrap (r = 0.69)... Done. #&gt; Bootstrap (r = 0.75)... Done. #&gt; Bootstrap (r = 0.88)... Done. #&gt; Bootstrap (r = 1.0)... Done. #&gt; Bootstrap (r = 1.06)... Done. #&gt; Bootstrap (r = 1.19)... Done. #&gt; Bootstrap (r = 1.25)... Done. #&gt; Bootstrap (r = 1.38)... Done. # Passo 3: dendrograma plot(analise2, hang=-1) pvrect(analise2) 9.2.3.1 Interpretação dos resultados Notem que se mudarmos o coeficiente de associação, o resultado também muda. Agora temos 1 grupo a mais, composto por Dendropsophus minutus e Scinax duartei que não apareciam antes. Isso se deve ao fato de que a distância de Hellinger dá menos peso para espécies raras do que a Chord. Neste sentido, os dados não suportam a nossa hipótese inicial da formação de dois grupos, independentemente do coeficiente de associação utilizado. 9.3 K-means e agrupamentos não-hierárquicos Ao contrário do dendrograma, o K-means é um agrupamento não-hierárquico e, desse modo, não é otimizado para busca grupos menores aninhados em grupos grupos maiores. Resumidamente, podemos calcular o K-means apartir de uma matriz quadrada ou de distância. Essa técnica procura particionar os objetos em k grupos de maneira a minimizar a soma de quadrados entre grupos e maximizá-la dentro dos grupos. Um critério similar ao de uma ANOVA (cap7?). Um diferencial do K-means em relação aos agrupamentos hierárquicos é que o usuário pode escolher antecipadamente o número de grupos que deseja formar. 9.3.0.1 Exemplo 1 Para este exemplo iremos utilizar um conjunto de dados disponível no pacote ade4 que contém dados de 27 espécies de peixes coletados em 30 pontos ao longo do Rio Doubs, na fronteira entre a França e Suiça. Pergunta Qual é o número de grupos que melhor sumariza o padrão de ocorrência de espécies de peixes ao longo de um riacho? (neste caso, estamos realizando uma análise exploratória e não temos uma predição) Variáveis Variáveis resposta: composição de espécies de peixes Checklist Vamos normalizar os dados de abundância antes de entrar na análise propriamente, já que existem muitos zeros na matriz. 9.3.1 Análise # mostrar somente seis primeiras espécies de seis localidades head(doubs$fish)[,1:6] #&gt; Cogo Satr Phph Neba Thth Teso #&gt; 1 0 3 0 0 0 0 #&gt; 2 0 5 4 3 0 0 #&gt; 3 0 5 5 5 0 0 #&gt; 4 0 4 5 5 0 0 #&gt; 5 0 2 3 2 0 0 #&gt; 6 0 3 4 5 0 0 # retirar a linha 8 (rio sem nenhuma ocorrência de peixe) spe &lt;- doubs$fish[-8,] # Função do pacote vegan para normalizar os dados spe.norm &lt;- decostand(spe, &quot;normalize&quot;) O argumento centers na função abaixo indica o número de grupos que se quer formar. Neste exemplo estamos utilizando centers = 4. spe.kmeans &lt;- kmeans(spe.norm, centers = 4, nstart = 100) spe.kmeans #&gt; K-means clustering with 4 clusters of sizes 3, 12, 6, 8 #&gt; #&gt; Cluster means: #&gt; Cogo Satr Phph Neba Thth Teso Chna Chto #&gt; 1 0.00000000 0.000000000 0.00000000 0.00000000 0.000000000 0.000000000 0.05205792 0.00000000 #&gt; 2 0.10380209 0.542300691 0.50086515 0.43325916 0.114024105 0.075651573 0.00000000 0.00000000 #&gt; 3 0.06167791 0.122088022 0.26993915 0.35942538 0.032664966 0.135403325 0.06212775 0.21568957 #&gt; 4 0.00000000 0.006691097 0.02506109 0.06987391 0.006691097 0.006691097 0.10687104 0.09377516 #&gt; Lele Lece Baba Spbi Gogo Eslu Pefl Rham Legi #&gt; 1 0.07647191 0.3166705 0.00000000 0.0000000 0.20500174 0.07647191 0.00000000 0.0000000 0.05205792 #&gt; 2 0.06983991 0.1237394 0.02385019 0.0000000 0.05670453 0.04722294 0.02949244 0.0000000 0.00000000 #&gt; 3 0.25887226 0.2722562 0.15647062 0.1574388 0.16822286 0.12276089 0.17261621 0.0793181 0.06190283 #&gt; 4 0.14194394 0.2011411 0.24327992 0.1326062 0.28386032 0.20630360 0.16920496 0.2214275 0.19066542 #&gt; Scer Cyca Titi Abbr Icme Acce Ruru Blbj Alal #&gt; 1 0.07647191 0.00000000 0.00000000 0.00000000 0.0000000 0.18058775 0.31667052 0.05205792 0.7618709 #&gt; 2 0.00000000 0.00000000 0.03833408 0.00000000 0.0000000 0.00000000 0.01049901 0.00000000 0.0000000 #&gt; 3 0.04516042 0.06190283 0.14539027 0.01473139 0.0000000 0.03192175 0.32201597 0.01473139 0.1095241 #&gt; 4 0.13171275 0.16019126 0.26230024 0.19561641 0.1331835 0.26713081 0.32103755 0.22883055 0.3326939 #&gt; Anan #&gt; 1 0.00000000 #&gt; 2 0.00000000 #&gt; 3 0.04739636 #&gt; 4 0.18873077 #&gt; #&gt; Clustering vector: #&gt; 1 2 3 4 5 6 7 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #&gt; 2 2 2 2 3 2 2 3 2 2 2 2 2 2 3 3 3 3 4 4 4 1 1 1 4 4 4 4 4 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 0.3560423 2.5101386 1.7361453 0.4696535 #&gt; (between_SS / total_SS = 66.7 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; #&gt; [7] &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; O objeto que fornece o resultado contém: 1) o tamanho (número de objetos) em cada um dos 4 grupos; 2) o centroide de cada grupo e o pertencimento de cada espécie a cada grupo; e 3) o quanto da Soma de Quadrados dos dados é explicada por esta conformação de grupos. No entanto, não é possível saber a priori qual o número ideal de grupos. Para descobrir isso repetimos o k-means com uma série de valores de K. Isso pode ser feito na função cascadeKM. spe.KM.cascade &lt;- cascadeKM(spe.norm, inf.gr=2, sup.gr=10, iter=100, criterion=&quot;ssi&quot;) Tanto calinski quando ssi são bons critérios para encontrar o número ideal de grupos. Quanto maior o valor de ssi, melhor (veja ?cascadeKM mais detalhes). # Resumo dos resultados spe.KM.cascade$results #&gt; 2 groups 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups 10 groups #&gt; SSE 8.2149405 6.4768108 5.0719796 4.3015573 3.58561200 2.9523667 2.4840549 2.05218880 1.75992916 #&gt; ssi 0.1312111 0.1675852 0.1398159 0.1138008 0.08295513 0.1410657 0.1461625 0.07877382 0.06817212 SSE: critério utilizado pelo algorítimo para achar o agrupamento ótimo dos objetos. plot(spe.KM.cascade, sortg=TRUE) 9.3.2 Interpretação dos resultados Diferentemente da nossa predição inicial, o resultado da análise mostra que o número ideal de grupos para explicar a variância no padrão de ocorrência de espécies é 3. Notem que o SSI máximo é alcançado neste número de grupos 0.1675852 (também indicado pela bola vermelha no plot). 9.4 Espécies indicadoras Uma pergunta normalmente feita por ecólogos é: qual espécie pode ser indicadora de uma determinada condição ambiental (e.g., poluição)? O índice IndVal mede dois aspectos das espécies: Especificidade e fidelidade. Uma alta fidelidade significa que espécies ocorrem em todos os locais do grupo e uma alta especificidade significa que as espécies ocorrem somente naquele grupo. Uma boa espécie indicadora é aquela na qual todos os indivíduos ocorrem em todas a amostras referentes a um grupo específico. A Especificidade é dada pela divisão da abundancia média da espécie no grupo pela somatória das abundancias médias dos grupos. Fidelidade é igual ao número de lugares no grupo onde a espécie está presente dividido pelo número total de lugares do grupo (Dufrene and Legendre 1997). Espécies raras podem receber o mesmo valor de IndVal das espécies indicadoras, porém são chamadas de indicadoras assimétricas, uma vez que contribuem com a especificidade do habitat mas não servem para predizer grupos. Ao contrário, as espécies indicadoras são verdadeiros indicadores simétricos e podem ser usadas para predizer grupos. A análise procede da seguinte forma: Uma matriz de distância é construída e as unidades amostrais são classificadas com alguma análise de agrupamento, hierárquico ou não; A variável ambiental para a qual se deseja classificar os grupos é inserida; As espécies indicadoreas de cada grupo são formadas através do cálculo da especificidade e fidelidade, obtendo-se o valor de IndVal para cada espécie; Por fim, o conjunto de dados originais é comparado para ver se análise faz sentido. O cálculo da significância do índice de IndVal é feito por aleatorização de Monte Carlo. Os métodos de Monte Carlo utilizam números aleatórios de dados reais para simular certos padrões esperados na ausência de um processo ecológico específico [numerica2012]. Assim, o valor do índice é aleatorizado 999 vezes (ou o número de vezes que você optar) dentro dos tratamentos e o valor de P é dado pelo número de vezes em que o índice observado foi igual ou maior que os valores aleatorizados. 9.4.0.1 Exemplo 1 Pergunta Podemos utilizar as espécies de girinos como indicadoras da fitofisionomia? Predições Espécies terrestres serão indicadoras de área aberta, enquanto espécies arborícolas serão indicadoras de áreas florestais. Variáveis Variáveis resposta: mesma matriz já utilizada contendo a abundância de girinos ao longo de poças na Serra da Bocaina. 9.4.1 Análise no R Para este exemplo vamos utilizado o mesmo conjunto de dados utilizado acima com abundância de 16 espécies de girinos coletados em 14 poças com diferentes graus de cobertura de dossel na Serra da Bocaina. O IndVal está disponível tanto no pacote indicspecies quando no labdsv. Para este exemplo iremos usar o labdsv. Primeiro vamos agrupar as unidades amostrais (poças) que informa os grupos de fitofisionomias onde as poças se localizam e para os quais deseja-se encontrar espécies indicadoras: ## Dados head(bocaina) #&gt; BP4 PP4 PP3 AP1 AP2 PP1 PP2 BP9 PT1 PT2 PT3 BP2 PT5 #&gt; Aper 0 3 0 0 2 0 0 0 0 0 0 181 0 #&gt; Bahe 859 14 14 0 87 312 624 641 0 0 0 14 0 #&gt; Rict 1772 1517 207 573 796 0 0 0 0 0 0 0 0 #&gt; Cleuco 0 0 0 0 0 0 0 0 0 29 369 0 84 #&gt; Dmic 0 0 6 60 4 0 0 0 2758 319 25 0 329 #&gt; Dmin 0 84 344 1045 90 0 0 0 8 0 0 0 0 fitofis &lt;- c(rep(1,4), rep(2,4), rep(3,4), rep(4,4), rep(5,4)) ## Análise de espécies indicadoras res_indval &lt;- indval(bocaina, fitofis) # A função summary só exibe o resultado para as espécies indicadoras summary(res_indval) #&gt; [1] cluster indicator_value probability #&gt; &lt;0 rows&gt; (or 0-length row.names) #&gt; #&gt; Sum of probabilities = 9.324 #&gt; #&gt; Sum of Indicator Values = 3.97 #&gt; #&gt; Sum of Significant Indicator Values = 0 #&gt; #&gt; Number of Significant Indicators = 0 #&gt; #&gt; Significant Indicator Distribution #&gt; &lt; table of extent 0 &gt; Para apresentar uma tabela dos resultados para todas as espécies temos de processar os dados: res_indval$maxcls # classe de maior valor indicador / espécie #&gt; BP4 PP4 PP3 AP1 AP2 PP1 PP2 BP9 PT1 PT2 PT3 BP2 PT5 #&gt; 1 1 2 2 2 2 1 1 4 2 1 2 2 res_indval$indcls # valor indicador (indval) #&gt; BP4 PP4 PP3 AP1 AP2 PP1 PP2 BP9 PT1 #&gt; 0.4268332 0.3554217 0.3990627 0.4887564 0.5812265 0.1283151 0.2194093 0.2260226 0.3535255 #&gt; PT2 PT3 BP2 PT5 #&gt; 0.1945122 0.2341371 0.1648885 0.1991525 res_indval$pval # significância do indval #&gt; BP4 PP4 PP3 AP1 AP2 PP1 PP2 BP9 PT1 PT2 PT3 BP2 PT5 #&gt; 0.281 0.613 0.282 0.146 0.316 1.000 1.000 1.000 0.742 1.000 1.000 0.944 1.000 tab_indval &lt;- cbind.data.frame(maxcls = res_indval$maxcls, ind.value = res_indval$indcls, P = res_indval$pval) tab_indval #&gt; maxcls ind.value P #&gt; BP4 1 0.4268332 0.281 #&gt; PP4 1 0.3554217 0.613 #&gt; PP3 2 0.3990627 0.282 #&gt; AP1 2 0.4887564 0.146 #&gt; AP2 2 0.5812265 0.316 #&gt; PP1 2 0.1283151 1.000 #&gt; PP2 1 0.2194093 1.000 #&gt; BP9 1 0.2260226 1.000 #&gt; PT1 4 0.3535255 0.742 #&gt; PT2 2 0.1945122 1.000 #&gt; PT3 1 0.2341371 1.000 #&gt; BP2 2 0.1648885 0.944 #&gt; PT5 2 0.1991525 1.000 9.4.2 Interpretação dos resultados No resultado podemos ver que temos duas espécies indicadoras da fitofisionimia 1: Rhinella icterica (Rict) e Scinax duartei (Sduar). Nenhuma espécie foi indicadora dos outros grupos neste exemplo. 9.5 Análises de Ordenação Os análises de ordenação representam um conjunto de métodos e técnicas multivariadas que buscam organizar objetos (e.g., localidades, indivíduos, espécies) em alguma ordem. Por exemplo, tais métodos permitem identificar se existem grupo de espécies que ocorrem exclusivamente em um determinado hábitat. Ao buscar esta ordem as técnicas de ordenação possuem três principais utilidades: (1) reduzir a dimensionalidade e revelar padrões, (2) separar variáveis mais e menos importantes em combinações complexas, e (3) separar relações mais e menos fortes ao comparar variáveis preditoras e dependentes. Em geral, os métodos são dividídos em ordenações irrestritas (ou análise de gradiente indireto) e restritas (ou análise de gradiente direto). As ordenações irrestritas organizam os objetos (e.g., espécies) de acordo com sua estrutura de covariância (ou correlação), o que demonstra que a proximidade (ou distância) dentro do espaço multidimensional representa semelhança (ou diferença) dos objetos. Por outro lado, as ordenações restritras posiciona os objetos (e.g., espécies) de acordo com sua relação linear com outras variáveis coletadas nas mesmas unidades amostraits (e.g., temperatura e precipitação). Ao passo que as ordenações irrestritas dependem somente de uma matriz (e.g., espécies por localidades), as ordenações restritas utilizam no mínimo duas matrizes (e.g., espécies por localidades e variáveis climáticas por localidade). Desse modo, fica claro por esta diferença entre os dados utilizados que as análises irrestritas são mais exploratórias, enquanto análises restritas são ideias para testar hipóteses com dados multidimensionais. A tabela a seguir apresenta as principais análises utilizadas em ecologia. Método Tipo de variável Função R Ordenação irrestrita PCA Variáveis contínuas (distância euclidiana) PCA, rda, dudi.pca PCoA Aceita qualquer tipo de variável, mas depende da escolha apropriada de uma medida de distância pcoa, dudi.pco nMDS Aceita qualquer tipo de variável, mas depende da escolha apropriada de uma medida de distância metaMDS, nmds CA dudi.coa Hill-Smith Aceita qualquer tipo de variável dudi.hillsmith Ordenação restrita RDA Variáveis preditoras de qualquer tipo e variáveis dependentes contínuas (ou presença e ausência) rda RDA parcial Variáveis preditoras de qualquer tipo e variáveis dependentes contínuas (ou presença e ausência) rda dbRDA Variáveis preditoras de qualquer tipo e matriz de distância obtida a partir das variáveis dependentes capscale, dbrda CCA Variáveis preditoras de qualquer tipo e variáveis dependentes contínuas (ou presença e ausência) rda PERMANOVA Variáveis preditoras de qualquer tipo e matriz de distância obtida a partir das variáveis dependentes adonis, adonis2 PCR Variável dependente necessariamente representada por escores da PCA ou PCoA e variáveis preditoras de qualquer tipo pca, pcoa, lm, glm 9.5.1 Ordenações irrestritas 9.5.1.1 Análise de Componentes Principais - PCA A PCA é uma das ordenações mais utilizadas em diversas áreas do conhecimento. Em ecologia, ela se popularizou por facilitar a visualização de dados complexos como de distribuição de espécies em diferentes localidades e de potenciais variáveis explicativas. Ao mesmo tempo que ganhou tamanha popularidade, a PCA tem sido empregada de maneira incorreta, uma vez que muitos estudos utilizam a visualização gráfica da ordenação (o biplot) para intepretar relações entre variáveis preditoras (ambientais) e dependentes (espécies). Porém, como informado anteriormente, as ordenações irrestritas utilizam a estrutura de covariância dos objetos para organizar suas relações de similaridade. Antes de explicar a análise, imagine que vamos usar uma matriz com cinco espécies de aranhas que foram encontradas em oito cidades diferentes. A quantidade de indivíduos de cada espécie coletada em cada cidade será o valor de preenchimento desta matriz. Sendo assim, a matriz possui oito objetos (cidades, representando unidades amostrais) e cinco descritores (espécies), como na tabela abaixo: Cidade sp1 sp2 sp3 sp4 sp5 Cidade 1 5 0 0 0 0 Cidade 2 7 6 0 0 0 Cidade 3 2 3 0 0 0 Cidade 4 0 4 9 0 0 Cidade 5 0 0 12 4 0 Cidade 6 0 0 3 10 6 Cidade 7 0 0 0 8 9 Cidade 8 0 0 0 0 12 O primeiro passo da PCA é obter uma matriz centralizada onde cada valor é subtraído da média da coluna que aquele valor pertence. Esta centralização pode ser calculada com a função scale. aranhas &lt;- data.frame(sp1 = c(5, 7, 2, 0, 0, 0, 0, 0), sp2 = c(0, 6, 3, 4, 0, 0, 0, 0), sp3 = c(0, 0, 0, 9, 12, 3, 0, 0), sp4 = c(0, 0, 0, 0, 4, 10, 8, 0), sp5 = c(0, 0, 0, 0, 0, 6, 9, 12), row.names = paste(&quot;cidade&quot;, 1:8, sep=&quot;&quot;)) aranha.cent &lt;- as.data.frame(base::scale(aranhas, center = TRUE, scale=FALSE)) O segundo passo é calcular uma matriz de covariância (ou matriz de dispersão) e, a partir desta matriz, obter os autovalores e autovetores. Os autovalores representam a porcentagem de explicação de cada eixo e podem ser calculados dividindo a soma do autovalor de cada eixo pela soma de todos os autovalores. No exemplo que apresentamos, os dois primeiros eixos representam 47,20% e 35,01%, respectivamente, de toda variação. Os autovetores, por sua vez, representam os valores que multiplicam as variáveis originais e, desse modo, indicam a direção desses valores. Por fim, os componentes principais (Matriz F) são obtidos multiplicando os autovetores com os valores da matriz centralizada. ## Matriz de covaiância matriz_cov &lt;- cov(aranha.cent) ## Autovalores e autovetores eigen_aranhas &lt;- eigen(matriz_cov) autovalores&lt;- eigen_aranhas$values autovetores &lt;- as.data.frame(eigen_aranhas$vectors) autovalores # eigenvalue #&gt; [1] 36.733031 27.243824 9.443805 2.962749 1.438020 colnames(autovetores) &lt;- paste(&quot;PC&quot;, 1:5, sep=&quot;&quot;) rownames(autovetores) &lt;- colnames(aranhas) autovetores #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; sp1 -0.2144766 0.38855265 0.29239380 -0.02330706 0.8467522 #&gt; sp2 -0.2442026 0.17463316 0.01756743 0.94587037 -0.1220204 #&gt; sp3 -0.3558368 -0.80222917 -0.27591770 0.10991178 0.3762942 #&gt; sp4 0.4159852 -0.41786654 0.78820962 0.17374202 0.0297183 #&gt; sp5 0.7711688 0.01860152 -0.46560957 0.25003826 0.3544591 matriz_F &lt;- as.data.frame(as.matrix(aranha.cent)%*%as.matrix(autovetores)) matriz_F #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; cidade1 -2.979363 4.4720575 1.1533417 -3.2641923 0.5433206 #&gt; cidade2 -4.873532 6.2969618 1.8435339 2.3644158 1.5047024 #&gt; cidade3 -3.068541 3.8302991 0.3288626 -0.3566600 -2.3629973 #&gt; cidade4 -6.086322 -3.9922356 -2.7216169 1.6250305 -0.7918743 #&gt; cidade5 -4.513082 -8.7689219 -0.4668012 -1.1337476 0.9439633 #&gt; cidade6 5.812374 -3.9444494 3.9520584 0.4197281 -0.1376205 #&gt; cidade7 8.361421 -0.6462243 1.8065636 0.4926235 -0.2625625 #&gt; cidade8 7.347046 2.7525126 -5.8959421 -0.1471979 0.5630683 ## Porcentagem de explicação de cada eixo 100* (autovalores / sum(autovalores)) #&gt; [1] 47.201691 35.008126 12.135225 3.807112 1.847846 Agora, é possível visualizar a relação entre as cidades e similaridade na espécies de aranhas que vivem em cada uma delas. matriz_F %&gt;% ggplot(aes(x = PC1, y = PC2, label = rownames(matriz_F))) + theme_bw() + geom_label() + geom_hline(yintercept = 0, linetype=2) + geom_vline(xintercept = 0, linetype=2) + theme(axis.title.x = element_text(size=14), axis.text.x = element_text(vjust=0.5, size=12), axis.title.y = element_text(size=14), axis.text.y = element_text(vjust=0.5, size=12)) Checklist Verifique se todas as variáveis utilizadas são contínuas. Caso contrário, considere utilizar PCoA. Apesar do exemplo acima ter apresentado a ocorrência de espécies de aranhas em diferentes cidades, é fundamental saber que utilizar PCA com esses dados pode ser problemático. Assim, tenha cuidado em usar de composição de espécies (especialmente abundância) com PCA, uma vez que duplos zeros podem gerar distorções na ordenação (P. Legendre and Legendre 2012a). Como alternativa, é possível utilizar PCA com dados padronizados com o método de Hellinger (Pierre Legendre and Gallagher 2001). 9.5.1.2 Exemplo 1 Neste exemplo vamos utilizar um conjunto de dados morfológicos de pinguins do arquipélago Palmer (Península Antártica) disponíveis no pacote palmerpenguins. Os dados representam medidas do comprimento e largura do bico (mm), comprimento da nadadeira (mm) e massa corporal (gramas) de três espécies: Adélie, Chinstrap e Gentoo. Como descrito acima, a PCA deve ser utilizada para exploração de dados ou para testes a posteriori (p. ex., PCR). Neste exemplo, iremos usar a estrutura de perguntas e predições para manter a proposta do livro. Pergunta Existe diferenças nas características morfológicas das espécies de pinguins do arquipélago Palmer? Predições Pinguins com dieta diferente possuem differentes características morfológicas. Variáveis Preditora: espécie (categórica com três níveis) Dependentes: variáveis morfológicas (contínua) 9.5.1.3 Análise no R Antes de começar, é necessário remover dados ausentes (se houver) e editar nomes das variáveis (ponto importante para determinar como devem aparecer no gráfico). ## Verificar se existem NAs nos dados. sum(is.na(penguins)) #&gt; [1] 19 ## Remover dados ausentes (NA), quando houver. penguins &lt;- na.omit(penguins) ## Editar nomes para aparecer nos gráficos. names(penguins) &lt;- c(&quot;species&quot;, &quot;island&quot;, &quot;Bill length&quot;, &quot;Bill depth&quot;, &quot;Flipper length&quot;, &quot;Body mass&quot;, &quot;Sex&quot;, &quot;Year&quot;) ## Manter somentes dados contínuos que pretende aplicar a PCA. penguins_trait &lt;- penguins[,3:6] Agora sim os dados estão prontos para fazer a PCA. Um argumento é essencial na análise, o scale.unit. Se você utiliar dentro deste argumento o seleção TRUE, a função padroniza automaticamente as variáveis para terem a média 0 e variância 1. Esta padronização é essencial quando as variáveis estão em escalas muito diferentes. No exemplo selecionado, temos variáveis como comprimento do bico (em milímetros) e massa corporal (em gramas). # Compare com este código a variância das variáveis penguins_trait %&gt;% dplyr::summarise(across(where(is.numeric), ~var(.x, na.rm=TRUE))) #&gt; # A tibble: 1 x 4 #&gt; `Bill length` `Bill depth` `Flipper length` `Body mass` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 29.9 3.88 196. 648372. # Agora, veja o mesmo cálculo se fizer a padronização (scale.unit da função PCA) penguins_pad &lt;- decostand(penguins_trait, &quot;standardize&quot;) penguins_pad %&gt;% dplyr::summarise(across(where(is.numeric), ~var(.x, na.rm=TRUE))) #&gt; Bill length Bill depth Flipper length Body mass #&gt; 1 1 1 1 1 # PCA pca.p &lt;- PCA(penguins_trait, scale.unit = TRUE, graph = FALSE) Apesar da simplicidade do comando para executar a PCA, o objeto resultante da análise possui diversas informações que são essenciais para sua plena interpretação. Dentre elas, se destacam os autovalores, escores e loadings. Os autovalores representam a porcentagem de explicação de cada eixo. O escores representam as coordenadas (posições no espaço multidimensional) representando os objetos (geralmente localidades ou indivíduos) e descritores (geralmente espécies ou variáveis ambientais e espaciais). Os loadings, por sua vez, representam a combinação linear entre os escores (nova posição do valor do descritor no espaço ordenado) e os valores originais dos descritores. ## Autovalores: porcentagem de explicação para usar no gráfico pca.p$eig #&gt; eigenvalue percentage of variance cumulative percentage of variance #&gt; comp 1 2.7453557 68.633893 68.63389 #&gt; comp 2 0.7781172 19.452929 88.08682 #&gt; comp 3 0.3686425 9.216063 97.30289 #&gt; comp 4 0.1078846 2.697115 100.00000 ## Visualização da porcentagem de explicação de cada eixo # nota: é necessário ficar atento ao valor máximo do eixo 1 da análise para determinar o valor do ylim (neste caso, colocamos que o eixo varia de 0 a 70) fviz_screeplot(pca.p, addlabels = TRUE, ylim = c(0, 70)) ## Outros valores importantes var_env &lt;- get_pca_var(pca.p) # Escores (posição) das variáveis em cada eixo var_env$coord #&gt; Dim.1 Dim.2 Dim.3 Dim.4 #&gt; Bill length 0.7518288 0.52943763 -0.3900969 -0.04768208 #&gt; Bill depth -0.6611860 0.70230869 0.2585287 0.05252186 #&gt; Flipper length 0.9557480 0.00510580 0.1433474 0.25684871 #&gt; Body mass 0.9107624 0.06744932 0.3592789 -0.19204478 # Contribuição (%) das variáveis para cada eixo var_env$contrib #&gt; Dim.1 Dim.2 Dim.3 Dim.4 #&gt; Bill length 20.58919 36.023392267 41.279994 2.107420 #&gt; Bill depth 15.92387 63.388588337 18.130600 2.556942 #&gt; Flipper length 33.27271 0.003350291 5.574092 61.149849 #&gt; Body mass 30.21423 0.584669105 35.015313 34.185789 # loadings - correlação das variáveis com os eixos var_env$cor #&gt; Dim.1 Dim.2 Dim.3 Dim.4 #&gt; Bill length 0.7518288 0.52943763 -0.3900969 -0.04768208 #&gt; Bill depth -0.6611860 0.70230869 0.2585287 0.05252186 #&gt; Flipper length 0.9557480 0.00510580 0.1433474 0.25684871 #&gt; Body mass 0.9107624 0.06744932 0.3592789 -0.19204478 # Qualidade da representação da variável. Esse valor é obtido multiplicado var_env$coord por var_env$coord var_env$cos2 #&gt; Dim.1 Dim.2 Dim.3 Dim.4 #&gt; Bill length 0.5652466 2.803042e-01 0.15217561 0.002273581 #&gt; Bill depth 0.4371669 4.932375e-01 0.06683710 0.002758546 #&gt; Flipper length 0.9134542 2.606919e-05 0.02054847 0.065971260 #&gt; Body mass 0.8294881 4.549411e-03 0.12908133 0.036881196 # Escores (posição) das localidades (&quot;site scores&quot;) em cada eixo ind_env &lt;- get_pca_ind(pca.p) O pacote FactoMineR criou uma função (dimdesc) que seleciona as melhores variáveis (aquelas mais explicativas) para cada eixo através de uma análise fatorial. No exemplo com pinguins, o primeiro eixo (objeto pca.p$eig) explica ~69% da variação morfológica. A função dimdesc mostra que as quatro variáveis morfológicas estão fortemente associadas com o eixo 1. Porém, enquanto comprimento da nadadeira, massa corporal e comprimento do bico estão positivamente associados com o eixo 1 (correlação positiva), a largura do bico tem relação negativa. O eixo 2, por sua vez, explica ~20% da variação, sendo relacionado somente com largura e comprimento do bico. # Variáveis mais importantes para o Eixo 1 dimdesc(pca.p)$Dim.1 #&gt; $quanti #&gt; correlation p.value #&gt; Flipper length 0.9557480 5.962756e-178 #&gt; Body mass 0.9107624 3.447018e-129 #&gt; Bill length 0.7518288 7.830597e-62 #&gt; Bill depth -0.6611860 3.217695e-43 #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;condes&quot; &quot;list&quot; # Variáveis mais importantes para o Eixo 2 dimdesc(pca.p)$Dim.2 #&gt; $quanti #&gt; correlation p.value #&gt; Bill depth 0.7023087 8.689230e-51 #&gt; Bill length 0.5294376 1.873918e-25 #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;condes&quot; &quot;list&quot; Agora podemos utilizar o famoso biplot para representar a comparação morfológica dos pinguins dentro e entre espécies fviz_pca_biplot(pca.p, geom.ind = &quot;point&quot;, fill.ind = penguins$species, col.ind = &quot;black&quot;, alpha.ind=0.7, pointshape = 21, pointsize = 4, palette = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;), addEllipses = FALSE, alpha.var = 1, col.var = &quot;black&quot;, gradient.cols = &quot;RdBu&quot;, invisible = &quot;quali&quot;, title = NULL) + theme_bw() + xlab(&quot;PC 1 (68.63%)&quot;) + ylab(&quot;PC 2 (19.45%)&quot;) + theme(axis.title.x = element_text(size=14), axis.text.x = element_text(vjust=0.5, size=12), axis.title.y = element_text(size=14), axis.text.y = element_text(vjust=0.5, size=12), legend.position = &quot;top&quot;, legend.title = element_blank())+ tema_livro() 9.5.2 Análise de Coordenadas Principais - PCoA Diferentemente da PCA, a PCoA é uma análise de ordenação irrestrita que aceita dados de diferentes tipos, como contínuos, categóricos, ordinais, binários, entre outros. Assim, a PCoA é aplicada para casos em que a distância euclidiana não é aplicada (como na PCA). Desse modo, o primeiro passo da análise é calcular uma matriz de similaridade ou de distância (discutido acima). Depois, os passos para obter autovalores e autovetores são bastante parecidos com a PCA. Da mesma forma, os eixos da PCoA e os valores ou posições dos objetos nesses eixos representam a relação de semelhança (ou diferença) baseada nos descritores desses objetos. A diferença, neste caso, é que a PCoA representa um espaço não-euclidiano, que irá ser afetado pela escolha do método de similaridade. As utilizações mais comuns da PCoA são a ordenação (1) da matriz de composição de espécies usando a distância apropriada (Jaccard, Sorensen, Bray Curits), (2) da matriz de variáveis ambientais com mistos (contínuos, categóricos, circulares, etc), e (3) da matriz filogenética (método PVR Jose Alexandre Felizola Diniz-Filho, SantAna, and Bini 1998). Abaixo, exemplificamos a ordenação da matriz de composição de espécies. Checklist Compare as dimensões das matrizes utilizadas para a PCoA. Com bastante frequência, a tentativa de combinar dados categóricos (algum descritor dos objetos) com os valores obtidos com a PCoA gera erros para plotar a figura ou para executar a análise. Verifique, então, se as linhas são as mesmas (nome das localidades ou indivíduos e quantidade). É fundamental conhecer o tipo de dados que está usando para selecionar a medida de distância apropriada. Essa escolha vai afetar a qualidade da ordenação e sua habilidade para interpretar a relação de semelhança entre os objetos comparados. Diferente da PCA, a PCoA aceita dados ausentes se a medida de distância escolhida também não tiver esta limitação. Por exemplo, a distância de Gower produz matrizes de similaridade mesmo com dados ausentes em determinados objetos. Em alguns casos, a autovalores negativos são produzidos na ordenação com PCoA (cap9?). Apesar deste problema, os autovalores mais importantes (eixos iniciais) não são afetados e, deste modo, a qualidade da representação dos objetos no espaço multidimensional não é afetada. Alguns autores sugerem utilizar correções métodos de correção, como Lingoes ou Cailliez (P. Legendre and Legendre 2012a). 9.5.2.1 Exemplo 1 Neste exemplo vamos utilizar a composição de ácaros Oribatidae em 70 manchas de musgo coletados por Borcard et al. (1992). Pergunta A composição de espécies de ácaros muda entre diferentes topografias? Predições Iremos encontrar ao menos dois grupos de espécies: aquelas que ocorrem em poças dentro de floresta vs. aquelas que ocorrem em poças de áreas abertas. Variáveis Preditora: topografia (categórica com dois níveis) Dependentes: composição de espécies de ácaro 9.5.2.2 Análise no R # Padronização dos dados com Hellinger mite.hel &lt;- decostand(mite, &quot;hellinger&quot;) # Cálculo da matriz de distância com método Bray Curtos sps.dis &lt;- vegdist(mite.hel, &quot;bray&quot;) # PCoA pcoa.sps &lt;- pcoa(sps.dis, correction=&quot;cailliez&quot;) Assim como na PCA, a porcentagem de explicação dos eixos é uma das informações mais importantes pois descrevem a efetividade da redução da dimensionalidade dos dados. ## Porcentagem de explicação do Eixo 1 100*(pcoa.sps$values[,1] / pcoa.sps$trace)[1] #&gt; [1] 49.10564 ## Porcentagem de explicação dos Eixo 2 100*(pcoa.sps$values[,1] / pcoa.sps$trace)[2] #&gt; [1] 14.30308 ## Porcentagem de explicação acumulada dos dois primeiros eixos sum(100*(pcoa.sps$values[,1] / pcoa.sps$trace)[1:2]) #&gt; [1] 63.40872 # Selecionar os dois primeiros eixos eixos &lt;- pcoa.sps$vectors[,1:2] ## Juntar com algum dado categórico de interesse para fazer a figura pcoa.dat &lt;- data.frame(topografia=mite.env$Topo, eixos) Para visualizar os resultados da PCoA, vamos exportar os escores dos eixos para usar no ggplot2. ## Escores dos dois primeiros eixos eixos &lt;- pcoa.sps$vectors[,1:2] ## Combinar dados dos escores com um dado categórico de interesse para nossa pergunta pcoa.dat &lt;- data.frame(topografia=mite.env$Topo, eixos) ### Gráfico biplot da PCoA pcoa.dat %&gt;% ggplot(aes(x = Axis.1, y = Axis.2, fill = topografia, color = topografia, shape = topografia)) + theme_bw() + geom_point(size=4, alpha = 0.7) + scale_shape_manual(values=c(21, 22)) + scale_color_manual(values=c(&quot;black&quot;, &quot;black&quot;)) + scale_fill_manual(values=c(&quot;darkorange&quot;, &quot;cyan4&quot;)) + xlab(&quot;PCO 1 (49.11%)&quot;) + ylab(&quot;PCO 2 (14.30%)&quot;) + theme(legend.position = &quot;top&quot;, legend.title=element_blank()) + geom_hline(yintercept = 0, linetype=2) + geom_vline(xintercept = 0, linetype=2)+ tema_livro() 9.5.2.3 Limitações importantes das ordenações irrestritas Com frequência, pesquisadores utilizam análises como PCA e PCoA para testar diferenças na composição de espécies entre determinados fatores relevantes (altitude, clima, etc). Porém, como falado acima, as análises de ordenação irrestritas não são utilizadas para testar qualquer hipótese. Ao invés disso, essas análises representam uma poderosa ferramente para explorar padrões em variáveis dependentes ou independentes para ajudar na interpretação ou mesmo para testar hipóteses em análises combinadas com as ordenações irrestritas. 9.6 PCR - Regressão de Componentes Principais Uma maneira de testar hipóteses utilizando ordenações irrestritas é utilizando os resultados da ordenação (escores) como variáveis preditoras ou dependentes como, por exemplo, em modelos lineares (e.g., regressão múltipla). O primeiro passo é utilizar uma ordenação, como a PCA, para gerar os novos dados que serão usados na análise. A utilização desses novos dados (que representam as coordenadas principais ou escores da PCA) vai depender da pergunta em questão. Por exemplo, pode ser que esses valores representem gradientes climáticos e, por este motivo, serão utilizados como variáveis preditoras em um modelo linear (e.g., regressão múltipla). Por outro lado, esses valores podem representar o espaço morfológicos de espécies de peixe e, como consequência, serão utilizados como variáveis dependentes para entender o efeito da presença de predador sobre a morfologia. Checklist Compare as dimensões das matrizes utilizadas para a PCR. Com bastante frequência, a tentativa de combinar dados categóricos (algum descritor dos objetos) com os valores obtidos com a PCoA gera erros para plotar a figura ou para executar a análise. Verifique, então, se as linhas são as mesmas (nome das localidades ou indivíduos e quantidade). Estudos recentes têm criticado a utilização de PCR para testar hipóteses ecológicas pelo fato dos escores não representarem, necessariamente, a variação total das variáveis originais, bem como a relação entre a variável preditora e a dependente. 9.6.0.1 Exemplo 1 Neste exemplo vamos utilizar a composição de espécies de aves em 23 regiões dos alpes franceses. Os dados ambientais (env) representam variáveis climáticas (temperatura e chuva) e altitude. Pergunta Gradientes climáticos afetam a riqueza de aves? Predições O aumento da umidade e redução da temperatura aumentam o número de espécies de aves. Variáveis Preditora: temperatura e chuva (contínuas) e altitude (categórica com três níveis) Dependentes: riqueza de espécies de aves # Dados env_cont &lt;- env[,-8] env.pca &lt;- PCA(env_cont, scale.unit = TRUE, graph = FALSE) var_env &lt;- get_pca_var(env.pca) # Contribuição (%) das variáveis para cada eixo var_env$contrib #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; mini.jan 10.93489 22.2975487 16.1607726 7.6025527 0.01782438 #&gt; maxi.jan 20.18065 3.2890767 2.1814486 4.2756350 41.05646526 #&gt; mini.jul 11.87396 21.1379132 0.3428843 0.7750666 44.70209396 #&gt; maxi.jul 18.47244 0.9159957 56.5369988 9.4368661 2.59283074 #&gt; rain.jan 9.95206 21.5387403 6.5737927 53.7375738 4.44283706 #&gt; rain.jul 16.14997 11.2368132 7.2608047 19.6972097 0.71454880 #&gt; rain.tot 12.43603 19.5839121 10.9432983 4.4750959 6.47339980 # Loadings - correlação das variáveis com os eixos var_env$cor #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; mini.jan 0.6830371 0.6766524 -0.21924927 0.12298817 -0.004517369 #&gt; maxi.jan 0.9279073 0.2598807 -0.08055260 0.09223249 0.216804944 #&gt; mini.jul 0.7117620 0.6588220 0.03193603 -0.03926930 -0.226225907 #&gt; maxi.jul 0.8877675 0.1371462 0.41008461 -0.13702428 0.054483561 #&gt; rain.jan -0.6516187 0.6650391 -0.13983474 -0.32698110 0.071319550 #&gt; rain.jul -0.8300858 0.4803509 0.14696011 0.19796389 -0.028601865 #&gt; rain.tot -0.7284135 0.6341424 0.18041856 0.09435932 0.086088397 ind_env &lt;- get_pca_ind(env.pca) env.pca$eig #&gt; eigenvalue percentage of variance cumulative percentage of variance #&gt; comp 1 4.26652359 60.9503370 60.95034 #&gt; comp 2 2.05340251 29.3343216 90.28466 #&gt; comp 3 0.29745014 4.2492878 94.53395 #&gt; comp 4 0.19896067 2.8422953 97.37624 #&gt; comp 5 0.11448717 1.6355310 99.01177 #&gt; comp 6 0.04312874 0.6161248 99.62790 #&gt; comp 7 0.02604718 0.3721025 100.00000 O objeto env.pca$eig demonstra que os três primeiros eixos explicam 94.54% da variação total dos dados climáticos. Como o intuito da PCR é reduzir a dimensionalidade (ou seja, o número de variáveis preditoras ou depedentes) para facilitar a interpretação e garantir que as variáveis não sejam correlacionadas. O próximo passo então é obter os valores dos escores que representam os valores convertidos para serem usados em uma determinada análise, como a regressão múltipla. # Passo 1: obter os primeiros eixos pred.env &lt;- ind_env$coord[,1:3] # Passo 2: calcular a riqueza de espécies riqueza &lt;- specnumber(species) # Passo 3: combinar os dois valores em um único data.frame dat &lt;- data.frame(pred.env, riqueza) Agora que os dados foram combinados em uma única matriz, podemos utilizar os comandos aprendidos no (cap7?) para testar nossa hipótese. # Regressão múltipla mod1 &lt;- lm(riqueza~Dim.1+Dim.2+Dim.3, data = dat) par(mfrow=c(2,2)) plot(mod1) # verificar pressupostos dos modelos lineares summary(mod1) # resultados do teste #&gt; #&gt; Call: #&gt; lm(formula = riqueza ~ Dim.1 + Dim.2 + Dim.3, data = dat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.4008 -1.1729 0.4356 1.2072 2.4571 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 13.30435 0.37639 35.347 &lt; 2e-16 *** #&gt; Dim.1 0.68591 0.18222 3.764 0.00131 ** #&gt; Dim.2 -0.09961 0.26267 -0.379 0.70874 #&gt; Dim.3 -0.21708 0.69014 -0.315 0.75654 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.805 on 19 degrees of freedom #&gt; Multiple R-squared: 0.4313, Adjusted R-squared: 0.3415 #&gt; F-statistic: 4.804 on 3 and 19 DF, p-value: 0.01179 dimdesc(env.pca)$Dim.1 #&gt; $quanti #&gt; correlation p.value #&gt; maxi.jan 0.9279073 1.846790e-10 #&gt; maxi.jul 0.8877675 1.607390e-08 #&gt; mini.jul 0.7117620 1.396338e-04 #&gt; mini.jan 0.6830371 3.282701e-04 #&gt; rain.jan -0.6516187 7.559358e-04 #&gt; rain.tot -0.7284135 8.112903e-05 #&gt; rain.jul -0.8300858 9.588034e-07 #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;condes&quot; &quot;list&quot; Como percebemos, a Dim.1 foi o único gradiente ambiental que afetou a riqueza de espécies. Para interpretar esta dimensão (e outras importantes), podemos usar a função dimdesc para verificar as variáveis mais importantes. Neste caso, os valores mais extremos de correlação (maior que 0.8) indicam que a temperatura do mês de janeiro e julho bem como a chuva do mês de julho foram as variáveis mais importantes para determinar o gradiente ambiental expresso na dimensão 1. Assim, podemos fazer um gráfico para representar a relação entre Eixo 1 (gradiente chuva-temperatura) e a riqueza de espécies de aves. Valores negativos do eixo 1 (Gradiente ambiental - PC1) representam localidades com mais chuva, ao passo que valores positivos indicam localidades com temperaturas maiores. dat %&gt;% ggplot(aes(x = Dim.1, y = riqueza)) + theme_bw() + geom_smooth(method = lm, fill = &quot;#525252&quot;, color = &quot;black&quot;) + geom_point(size=4, shape=21, alpha = 0.7, color=&quot;#1a1a1a&quot;, fill=&quot;cyan4&quot;) + xlab(&quot;Gradiente ambiental (PC1)&quot;) + ylab(&quot;Riqueza de aves&quot;) + theme(axis.title.x = element_text(size=14), axis.text.x = element_text(vjust=0.5, size=12), axis.title.y = element_text(size=14), axis.text.y = element_text(vjust=0.5, size=12))+ tema_livro() 9.6.0.2 Exemplo 2 É possível que os dados utilizados em seu estudo sejam mistos, ou seja, incluem tanto variáveis categóricas quanto contínuas. Como falado acima, nesses casos a análise indicada é a PCoA. Assim como na PCA, podemos extrair os escores da PCoA para utilizar a posteriori em análises univariadas e multivariadas. Pergunta: Variáveis climáticas, vegetacionais e topográficas afetam a riqueza de ácaros? Predições A densidade da vegetação e disponibilidade de água aumentam a riqueza de espécies de ácaros. Variáveis Preditoras: densidade de substrado e disponibilidade de água (contínuas), tipo de substrado (categórica com 7 níveis), densidade arbusto (ordinal com 3 níveis), e topografia (categórica com 2 níveis) Dependentes: riqueza de espécies de ácaros O primeiro passo então é utilizar um método de distância apropriado para o seu conjunto de dados. Em nosso exemplo, utilizaremos a distância de Gower, que é usada para dados mistos ((cap14?)). ## Matriz de distância env.dist &lt;- gowdis(mite.env) ## PCoA env.mite.pco &lt;- pcoa(env.dist, correction=&quot;cailliez&quot;) ## Porcentagem de explicação do Eixo 1 100*(env.mite.pco$values[,1] / env.mite.pco$trace)[1] #&gt; [1] 61.49635 ## Porcentagem de explicação dos Eixo 2 100*(env.mite.pco$values[,1] / env.mite.pco$trace)[2] #&gt; [1] 32.15486 O próximo passo é exportar os escores para as análises a posteriori. ## Selecionar os dois primeiros eixos pred.scores.mite &lt;- env.mite.pco$vectors[,1:2] ## Juntar com os dados da área para fazer a figura mite.riqueza &lt;- specnumber(mite) pred.vars &lt;- data.frame(riqueza=mite.riqueza, pred.scores.mite) ### Regressão múltipla mod.mite &lt;- lm(riqueza~Axis.1+Axis.2, data=pred.vars) par(mfrow=c(2,2)) plot(mod.mite) summary(mod.mite) #&gt; #&gt; Call: #&gt; lm(formula = riqueza ~ Axis.1 + Axis.2, data = pred.vars) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -10.6874 -2.3960 -0.1378 2.5032 8.6873 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 15.1143 0.4523 33.415 &lt; 2e-16 *** #&gt; Axis.1 -11.4303 2.0013 -5.711 2.8e-07 *** #&gt; Axis.2 5.6832 2.7677 2.053 0.0439 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.784 on 67 degrees of freedom #&gt; Multiple R-squared: 0.3548, Adjusted R-squared: 0.3355 #&gt; F-statistic: 18.42 on 2 and 67 DF, p-value: 4.225e-07 Finalmente, após interpretar os resultados do modelo, podemos fazer a figura com as variáveis (eixos) importantes g_acari_axi1 &lt;- pred.vars %&gt;% ggplot(aes(x = Axis.1, y = riqueza)) + theme_bw() + geom_smooth(method = lm, fill = &quot;#525252&quot;, color = &quot;black&quot;) + geom_point(size=4, shape=21, alpha = 0.7, color=&quot;#1a1a1a&quot;, fill=&quot;cyan4&quot;) + xlab(&quot;Gradiente ambiental (PC1)&quot;) + ylab(&quot;Riqueza de ácaros&quot;) + theme(axis.title.x = element_text(size=14), axis.text.x = element_text(vjust=0.5, size=12), axis.title.y = element_text(size=14), axis.text.y = element_text(vjust=0.5, size=12))+ tema_livro() g_acari_axi2 &lt;-pred.vars %&gt;% ggplot(aes(x = Axis.2, y = riqueza)) + theme_bw() + geom_smooth(method = lm, fill = &quot;#525252&quot;, color = &quot;black&quot;) + geom_point(size=4, shape=21, alpha = 0.7, color=&quot;#1a1a1a&quot;, fill=&quot;darkorange&quot;) + xlab(&quot;Gradiente ambiental (PC2)&quot;) + ylab(&quot;Riqueza de ácaros&quot;) + theme(axis.title.x = element_text(size=14), axis.text.x = element_text(vjust=0.5, size=12), axis.title.y = element_text(size=14), axis.text.y = element_text(vjust=0.5, size=12))+ tema_livro() ## Função para combinar os dois plots em uma única janela grid.arrange(g_acari_axi1, g_acari_axi2, nrow=1) 9.7 Ordenação restrita A ordenação restrita, ou análise de gradiente direto organiza os objetos de acordo com suas relações com outras variáveis (preditoras) coletadas nas mesmas unidades amostrais. O exemplo mais comum na ecologia é de investigar a relação entre diversas variáveis ambientais (matriz X) coletadas em n localidades e a abundância (ou presença ausência) de y espécies coletadas nas mesmas localidades (matrix Y). Com frequência, outras dados são utilizados como as coordenadas geográficas das unidades amostrais (matriz W), os atributos funcionais das espécies coletadas (matriz T) e a relação filogenética dessas espécies (matriz P). Diversos métodos são utilizados para combinar duas ou mais matrizes, mas neste capítulo iremos apresentar a RDA, RDAp e métodos espaciais para incluir a matriz W nas análises de gradiente direto. 9.7.1 RDA: Análise de Redundância A RDA é uma análise semelhante a regressão múltipla (cap7?) mas que usa dados multivariados como variável dependente. As duas matrizes comuns, matriz X (n unidades amostraits e m variáveis) e matriz Y (n unidades amostrais e p descritores - geralmente, espécies). O primeiro passo da RDA é centralizar (assim como na PCA, exemplo acima) as matrizes X e Y. Após a centralização, realiza-se regressões lineares entre X e Y para obter os valores preditos de Y (ou seja, os valores de Y que representação uma combinação linear com X). O passo seguinte é realizar uma PCA dos valores preditos de Y. Este último procedimento gera os autovalores, autovetores e os eixos canônicos que correspondem às coordenadas dos objetos (unidades amostrais), variáveis preditoras e das variáveis resposta. A diferença da ordenação do valor de Y predito e da ordenação somente de Y (como na PCA implementada acima) é que a segunda mostra a posição prevista pela relação linear entre X e Y. Logo, essa é exatamente o motivo da ordenação ser conhecida como restrita, pois a variação em Y é restrita (linearmente) pela variação de X. Assim como na regressão múltipla, a estatística da RDA é representada pelo valor de R2 e F. O valor de R2 indica a força da relação linear entre X e Y e o valor do F representa o teste global de significância. Além disso, é possível testar a significância de cada um dos eixos da ordenação (e a presença de pelo menos um eixo significativo é pré-requisito para que exista a relação linear entre X e Y) e de cada uma das variáveis preditoras da matriz X. Checklist Variáveis preditoras: importante verificar (1) a estrutura de correlação das variáveis ambientais, e a (2) presença de autocorrelação espacial. Composição de espécies como matriz Y: fundamental observar se os valores utilizados representam abundância ou presença-ausência e qual a necessidade de padronização (e.g., Hellinger). Assim como em modelos de regressão linear e múltipla, os valores de R2 ajustado devem ser selecionados ao invés do valor de R2. 9.7.1.1 Exemplo 1 Espécies de aves que ocorrem em localidades com diferentes altitudes. Pergunta: O clima e a altitude modificam a composição de espécies de aves? Predições Diferenças climáticas (temperatura e chuva) e altitudinais alteram a composição de espécies de aves. Variáveis (mesmo conjunto de dados usados na PERMANOVA) Preditoras: Temperatura e chuva (contínuas) e altitude (categórica com três níveis) Dependente: composição de espécies de aves ## Passo 1: transformação de hellinger da matriz de espécies # caso tenha dados de abundância. species.hel &lt;- decostand(species, &quot;hellinger&quot;) ## Passo 2: selecionar variáveis importantes. # Para isso, é necessário remover a variável categórica. env.contin &lt;- env[,-8] ## Evite usar variáveis muito correlacionadas sel.vars &lt;- forward.sel(species.hel, env.contin) #&gt; Testing variable 1 #&gt; Testing variable 2 #&gt; Testing variable 3 #&gt; Procedure stopped (alpha criteria): pvalue for variable 3 is 0.219000 (&gt; 0.050000) sel.vars$variables #&gt; [1] &quot;rain.jul&quot; &quot;maxi.jul&quot; env.sel &lt;- env[,sel.vars$variables] ## Passo 3: padronizar matriz ambiental (somente variáveis contínuas) env.pad &lt;- decostand(env.sel, &quot;standardize&quot;) ## Matriz final com variáveis preditoras env.pad.cat &lt;- data.frame(env.pad, altitude = env$altitude) Depois de selecionar um subconjunto dos dados com o método Forward Selection e padronizá-los (média 0 e desvio padrão 1), o modelo da RDA é construído como modelos lineares e PERMANOVA. ## RDA com dados selecionados e padronizados rda.bird &lt;- rda(species.hel~rain.jul+maxi.jul+altitude, data=env.pad.cat) # Para interpretar, é necessário saber a significância dos eixos para representar a relação entre as variáveis preditoras e a composição de espécies res.axis &lt;- anova.cca(rda.bird, by=&quot;axis&quot;) res.axis #&gt; Permutation test for rda under reduced model #&gt; Forward tests for axes #&gt; Permutation: free #&gt; Number of permutations: 999 #&gt; #&gt; Model: rda(formula = species.hel ~ rain.jul + maxi.jul + altitude, data = env.pad.cat) #&gt; Df Variance F Pr(&gt;F) #&gt; RDA1 1 0.045759 12.0225 0.001 *** #&gt; RDA2 1 0.009992 2.6252 0.062 . #&gt; RDA3 1 0.007518 1.9752 0.133 #&gt; RDA4 1 0.003582 0.9410 0.471 #&gt; Residual 18 0.068510 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Em seguida, é possível identificar quais são as variáveis que contribuem ou que mais contribuem para a variação na composição de espécies res.var &lt;- anova.cca(rda.bird, by=&quot;term&quot;) ## Qual variável? res.var #&gt; Permutation test for rda under reduced model #&gt; Terms added sequentially (first to last) #&gt; Permutation: free #&gt; Number of permutations: 999 #&gt; #&gt; Model: rda(formula = species.hel ~ rain.jul + maxi.jul + altitude, data = env.pad.cat) #&gt; Df Variance F Pr(&gt;F) #&gt; rain.jul 1 0.036514 9.5936 0.001 *** #&gt; maxi.jul 1 0.011264 2.9596 0.016 * #&gt; altitude 2 0.019071 2.5053 0.010 ** #&gt; Residual 18 0.068510 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Além disso, é possível obter o valor do R2 do modelo r_quadr &lt;- RsquareAdj(rda.bird) r_quadr #&gt; $r.squared #&gt; [1] 0.4938685 #&gt; #&gt; $adj.r.squared #&gt; [1] 0.3813949 # Ordenação multi-escala (MSO) para entender os resultados da ordenação em relação à distância geográfica bird.rda &lt;- mso(rda.bird, xy, grain = 1, permutations = 99) msoplot(bird.rda) #&gt; Error variance of regression model underestimated by -2 percent ## Triplot da RDA ggord(rda.bird, ptslab = TRUE, size = 1, addsize = 3, parse = TRUE) + theme_bw() + geom_hline(yintercept = 0, linetype=2) + geom_vline(xintercept = 0, linetype=2) + theme(axis.title.x = element_text(size=14), axis.text.x = element_text(vjust=0.5, size=12), axis.title.y = element_text(size=14), axis.text.y = element_text(vjust=0.5, size=12))+ tema_livro() 9.7.1.2 Interpretação dos resultados Os objetos res.axis, res.var e r_quadr mostram, respectivamente, (i) as dimensões (RDA1, RDA2, etc.) que possuem variação na composição de espécies, (ii) as variáveis preditoras que explicam esta variação, e (iii) o valor do R2 ajustado. Neste exemplo, podemos observar que somente a dimensão 1 (RDA1) representa uma variação significativa da composição de espécies (P = 0,001). As variáveis rain.jul, maxi.jul e altitude foram todas preditoras importantes da composição de espécies, mas rain.jul se destacada com maior valor de F. Além disso, o valor do R2 ajustado de 0.381 indica forte contribuição dessas variáveis preditoras. Porém, uma das limitações desta análise é não considerar que tanto espécies quanto variáveis preditoras podem estar estruturadas espacialmente. Como resultado, os resíduos da análises podem apresentar autocorrelação espacial que, por sua vez, aumenta o erro do tipo I (P. Legendre and Legendre 2012a). A figura obtida com o comando msoplot(bird.rda) demonstra que existe autocorrelação espacial em algumas distâncias da análise. Veja abaixo algumas alternativas para resíduos com autocorrelação espacial. 9.7.2 RDAp: Análise de Redundância parcial Um dos problemas da abordagem anterior é que tanto a composição de espécies como as variáveis ambientais estão estruturadas espacialmente. Talvez mais importantes, para que os valores de probabilidade da RDA sejam interpretados corretamente (e para evitar erro do tipo I), os resíduos do modelo não devem estar correlacionados espacialmente, como demonstrado com a análise MSO. Uma alternativa é de incluir a matriz de dados espaciais (matrix W) como valor condicional dentro da RDA. Esta análise é conhecida como RDA parcial. Porém, a obtenção dos dados espaciais da matriz W é mais complexo do que simplesmente incluir dados de localização geográfica (latitude e longitude), como feito em alguns modelos lineares (gls, (cap7?)). Existem diversas ferramentas que descrevem e incorporam o componente espacial em métodos mulitidimensionais, mas os Mapas de autovetores de Moran (MEM) são certamente os mais utilizados (Dray et al. 2012). A análise MEM consiste na ordenação (PCoA) de uma matriz truncada obtida através da localização geográfica das localidades utilizando distância euclidiana, matriz de conectividade e matriz espacial ponderada. Os autovalores obtidos no MEM são idênticos aos coeficientes de correlação espacial de Moran I. Um procedimento chave desta análise é a definição de um limiar de trucamento (do inglês truncate threshold). Este limiar é calculado a partir de uma árvore de espaço mínimo (MST, do inglês minimum spanning tree) que conecta todos os pontos de coleta. Na prática, os valores menores do que o limiar definido pela MST indicam que os pontos com aqueles valores estão conectados e, assim possuem correlação positiva. Outro ponto importante desta análise é a obtenção da matriz espacial ponderada (SWM, do inglês spatial weighthing matrix). A seleção da matriz SWM é parte essencial do cálculo dos MEM e não deve ser feita arbitratiamente (Bauman, Drouet, Fortin, et al. 2018). Por este motivo a análise recebe este nome (P. Legendre and Legendre 2012a). Finalmente, o método produz autovetores que representam preditores espaciais que podem ser utilizados na RDA parcial (e outras análises). É importante ressaltar que o critério de seleção do número de autovetores é bastante debatido na literatura e, para isso, sugerimos a leitura dos seguintes artigos Bauman, Drouet, Dray, et al. (2018). Então, o primeiro passo para realizar uma RDA parcial é de gerar os autovetores espaciais (MEMs). ## Dados # matriz padronizada de composição de espécies. head(species.hel)[,1:6] #&gt; Fauvette_orphee Fauvette_des_jardins Fauvette_a_tete_noire Fauvette_babillarde #&gt; S01 0 0.3651484 0.3651484 0.2581989 #&gt; S02 0 0.3333333 0.3333333 0.2357023 #&gt; S03 0 0.3162278 0.3162278 0.3162278 #&gt; S04 0 0.4200840 0.3429972 0.2425356 #&gt; S05 0 0.3872983 0.3162278 0.2236068 #&gt; S06 0 0.3779645 0.3779645 0.2672612 #&gt; Fauvette_grisette Fauvette_pitchou #&gt; S01 0.2581989 0 #&gt; S02 0.2357023 0 #&gt; S03 0.2236068 0 #&gt; S04 0.0000000 0 #&gt; S05 0.3162278 0 #&gt; S06 0.0000000 0 # latitude e longitude. head(xy) #&gt; x y #&gt; S01 156 252 #&gt; S02 141 217 #&gt; S03 171 233 #&gt; S04 178 215 #&gt; S05 123 189 #&gt; S06 154 195 # dados ambientais padronizados e altitude head(env.pad.cat) #&gt; rain.jul maxi.jul altitude #&gt; S01 1.333646 0.1462557 Montanhoso #&gt; S02 1.468827 -0.6848206 Intermediário #&gt; S03 1.505694 -0.2099199 Montanhoso #&gt; S04 1.296778 -2.0699476 Montanhoso #&gt; S05 1.075572 -0.3682201 Plano #&gt; S06 1.100151 -0.6056705 Intermediário # Passo 1: Gerar um arquivo LIST W: list binária de vizinhança mat_knn &lt;- knearneigh(as.matrix(xy), k=2, longlat = FALSE) mat_nb &lt;- knn2nb(mat_knn, sym=TRUE) mat_listw &lt;- nb2listw(mat_nb, style = &quot;W&quot;) mat_listw #&gt; Characteristics of weights list object: #&gt; Neighbour list object: #&gt; Number of regions: 23 #&gt; Number of nonzero links: 58 #&gt; Percentage nonzero weights: 10.96408 #&gt; Average number of links: 2.521739 #&gt; #&gt; Weights style: W #&gt; Weights constants summary: #&gt; n nn S0 S1 S2 #&gt; W 23 529 23 18.84444 96.01111 # Passo 2: Listar os métodos &quot;candidatos&quot; para obter a matriz SWM MEM_mat &lt;- scores.listw(mat_listw, MEM.autocor = &quot;positive&quot;) candidates &lt;- listw.candidates(xy, nb = c(&quot;gab&quot;, &quot;mst&quot;, &quot;dnear&quot;), weights = c(&quot;binary&quot;, &quot;flin&quot;)) # Passo 3: Selecionar a melhor matriz SWM e executar o MEM W_sel_mat &lt;- listw.select(species.hel, candidates, MEM.autocor = &quot;positive&quot;, p.adjust = TRUE, method = &quot;FWD&quot;) #&gt; Procedure stopped (alpha criteria): pvalue for variable 5 is 0.088000 (&gt; 0.050000) #&gt; Procedure stopped (alpha criteria): pvalue for variable 3 is 0.064000 (&gt; 0.050000) #&gt; Procedure stopped (alpha criteria): pvalue for variable 3 is 0.061000 (&gt; 0.050000) #&gt; Procedure stopped (alpha criteria): pvalue for variable 4 is 0.159000 (&gt; 0.050000) # Passo 4: Matriz dos preditores espaciais escolhidos (MEMs) spatial.pred &lt;- as.data.frame(W_sel_mat$best$MEM.select) # necessário atribuir os nomes das linhas rownames(spatial.pred) &lt;- rownames(xy) Depois de gerar os valores dos autovetores espaciais (MEM), é possível executar a a RDA parcial utilizando esses valores no argumento Conditional. ## Combinar variáveis ambientais e espaciais em um único data.frame pred.vars &lt;- data.frame(env.pad.cat, spatial.pred) ## RDA parcial rda.p &lt;- rda(species.hel ~ rain.jul + maxi.jul + altitude + # Preditores ambientais Condition(MEM1+MEM2+MEM4+MEM5), # Preditores espaciais data = pred.vars) # Para interpretar, é necessário saber a significância dos eixos para representar a relação entre as variáveis preditoras e a composição de espécies res.p.axis &lt;- anova.cca(rda.p, by=&quot;axis&quot;) res.p.axis #&gt; Permutation test for rda under reduced model #&gt; Forward tests for axes #&gt; Permutation: free #&gt; Number of permutations: 999 #&gt; #&gt; Model: rda(formula = species.hel ~ rain.jul + maxi.jul + altitude + Condition(MEM1 + MEM2 + MEM4 + MEM5), data = pred.vars) #&gt; Df Variance F Pr(&gt;F) #&gt; RDA1 1 0.008471 2.1376 0.312 #&gt; RDA2 1 0.004830 1.2189 0.782 #&gt; RDA3 1 0.003240 0.8176 0.892 #&gt; RDA4 1 0.001891 0.4773 0.902 #&gt; Residual 14 0.055477 # Em seguida, é possível identificar quais são as variáveis que contribuem ou que mais contribuem para a variação na composição de espécies res.p.var &lt;- anova.cca(rda.p, by=&quot;term&quot;) ## Qual variável? res.p.var #&gt; Permutation test for rda under reduced model #&gt; Terms added sequentially (first to last) #&gt; Permutation: free #&gt; Number of permutations: 999 #&gt; #&gt; Model: rda(formula = species.hel ~ rain.jul + maxi.jul + altitude + Condition(MEM1 + MEM2 + MEM4 + MEM5), data = pred.vars) #&gt; Df Variance F Pr(&gt;F) #&gt; rain.jul 1 0.004406 1.1119 0.340 #&gt; maxi.jul 1 0.004446 1.1220 0.337 #&gt; altitude 2 0.009579 1.2087 0.238 #&gt; Residual 14 0.055477 RsquareAdj(rda.p) #&gt; $r.squared #&gt; [1] 0.1361661 #&gt; #&gt; $adj.r.squared #&gt; [1] 0.02330319 Se você comparar os resultados do objeto res.p.var (RDA parcial) com res.var (RDA simples) é possível perceber como a estrutura espacial nos resíduos aumenta a probabilidade de cometer erro do tipo 1. O modelo da RDA parcial mostra que não existem qualquer efeito direto das variáveis ambientais sobre a composição de espécies (conclusão com a RDA simples). Na verdade, tanto a composição de espécies quanto as variáveis climáticas estão fortemente estruturadas no espaço, como demonstramos a seguir: ## Padrão espacial na composição de espécies pca.comp &lt;- dudi.pca(species.hel, scale = FALSE, scannf = FALSE) moran.comp &lt;- moran.mc(pca.comp$li[, 1], mat_listw, 999) ## Padrão espacial das variáveis ambientais env$altitude &lt;- as.factor(env$altitude) ca.env &lt;- dudi.hillsmith(env, scannf = FALSE) moran.env &lt;- moran.mc(ca.env$li[, 1], mat_listw, 999) ## Estrutura espacial na composição de espécies? moran.comp #&gt; #&gt; Monte-Carlo simulation of Moran I #&gt; #&gt; data: pca.comp$li[, 1] #&gt; weights: mat_listw #&gt; number of simulations + 1: 1000 #&gt; #&gt; statistic = 0.62815, observed rank = 1000, p-value = 0.001 #&gt; alternative hypothesis: greater ## Estrutura espacial na variação ambiental? moran.env #&gt; #&gt; Monte-Carlo simulation of Moran I #&gt; #&gt; data: ca.env$li[, 1] #&gt; weights: mat_listw #&gt; number of simulations + 1: 1000 #&gt; #&gt; statistic = 0.72714, observed rank = 1000, p-value = 0.001 #&gt; alternative hypothesis: greater Como resultado, é possível que a variação ambiental espacialmente estruturada é o principal efeito sobre a composição de espécies. Uma maneira de visualizar a contribuição relativa de diferentes matrizes (ambiental e espacial, por exemplo) é utilizar o método de partição de variância. O resultado deste modelo indica que, de fato, não existe efeito direto das variáveis ambientais e sim do componente representado pela autocorrelação espacial dessas variáveis. ### Partição de variância pv.birds &lt;- varpart(species.hel, env.pad.cat, spatial.pred) plot(pv.birds) 9.8 PERMANOVA A PERMANOVA é um acrônimo, em inglês, de permutational multivariate analysis of variance, análise proposta por Anderson (Marti J. Anderson 2001). A PERMANOVA é usada para testar hipóteses multivariadas que comparam a abundância de diferentes espécies em resposta a diferentes tratamentos ou gradientes ambientais. Esta análise foi desenvolvida como forma de solucionar algumas limitações da tradicional ANOVA multivariada (MANOVA). Em especial, o pressuposto da MANOVA de distribuição normal multivariada é raramente encontrado em dados ecológicos. O primeiro passo da PERMANOVA é selecionar uma medida de distância apropriada aos dados e, além disso, verificar a necessidade de padronização ou transformação dos dados. Em seguida, as distâncias são comparadas entre os grupos de interesse (por exemplo, tratamento vs. controle) usando a estatística F de maneira muito parecida com uma ANOVA (cap7?), chamada de pseudo-F: \\[ F = (SSa / SSr)*[(N-g) / (g-1)] \\] onde SSa representa a soma dos quadrados entre grupos, SSr a soma de quadrados dentro do grupo (residual), N o número de unidades amostrais e g os grupos (ou níveis da variável categórica). Esta fórmula do pseudo-F é específica para desenho experimental com um fator. Outros desenhos mais complexos são apresetandos em Anderson [(2001); (2017). O cálculo do valor de probilidade é realizado por métodos de permutação que são discutidos em Anderson &amp; Ter Braak (2003). 9.8.0.1 Exemplo 1 Espécies de aves que ocorrem em localidades com diferentes altitudes. Pergunta O clima e a altitude modificam a composição de espécies de aves? Predições Diferenças climáticas (temperatura e chuva) e altitudinais alteram a composição de espécies de aves. Variáveis Preditoras: Temperatura e chuva (contínuas) e altitude (categórica com três níveis) Dependente: composição de espécies de aves # Composição de espécies padronizar com método de Hellinger species.hel &lt;- decostand(species, &quot;hellinger&quot;) # Matriz de distância com método Bray Curtis sps.dis &lt;- vegdist(species.hel, &quot;bray&quot;) Para reduzir o número de variáveis no modelo, você pode considerar duas abordangens. A primeira, e mais importante delas, é manter somente variáveis preditoras que você tenha razão biológica para mantê-la e, além disso, que esteja relacionada com suas hipóteses. Assim, uma vez que você já removeu variáveis que não tem relevância biológica, você deve usar diferentes métodos para remover as variáveis muito correlacionadas (forward selection, Variance Inflation Factor (VIF), entre outros). Neste exemplo, vamos simplesmente fazer uma correlação múltipla e remover as variáveis com correlação maior do que 0.9 ou -0.9. A função ggpairs mostra um gráfico bem didático para representa a relação entre todas as variáveis e o valor (r) desta correlação. ## Verifica correlação entre as variáveis ggpairs(env) # Após verificar a estrutura de correlação, vamos manter somente três #variáveis env2 &lt;- env[,c(&quot;mini.jan&quot;, &quot;rain.tot&quot;, &quot;altitude&quot;)] Após selecionar as variáveis do modelo, vamos executar a PERMANOVA e entender as principais etapas para interpretar corretamente o teste. A função adonis do pacote vegan é a melhor opção no vegan. Porém, é importante referir o programa PRIMER e PERMANOVA+ como ótima opção para implementar a PERMANOVA e ter maior controle em desenhos complexos (M. J. Anderson, Gorley, and Clarke 2008). Assim como nos modelos lineares apresentados no (cap7?), os argumento seguem o mesmo formato, com variável dependente separada por um ~ das variáveis preditoras. Porém, alguns autores demonstraram que a PERMANOVA (assim como Mantel e ANOSIM) não pode identificar se diferenças significativas do teste (usando a estatística pseudo-F) se devem a diferenças no posição, na dispersão ou ambos. Ou seja, ao comparar grupos não é possível identificar se existe mudanças de composição (posição) ou a variação da composição de espécies dentro de um grupo (dispersão) é maior do que a variação dentro do outro grupo (Marti J. Anderson and Walsh 2013). Para solucionar este problema, é possível combinar a PERMANOVA com a análise PERMDISP (ou BETADISPER, como chamado no pacote vegan). Esta análise permite comparar se existe heterogeneidade nas variâncias entre grupos. Deste modo, a presença de heterogeneidade de variâncias (valor do BETADISPER significativo), é possível saber que as diferenças entre os grupos ocorre principalmente por diferenças na dispersão e não, necessariamente, de posição. Mais detalhes sobre a relevância de combinar essas duas análises estão disponíveis em Anderson &amp; Walsh (2013). perm.aves &lt;- adonis2(sps.dis ~ mini.jan + rain.tot + altitude, data = env2) perm.aves ### Diferenças entre os tratamentos? #&gt; Permutation test for adonis under reduced model #&gt; Terms added sequentially (first to last) #&gt; Permutation: free #&gt; Number of permutations: 999 #&gt; #&gt; adonis2(formula = sps.dis ~ mini.jan + rain.tot + altitude, data = env2) #&gt; Df SumOfSqs R2 F Pr(&gt;F) #&gt; mini.jan 1 0.09069 0.15997 6.3307 0.001 *** #&gt; rain.tot 1 0.12910 0.22771 9.0118 0.001 *** #&gt; altitude 2 0.08929 0.15749 3.1163 0.021 * #&gt; Residual 18 0.25787 0.45483 #&gt; Total 22 0.56695 1.00000 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 betad.aves &lt;-betadisper(sps.dis, env2$altitude) permutest(betad.aves) #&gt; #&gt; Permutation test for homogeneity of multivariate dispersions #&gt; Permutation: free #&gt; Number of permutations: 999 #&gt; #&gt; Response: Distances #&gt; Df Sum Sq Mean Sq F N.Perm Pr(&gt;F) #&gt; Groups 2 0.0042643 0.0021322 1.4672 999 0.247 #&gt; Residuals 20 0.0290636 0.0014532 Em nosso exemplo, temperatura, chuva e altitude afetaram a variação na composição de espécies. Porém, para identificar se as diferenças de composição entre os níveis da variável altitude, é necessário interpretar os resultados da análise BETADISPER. O comando permutest(betad.aves) mostra que o valor de probabilidade da análise foi de 0.253, ou seja, a hipótese nula de que a variância entre grupos é homogênea é aceita. Assim, não existe diferenças na dispersão entre grupo, sugerindo que a diferença encontrada na PERMANOVA (objeto perm.aves) se deve, em parte, a mudança na composição de espécies de aves entre diferentes altitudes (R2 = 0.135). Além disso, a chuva (R2 = 0.183) e temperatura (R2 = 0.127) foram fatores importantes na variação da composição de espécies. Como falado anteriormente, as análises de ordenação irrestritas (PCA, PCoA, nMDS) são utilizadas para explorar dados. Uma maneira poderosa de usá-las é combinando com análises que testam hipóteses, como PERMANOVA e RDA (abaixo). A literatura ecológica tem usado a análise de escalonamento não métrico (nMDS) combinado com análises multidimensionais de variância (como a PERMANOVA) para visualização da similaridade na composição de espécies dentro e entre grupos. A seguir, implementamos o nMDS na matriz de composição de espécies de ácaros. # Matriz de distância representando a variação na composição de espécies (método Bray-Curtis) as.matrix(sps.dis)[1:6, 1:6] #&gt; S01 S02 S03 S04 S05 S06 #&gt; S01 0.0000000 0.15133734 0.16720405 0.2559122 0.2559882 0.2588892 #&gt; S02 0.1513373 0.00000000 0.04114702 0.1190172 0.1289682 0.1391056 #&gt; S03 0.1672040 0.04114702 0.00000000 0.1420233 0.1358127 0.1410867 #&gt; S04 0.2559122 0.11901720 0.14202325 0.0000000 0.1140283 0.1199803 #&gt; S05 0.2559882 0.12896823 0.13581271 0.1140283 0.0000000 0.2129054 #&gt; S06 0.2588892 0.13910558 0.14108668 0.1199803 0.2129054 0.0000000 # É preciso calcular uma primeira &quot;melhor&quot; solução do nMDS sol1 &lt;- metaMDS(sps.dis) #&gt; Run 0 stress 0.1344042 #&gt; Run 1 stress 0.1272417 #&gt; ... New best solution #&gt; ... Procrustes: rmse 0.0789082 max resid 0.3526074 #&gt; Run 2 stress 0.1344042 #&gt; Run 3 stress 0.1336481 #&gt; Run 4 stress 0.1344042 #&gt; Run 5 stress 0.1336481 #&gt; Run 6 stress 0.1336481 #&gt; Run 7 stress 0.1338432 #&gt; Run 8 stress 0.1338432 #&gt; Run 9 stress 0.1378506 #&gt; Run 10 stress 0.1336481 #&gt; Run 11 stress 0.1338433 #&gt; Run 12 stress 0.1338432 #&gt; Run 13 stress 0.1338432 #&gt; Run 14 stress 0.1336481 #&gt; Run 15 stress 0.1344042 #&gt; Run 16 stress 0.1600808 #&gt; Run 17 stress 0.1272418 #&gt; ... Procrustes: rmse 8.48708e-05 max resid 0.0003246819 #&gt; ... Similar to previous best #&gt; Run 18 stress 0.1344042 #&gt; Run 19 stress 0.1611279 #&gt; Run 20 stress 0.1336481 #&gt; *** Solution reached # Depois, executar a mesma função, mas utilizando uma &quot;melhor solução inicial&quot; para evitar resultdos subótimos no nMDS nmds.beta &lt;- metaMDS(sps.dis, previous.best = sol1) #&gt; Starting from 2-dimensional configuration #&gt; Run 0 stress 0.1272417 #&gt; Run 1 stress 0.1272418 #&gt; ... Procrustes: rmse 2.997893e-05 max resid 0.000115054 #&gt; ... Similar to previous best #&gt; Run 2 stress 0.1378506 #&gt; Run 3 stress 0.1336481 #&gt; Run 4 stress 0.1344042 #&gt; Run 5 stress 0.1344042 #&gt; Run 6 stress 0.1272417 #&gt; ... New best solution #&gt; ... Procrustes: rmse 3.864488e-06 max resid 1.312654e-05 #&gt; ... Similar to previous best #&gt; Run 7 stress 0.1600803 #&gt; Run 8 stress 0.1336481 #&gt; Run 9 stress 0.1272417 #&gt; ... Procrustes: rmse 2.636369e-06 max resid 7.540622e-06 #&gt; ... Similar to previous best #&gt; Run 10 stress 0.1272417 #&gt; ... Procrustes: rmse 2.44969e-05 max resid 9.417254e-05 #&gt; ... Similar to previous best #&gt; Run 11 stress 0.1336481 #&gt; Run 12 stress 0.1272417 #&gt; ... New best solution #&gt; ... Procrustes: rmse 4.828567e-06 max resid 1.820829e-05 #&gt; ... Similar to previous best #&gt; Run 13 stress 0.1344042 #&gt; Run 14 stress 0.1272417 #&gt; ... Procrustes: rmse 7.317336e-06 max resid 2.766948e-05 #&gt; ... Similar to previous best #&gt; Run 15 stress 0.3652494 #&gt; Run 16 stress 0.1659926 #&gt; Run 17 stress 0.1338432 #&gt; Run 18 stress 0.1336481 #&gt; Run 19 stress 0.1344042 #&gt; Run 20 stress 0.1272418 #&gt; ... Procrustes: rmse 2.223437e-05 max resid 8.504391e-05 #&gt; ... Similar to previous best #&gt; *** Solution reached # O stress é o valor mais importante para interpretar a qualidade da ordenação () nmds.beta$stress # valor ideal entre 0 e 0.2 #&gt; [1] 0.1272417 # Exportar os valores para fazer gráfico dat.graf &lt;- data.frame(scores(nmds.beta), altitude = env2$altitude) # Definir os grupos (&quot;HULL&quot;) para serem categorizados no gráfico grp.mon &lt;- dat.graf[dat.graf$altitude == &quot;Montanhoso&quot;, ][chull(dat.graf[dat.graf$altitude == &quot;Montanhoso&quot;, c(&quot;NMDS1&quot;, &quot;NMDS2&quot;)]), ] grp.int &lt;- dat.graf[dat.graf$altitude == &quot;Intermediário&quot;, ][chull(dat.graf[dat.graf$altitude == &quot;Intermediário&quot;, c(&quot;NMDS1&quot;, &quot;NMDS2&quot;)]), ] grp.pla &lt;- dat.graf[dat.graf$altitude == &quot;Plano&quot;, ][chull(dat.graf[dat.graf$altitude == &quot;Plano&quot;, c(&quot;NMDS1&quot;, &quot;NMDS2&quot;)]), ] ## Combinar dados dos grupos para cada Convex Hull hull.data &lt;- rbind(grp.mon, grp.int, grp.pla) head(hull.data) #&gt; NMDS1 NMDS2 altitude #&gt; S04 -0.10578360 -0.10682795 Montanhoso #&gt; S01 -0.25332377 0.04198598 Montanhoso #&gt; S11 -0.12504868 0.14477145 Montanhoso #&gt; S15 0.09166003 0.09857211 Montanhoso #&gt; S18 0.01968282 -0.12417413 Intermediário #&gt; S06 -0.16053934 -0.08924307 Intermediário # Gráfico combinado os escores do nMDS com polígonos dos valores por cada cota altitudinal dat.graf %&gt;% ggplot(aes(x = NMDS1, y = NMDS2, color = altitude, shape = altitude)) + theme_bw() + geom_point(size=4, alpha=0.7) + geom_polygon(data = hull.data, aes(fill = altitude, group = altitude), alpha=0.3) + scale_color_manual(values=c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) + scale_fill_manual(values=c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) + xlab(&quot;NMDS1&quot;) + ylab(&quot;NMDS2&quot;) + theme(axis.title.x = element_text(size=14), axis.text.x = element_text(vjust=0.5, size=12), axis.title.y = element_text(size=14), axis.text.y = element_text(vjust=0.5, size=12), legend.position = &quot;top&quot;, legend.title = element_blank())+ tema_livro() 9.8.1 Para se aprofundar Agrupamento de espécies e locais baseado em modelos Numerical Ecology with R James &amp; McCulloch (1990) Legendre &amp; Legendre (2012) Referências "],["cap10.html", "Capítulo 10 Rarefação 10.1 Aspectos teóricos 10.2 Curva de rarefação baseada no indivíduo (individual-based) 10.3 Curva de rarefação baseada em amostras (sample-based) 10.4 Curva de rarefação coverage-based", " Capítulo 10 Rarefação Pré-requisitos do capítulo # Pacotes library(iNEXT) library(devtools) library(ecodados) library(ggplot2) library(vegan) library(nlme) library(dplyr) library(piecewiseSEM) ## Dados necessários data(&quot;mite&quot;) data(&quot;mite.xy&quot;) coord &lt;- mite.xy colnames(coord) &lt;- c(&quot;long&quot;, &quot;lat&quot;) data(&quot;mite.env&quot;) agua &lt;- mite.env[,2] dados_rarefacao &lt;- ecodados::rarefacao_morcegos rarefacao_repteis &lt;- ecodados::rarefacao_repteis rarefacao_anuros &lt;- ecodados::rarefacao_anuros dados_amostras &lt;- ecodados::morcegos_rarefacao_amostras 10.1 Aspectos teóricos Uma das grandes dificuldades na comparação da riqueza de espécies entre comunidades é decorrente da diferença no esforço amostral (e.g. diferença no número de indivíduos, discrepância na quantidade de unidades amostrais ou área amostrada) que inevitavelmente influenciará no número de espécies observadas (N. J. Gotelli and Chao 2013; Roswell, Dushoff, and Winfree 2021). O método de rarefação nos permite comparar o número de espécies entre comunidades quando o tamanho da amostra (e.g. número de unidades amostrais), o esforço amostral (e.g. tempo de amostragem) ou a abundância de indivíduos não são iguais. A rarefação calcula o número esperado de espécies em cada comunidade tendo como base comparativa um valor em que todas as amostras atinjam um tamanho padrão. Gotelli &amp; Colwell (2001) descrevem dois tipos de curvas de rarefação: i) baseada em indivíduos (individual-based) - as comparações são feitas considerando a abundância da comunidade padronizada pelo menor número de indivíduos; e ii) baseada na amostra (sampled-based) - as comparações são padronizadas pela comunidade com menor número de amostragens. O método foi formulado considerando a seguinte pergunta: Se considerarmos n indivíduos ou amostras (n &lt; N) para cada comunidade, quantas espécies registraríamos nas comunidades considerando o mesmo número de indivíduos ou amostras? Gotelli &amp; Colwell (2001) descrevem este método e discutem em detalhes as restrições sobre seu uso na ecologia: As amostras a serem comparadas devem ser consistentes do ponto de vista taxonômico, ou seja, todos os indivíduos devem pertencer ao mesmo grupo taxonômico; As comparações devem ser realizadas somente entre amostras com as mesmas técnicas de coleta. Por exemplo, não é recomendado comparar amostras onde a riqueza de espécies de anuros de uma amostra foi estimada utilizando armadilhas de interceptação e queda e a outra foi estimada por vocalizações em sítios de reprodução; Os tipos de hábitat onde as amostras são obtidas devem ser semelhantes; É um método para estimar a riqueza de espécies em uma amostra menor  não pode ser usado para extrapolar a riqueza para amostras maiores.  Importante: Esta última restrição foi superada por Colwell et al. (2012) e Chao &amp; Jost (2012) que desenvolveram uma nova abordagem onde os dados podem ser interpolados (rarefeito) para amostras menores e extrapolados para amostras maiores. Além disso, Chao &amp; Jost (2012) propõem a curva de rarefação coverage-based que padroniza as amostras pela cobertura ou totalidade (completeness) da amostra ao invés do tamanho. As rarefações tradicionais apresentam limitações matemáticas que são superadas por essa nova abordagem (Anne Chao and Jost 2012). 10.2 Curva de rarefação baseada no indivíduo (individual-based) 10.2.0.1 Exemplo prático 1 - Morcegos Explicação dos dados Usaremos os dados de espécies de morcegos amostradas em três fragmentos florestais (Breviglieri 2008): i) Mata Ciliar do Córrego Talhadinho com 12 hectares; ii) Mata Ciliar do Córrego dos Tenentes com 10 hectares; e iii) Fazenda Experimental de Pindorama com 128 hectares. Pergunta: A riqueza de espécies de morcegos é maior na Fazenda Experimental do que nos fragmentos florestais menores? Predições O número de espécies será maior em fragmentos florestais maiores. Variáveis Variáveis resposta e preditoras Matriz ou dataframe com as abundâncias das espécies de morcegos (variável resposta) registradas nos três fragmentos florestais (variável preditora). Checklist Verificar se a sua matriz ou dataframe estão com as espécies nas linhas e os fragmentos florestais nas colunas Análise Vamos olhar os dados usando a função head head(dados_rarefacao) #&gt; MC_Tenentes MC_Talhadinho FF_Experimental #&gt; Chrotopterus_auritus 0 1 1 #&gt; Phyllostomus_hastatus 0 1 0 #&gt; Phyllostomus_discolor 0 2 2 #&gt; Artibeus_lituratus 17 26 26 #&gt; Artibeus_obscurus 1 4 6 #&gt; Artibeus_planirostris 34 72 52 Usaremos as funções do pacote iNEXT (iNterpolation e EXTrapolation) para o cálculo da rarefação. Esta função permite estimar a riqueza de espécies utilizando a família Hill-numbers (Hill 1973; explicação dos conceitos da família Hill-numbers está detalhada no @[cap12]). O argumento q refere-se a família Hill-numbers onde: 0 = riqueza de espécies; 1 = diversidade de Shannon; e 2 = diversidade de Simpson. # Datatype refere-se ao tipo de dados que você vai analisar (e.g. abundância, # incidência). # Endpoint refere-se ao valor máximo que você determina para a extrapolação. resultados_morcegos &lt;- iNEXT(dados_rarefacao, q = 0, datatype = &quot;abundance&quot;, endpoint = 800) Vamos visualizar os resultados. # type define o tipo de curva de rarefação: # 1 = curva de rarefação baseada no indivíduo ou amostra; # 2 = curva de representatividade da amostra; e # 3 = curva de rarefação baseada na representatividade (coverage-based). ggiNEXT(resultados_morcegos, type = 1) + labs(x = &quot;Número de indivíduos&quot;, y = &quot; Riqueza de espécies&quot;) + scale_linetype_discrete(labels = c(&quot;Interpolado&quot;, &quot;Extrapolado&quot;)) + scale_colour_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) Interpretação dos resultados Foram registrados 166 indivíduos na MC_Tenentes, 413 na MC_Talhadinho e 223 na FF_Experimental. Lembrando, você não pode comparar a riqueza de espécies observada diretamente: 15 espécies na MC_Tenentes, 19 espécies na MC_Talhadinho, e 17 espécies no FF_Experimental. A comparação da riqueza de espécies entre as comunidades deve ser feita com base na riqueza de espécies rarefeita, que é calculada com base no número de indivíduos da comunidade com menor abundância (166 indivíduos). Olhando o gráfico é possível perceber que a riqueza de espécies de morcegos rarefeita não é diferente entre os três fragmentos florestais quando corrigimos o problema da diferença na abundância pela rarefação. A interpretação é feita com base no intervalo de confiança de 95%. As curvas serão diferentes quando os intervalos de confiança não se sobreporem (A. Chao et al. 2014). Percebam que esta abordagem, além da interpolação (rarefação), também realiza extrapolações que podem ser usadas para estimar o número de espécies caso o esforço de coleta fosse maior. Este é o assunto do nosso próximo capítulo. 10.2.0.2 Exemplo prático 2 - Anuros e Répteis Explicação dos dados Neste exemplo, iremos comparar o número de espécies de anuros e répteis (serpentes e lagartos) usando informações dos indivíduos depositados em coleções científicas e coletas de campo (da Silva et al. 2017). Pergunta: A riqueza de espécies estimada para uma mesma região é maior usando informações de coleções científicas do que informações de coletas de campo? Predições O número de espécies será maior em coleções científicas devido ao maior esforço amostral (i.e. maior variação temporal para depositar os indivíduos e maior número de pessoas contribuindo com coletas esporádicas). Variáveis Variáveis resposta e preditoras Matriz ou dataframe com as abundâncias das espécies de anuros e répteis (variável resposta) registradas em coleções científicas e coletas de campo (variável preditora). Checklist Verificar se a sua matriz ou dataframe estão com as espécies nas linhas e a fonte dos dados nas colunas. Análise Olhando os dados dos répteis. head(rarefacao_repteis) #&gt; Coleta.Campo Colecoes.Cientificas #&gt; Ameiva_ameiva 1 0 #&gt; Amphisbaena_mertensii 1 0 #&gt; Apostolepis_dimidiata 0 1 #&gt; Bothrops__itapetiningae 0 2 #&gt; Bothrops__pauloensis 0 1 #&gt; Bothrops_alternatus 0 1 Análise usando o pacote iNEXT. # Análise resultados_repteis &lt;- iNEXT(rarefacao_repteis, q = 0, datatype = &quot;abundance&quot;, endpoint = 200) # Visualizar os resultados. ggiNEXT(resultados_repteis, type = 1) + labs(x = &quot;Número de indivíduos&quot;, y = &quot; Riqueza de espécies&quot;) + scale_linetype_discrete(labels = c(&quot;Interpolado&quot;, &quot;Extrapolado&quot;)) + scale_colour_manual(values = c(&quot;darkorange&quot;, &quot;cyan4&quot;)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;cyan4&quot;)) Interpretação dos resultados - répteis Foram registradas oito espécies de répteis nas coletas de campo (40 indivíduos) e 28 espécies nas coleções científicas (77 indivíduos). Com base na rarefação, concluímos que a riqueza de espécies de répteis obtida nas coleções científicas (20,5) é 2,9 vezes maior do que a obtida em coletas de campo (7,05). Olhando os dados dos anuros head(rarefacao_anuros) #&gt; Coleta.Campo Colecoes.Cientificas #&gt; Chiasmocleis_albopunctata 15 0 #&gt; Dendropsophus_elianae 11 1 #&gt; Dendropsophus_jimi 15 2 #&gt; Dendropsophus_nanus 0 1 #&gt; Dendropsophus_minutus 24 0 #&gt; Dendropsophus_sanborni 0 1 Análise e visualização do gráfico. # Análise resultados_anuros &lt;- iNEXT(rarefacao_anuros, q = 0, datatype = &quot;abundance&quot;, endpoint = 800) # Visualizar os resultados. ggiNEXT(resultados_anuros, type = 1) + labs(x = &quot;Número de indivíduos&quot;, y = &quot; Riqueza de espécies&quot;) + scale_linetype_discrete(labels = c(&quot;Interpolado&quot;, &quot;Extrapolado&quot;)) + scale_colour_manual(values = c(&quot;darkorange&quot;, &quot;cyan4&quot;)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;cyan4&quot;)) Interpretação dos resultados - anuros Foram registradas 21 espécies de anuros nas coletas de campo (709 indivíduos) e 12 espécies nas coleções científicas (37 indivíduos). Com base na rarefação, concluímos que não há diferença entre a riqueza de espécies de anuros obtida em coletas de campo e coleções científicas. 10.3 Curva de rarefação baseada em amostras (sample-based) 10.3.0.1 Exemplo prático 3 - Morcegos Explicação dos dados Usaremos os mesmos dados de espécies de morcegos amostradas em três fragmentos florestais (Breviglieri 2008). Contudo, ao invés de padronizarmos a riqueza de espécies pela abundância, iremos padronizar pelo número de amostras. Variáveis Variáveis resposta e preditoras Lista de vetores. Cada vetor deve conter como primeira informação, o número total de amostras (variável preditora), seguido da frequência de ocorrência das espécies (i.e. número de amostras em que cada espécie foi registrada - variável resposta). Checklist Verificar se a sua lista está com o número total de amostras e a frequência de ocorrência das espécies. Análise Vamos olhar os dados. head(dados_amostras) #&gt; MC_Tenentes MC_Talhadinho FF_Experimental #&gt; amostras 12 20 12 #&gt; sp1 12 20 12 #&gt; sp2 12 19 10 #&gt; sp3 10 15 8 #&gt; sp4 8 10 8 #&gt; sp5 6 7 7 Vamos criar uma lista com as amostragens de cada comunidade e os comandos da análise. # Usamos [,] para excluir os NAs. Lembrando que valores antes da # vírgula representam as linhas e os posteriores representam as colunas. lista_rarefacao &lt;- list(Tenentes = dados_amostras[1:18,1], Talhadinho = dados_amostras[,2], Experimental = dados_amostras[1:16,3]) # Análise. res_rarefacao_amostras &lt;- iNEXT(lista_rarefacao, q = 0, datatype=&quot;incidence_freq&quot;) Visualizar os resultados. # Gráfico ggiNEXT(res_rarefacao_amostras , type = 1, color.var = &quot;site&quot;) + theme_bw(base_size = 18) + theme(legend.position = &quot;right&quot;) + labs(x = &quot;Número de amostras&quot;, y = &quot; Riqueza de espécies&quot;) + scale_linetype_discrete(name = &quot;Método&quot;, labels = c(&quot;Interpolado&quot;, &quot;Extrapolado&quot;)) + scale_colour_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) Interpretação dos resultados Olhando o gráfico é possível perceber que a riqueza de espécies de morcegos rarefeita não é diferente entre os três fragmentos florestais quando corrigimos o problema da diferença no número de amostras. 10.4 Curva de rarefação coverage-based 10.4.0.1 Exemplo prático 4 - Morcegos Explicação dos dados Neste exemplo, usaremos os mesmos dados de espécies de morcegos amostradas em três fragmentos florestais (Breviglieri 2008). Análise Os comandos para a rarefação coverage-based são idênticos aos utilizados para o cálculo das curvas de rarefações baseadas nas abundâncias e amostras. Portanto, não repetiremos as linhas de comando aqui e utilizaremos os resultados já calculados para a visualização dos gráficos. Para isso, digitamos type = 3 que representa a curva de rarefação coverage-based. # Visualizar os resultados da rarefação *coverage-based*. ggiNEXT(res_rarefacao_amostras, type = 3, color.var = &quot;site&quot;) + theme_bw(base_size = 18) + theme(legend.position = &quot;right&quot;) + labs(x = &quot;Representatividade nas amostras&quot;, y = &quot;Riqueza de espécies&quot;) + scale_linetype_discrete(labels = c(&quot;Interpolado&quot;, &quot;Extrapolado&quot;)) + scale_colour_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;darkorchid&quot;, &quot;cyan4&quot;)) Interpretação dos resultados Coverage é uma medida que determina a proporção de amostras (sampled-based) ou do número de indivíduos (abundance-based) da comunidade que representa as espécies presentes na amostra. Um valor de coverage = 0,85 representa a riqueza estimada com base em 85% das amostragens ou da abundância da comunidade. No nosso exemplo, os valores de coverage foram acima de 0,93 indicando que precisamos de praticamente todas as amostras para estimar a riqueza observada em cada comunidade. Comparando as comunidades considerando o mesmo valor de coverage, 0,937 na comunidade Tenentes, identificamos que a riqueza de espécies de morcegos estimada na comunidade Experimental é menor do que a estimada para a comunidade de Talhadinho (não há sobreposição do intervalo de confiança). Percebam que usando a curva de rarefação coverage-based, a interpretação dos resultados foi diferente das observadas usando as curvas baseadas nos indivíduos ou amostras. Veja Chao &amp; Jost (2012) e Roswell et al. (2021) para explicações mais detalhadas sobre esta metodologia. 10.4.0.2 Exemplo prático 5 - Generalized Least Squares (GLS) Explicação dos dados Neste exemplo, iremos refazer o exercício do @{cap8} onde usamos Generalized Least Squares (GLS) para testar a relação da riqueza de ácaros com a quantidade de água no substrato. Contudo, ao invés de considerar a riqueza de espécies de ácaros observada como variável resposta, iremos utilizar a riqueza rarefeita para controlar o efeito da amostragem (i.e. diferentes abundâncias entre as comunidades). Os dados que usaremos estão disponíveis no pacote vegan e representam a composição de espécies de ácaros amostradas em 70 amostras. Pergunta: A riqueza rarefeita de espécies de ácaros é maior em comunidades localizadas em áreas com substratos secos? Predições O número de espécies rarefeita será maior em substratos secos, uma vez que as limitações fisiológicas impostas pela umidade limitam a ocorrência de várias espécies de ácaros. Variáveis Variáveis resposta e preditoras Matriz ou dataframe com as abundâncias das espécies de ácaros (variável resposta) registradas em 70 comunidades (variável preditora). Checklist Verificar se a sua matriz ou dataframe estão com as espécies nas linhas e as comunidades nas colunas. Análise Vamos calcular a riqueza rarefeita com base na comunidade com menor abundância. # Os dados estão com as comunidades nas colunas e as espécies nas linhas. # Para as análises teremos que transpor a planilha. composicao_acaros &lt;- t(mite) # Verificar qual é a menor abundância registrada nas comunidades. min(colSums(composicao_acaros)) #&gt; [1] 8 Vamos calcular a riqueza rarefeita de espécies para todas as comunidades considerando a menor abundância. Para padronizar e facilitar a extração dos resultados, definimos os argumentos knots (i.e. representa o intervalo igualmente espaçado que a função irá utilizar para determinar a riqueza estimada) e endpoint (i.e. o valor final de amostras ou abundância extrapolados) com o valor de abundância = 8. resultados_rarefacao &lt;- iNEXT(composicao_acaros, q = 0, datatype = &quot;abundance&quot;, knots = 8, endpoint = 8) Vamos criar um loop para facilitar a extração da riqueza rarefeita para as 70 comunidades. resultados_comunidades &lt;- data.frame() riqueza_rarefeita &lt;- c() for (i in 1:70){ resultados_comunidades &lt;- data.frame(resultados_rarefacao$iNextEst[i]) riqueza_rarefeita[i] &lt;- resultados_comunidades[8,4] } Vamos juntar esses resultados com os dados geográficos e ambientais. # Agrupando os dados em um dataframe final. dados_combinado &lt;- data.frame(riqueza_rarefeita, agua, coord) Agora, seguindo os passos descritos no @[cap8], vamos identificar o melhor modelo que representa a estrutura espacial dos dados da riqueza rarefeita. # Criando diferentes modelos usando a função gls. # sem estrutura espacial no_spat_gls &lt;- gls(riqueza_rarefeita ~ agua, data = dados_combinado, method = &quot;REML&quot;) # Covariância esférica espher_model &lt;- gls(riqueza_rarefeita ~ agua, data = dados_combinado, corSpher(form = ~lat + long, nugget = TRUE)) # Covariância exponencial expon_model &lt;- gls(riqueza_rarefeita ~ agua, data = dados_combinado, corExp(form = ~lat + long, nugget = TRUE)) # Covariância Gaussiana gauss_model &lt;- gls(riqueza_rarefeita ~ agua, data = dados_combinado, corGaus(form = ~lat + long, nugget = TRUE)) # Covariância razão quadrática ratio_model &lt;- gls(riqueza_rarefeita ~ agua, data = dados_combinado, corRatio(form = ~lat + long, nugget = TRUE)) Agora vamos usar o AIC para selecionar o modelo mais provável explicando a distribuição da riqueza rarefeita das espécies de ácaros. # Seleção dos modelos. aic_fit &lt;- AIC(no_spat_gls, espher_model, expon_model, gauss_model, ratio_model) aic_fit %&gt;% arrange(AIC) #&gt; df AIC #&gt; 1 5 164.5840 #&gt; 2 5 165.7649 #&gt; 3 5 165.8698 #&gt; 4 3 166.7530 #&gt; 5 5 169.0242 # Visualizando os resíduos do modelo selecionado. plot(gauss_model) Percebam que os pontos estão dispersos no gráfico e não apresentam padrões que indiquem heterogeneidade de variância. # Visualizando os resultados. summary(gauss_model)$tTable #&gt; Value Std.Error t-value p-value #&gt; (Intercept) 6.086125990 0.2927633293 20.788553 3.550849e-31 #&gt; agua -0.003142615 0.0006670097 -4.711498 1.258304e-05 # Calculando o R-squared. rsquared(gauss_model) #&gt; Response family link method R.squared #&gt; 1 riqueza_rarefeita gaussian identity none 0.2991059 # Obtendo os valores preditos pelo modelo. predito &lt;- predict(gauss_model) # Plotando os resultados no gráfico. ggplot(data = dados_combinado, aes(x= agua, y= riqueza_rarefeita)) + labs(x = &quot;Concentração de água no substrato&quot;, y = &quot;Riqueza rarefeita \\ndas espécies de ácaros&quot;, size = 15) + geom_point(size = 4, shape = 21, fill = &quot;gray&quot;, alpha = 0.7) + tema_livro() + geom_line(aes(y = predito), size = 1) Interpretação dos resultados A concentração de água no substrato explica 29,9% da variação na riqueza rarefeita das espécies de ácaros. Como predito, a riqueza de espécies de ácaros foi maior em comunidades localizadas em áreas com substratos secos do que em áreas com substratos úmidos (t = -4.71, df = 68, P &lt; 0.01). 10.4.1 Para se aprofundar Recomendamos aos interessados que olhem a página do EstimateS software e baixem o manual do usuário que contém informações detalhadas sobre os índices de rarefação. Este site foi criado e é mantido pelo Dr. Robert K. Colwell, um dos maiores especialistas do mundo em estimativas da biodiversidade Recomendamos a página pessoal da pesquisadora Anne Chao que é uma das responsáveis pelo desenvolvimento da metodologia e do pacote iNEXT. Nesta página, vocês irão encontrar exemplos e explicações detalhadas sobre as análises. Recomendamos também o livro Biological Diversity Frontiers in Measurement and Assessment (Magurran and McGill 2011). 10.4.2 Referências Referências "],["referências-2.html", "Referências", " Referências "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
